import streamlit as st
import io
import re
import zipfile
import math
import chardet
import os
import json
import random
import time
from github import Github

# NLP Imports
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet, stopwords
from nltk import pos_tag

# DOCX Import
from docx import Document

# Optional Spacy
try:
    import spacy
    _HAS_SPACY = True
except ImportError:
    _HAS_SPACY = False

# ------------------ é¡µé¢é…ç½® & æ ·å¼ä¼˜åŒ– ------------------
st.set_page_config(
    page_title="VocabMaster | æ™ºèƒ½è¯ä¹¦å·¥åŠ", 
    page_icon="âš¡", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# æ³¨å…¥ CSSï¼šå¢åŠ å‘¼å¸æ„Ÿï¼Œä¼˜åŒ–é˜…è¯»ä½“éªŒ
st.markdown("""
<style>
    /* å…¨å±€èƒŒæ™¯è‰²å¾®è°ƒ */
    .stApp { background-color: #fcfdfe; }
    
    /* æ ‡é¢˜å¢å¼º */
    h1, h2, h3 { font-family: 'Segoe UI', sans-serif; color: #2c3e50; }
    
    /* æ­¥éª¤æ ‡é¢˜æ ·å¼ */
    .step-header {
        font-size: 1.1rem;
        font-weight: 700;
        color: #4f46e5;
        margin-bottom: 10px;
        display: flex;
        align-items: center;
    }
    
    /* å¡ç‰‡å®¹å™¨ä¼˜åŒ– */
    [data-testid="stExpander"], [data-testid="stForm"] {
        background: white;
        border-radius: 12px;
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05);
        border: 1px solid #e5e7eb;
    }
    
    /* æŒ‰é’®ä¼˜åŒ– */
    div.stButton > button {
        border-radius: 8px;
        padding: 0.5rem 1rem;
        font-weight: 600;
        transition: all 0.2s;
    }
    div.stButton > button:hover { transform: translateY(-1px); }
    
    /* æç¤ºæ¡†æ ·å¼ */
    .info-box {
        background-color: #eff6ff;
        border-left: 4px solid #3b82f6;
        padding: 10px 15px;
        border-radius: 4px;
        color: #1e3a8a;
        font-size: 0.9em;
        margin-bottom: 15px;
    }
    
    /* èµ„æºé“¾æ¥æ ·å¼ */
    .resource-link {
        text-decoration: none;
        color: #0366d6;
        font-weight: 500;
    }
    .resource-link:hover {
        text-decoration: underline;
    }
</style>
""", unsafe_allow_html=True)

# ------------------ èµ„æºåŠ è½½ (æ ¸å¿ƒåŠŸèƒ½ä¸å˜) ------------------
@st.cache_resource
def download_nltk_resources():
    resources = ["punkt", "averaged_perceptron_tagger", "averaged_perceptron_tagger_eng", "wordnet", "omw-1.4", "stopwords"]
    for r in resources:
        try:
            nltk.data.find(f'tokenizers/{r}')
        except LookupError:
            nltk.download(r, quiet=True)
        except ValueError:
            nltk.download(r, quiet=True)

@st.cache_resource
def load_spacy_model():
    if _HAS_SPACY:
        try:
            return spacy.load("en_core_web_sm", disable=["ner", "parser"])
        except Exception:
            return None
    return None

download_nltk_resources()
nlp_spacy = load_spacy_model()

# ------------------ æ ¸å¿ƒé€»è¾‘å‡½æ•° ------------------
def save_to_github_library(filename, content, title, desc):
    """GitHub ä¸Šä¼ é€»è¾‘"""
    try:
        # å°è¯•è·å– Secretsï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ç»™å‡ºå‹å¥½æç¤º
        if "GITHUB_TOKEN" not in st.secrets:
            st.error("ğŸ”’ ç³»ç»Ÿæœªé…ç½® GitHub Tokenï¼Œæ— æ³•è¿æ¥äº‘ç«¯ã€‚è¯·åœ¨ .streamlit/secrets.toml ä¸­é…ç½®ã€‚")
            return

        token = st.secrets["GITHUB_TOKEN"]
        username = st.secrets["GITHUB_USERNAME"]
        repo_name = st.secrets["GITHUB_REPO"]
        
        g = Github(token)
        repo = g.get_repo(f"{username}/{repo_name}")
        
        library_path = f"library/{filename}"
        info_path = "library/info.json"
        
        # 1. ä¸Šä¼ /æ›´æ–°è¯ä¹¦æ–‡ä»¶
        try:
            contents = repo.get_contents(library_path)
            repo.update_file(library_path, f"Update {filename}", content, contents.sha)
        except:
            repo.create_file(library_path, f"Create {filename}", content)

        # 2. æ›´æ–° info.json
        try:
            info_contents = repo.get_contents(info_path)
            info_data = json.loads(info_contents.decoded_content.decode("utf-8"))
        except:
            info_data = {}
            info_contents = None

        info_data[filename] = {
            "title": title,
            "desc": desc,
            "date": time.strftime("%Y-%m-%d"),
            "author": "User" 
        }
        
        new_info_str = json.dumps(info_data, indent=2, ensure_ascii=False)
        if info_contents:
            repo.update_file(info_path, "Update info.json", new_info_str, info_contents.sha)
        else:
            repo.create_file(info_path, "Create info.json", new_info_str)
            
        # 3. åŒæ—¶ä¿å­˜åˆ°æœ¬åœ° library æ–‡ä»¶å¤¹ï¼Œç¡®ä¿ç«‹å³åœ¨â€œå…¬å…±åº“â€å¯è§
        local_lib = "library"
        if not os.path.exists(local_lib): os.makedirs(local_lib)
        
        with open(os.path.join(local_lib, filename), "w", encoding="utf-8") as f:
            f.write(content)
        
        local_info_path = os.path.join(local_lib, "info.json")
        # è¯»å–æœ¬åœ°ç°æœ‰info
        local_info = {}
        if os.path.exists(local_info_path):
            with open(local_info_path, "r", encoding="utf-8") as f:
                try: local_info = json.load(f)
                except: pass
        local_info[filename] = info_data[filename]
        with open(local_info_path, "w", encoding="utf-8") as f:
            json.dump(local_info, f, indent=2, ensure_ascii=False)

        st.toast("âœ… å‘å¸ƒæˆåŠŸï¼", icon="ğŸ‰")
        time.sleep(1.5)
        st.rerun()
        
    except Exception as e:
        st.error(f"è¿æ¥äº‘ç«¯å¤±è´¥: {e}")

def extract_text_from_bytes(file_obj, filename):
    if '.' in filename: ext = filename.split('.')[-1].lower()
    else: ext = 'txt'
    text = ""
    try:
        if ext == 'docx':
            doc = Document(file_obj)
            text_content = []
            for p in doc.paragraphs:
                if p.text.strip(): text_content.append(p.text)
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if cell.text.strip(): text_content.append(cell.text)
            text = "\n".join(text_content)
        elif ext == 'doc':
            return ""
        else:
            raw = file_obj.read()
            enc = chardet.detect(raw).get('encoding') or 'utf-8'
            text = raw.decode(enc, errors='ignore')
    except Exception: return ""
    
    if ext in ['srt', 'vtt', 'ass']:
        clean_text = re.sub(r"<.*?>", "", text)
        clean_text = re.sub(r"\d{2}:\d{2}:\d{2},\d{3} --> \d{2}:\d{2}:\d{2},\d{3}", "", clean_text)
        return clean_text
    return text

def process_words(all_text, mode, min_len, filter_set=None):
    TOKEN_RE = re.compile(r"[A-Za-z-]+")
    cleaned = [re.sub(r'[^a-z]', '', w.lower()) for w in TOKEN_RE.findall(all_text) if w]
    lemmatized = []
    
    if mode == "spacy" and nlp_spacy:
        chunk_size = 50000
        chunks = [cleaned[i:i + chunk_size] for i in range(0, len(cleaned), chunk_size)]
        for chunk in chunks:
            doc = nlp_spacy(" ".join(chunk))
            for token in doc:
                lw = token.lemma_.lower()
                if lw.isalpha() and wordnet.synsets(lw): lemmatized.append(lw)
    else:
        lemmatizer = WordNetLemmatizer()
        for w, tag in pos_tag(cleaned):
            wn = {'J':wordnet.ADJ,'V':wordnet.VERB,'N':wordnet.NOUN,'R':wordnet.ADV}.get(tag[0], None)
            lw = lemmatizer.lemmatize(w, wn) if wn else lemmatizer.lemmatize(w)
            if wordnet.synsets(lw): lemmatized.append(lw)

    seen = set()
    final_words = []
    sys_stopwords = set(stopwords.words('english'))
    for w in lemmatized:
        if len(w) < min_len: continue
        if w in sys_stopwords: continue
        if filter_set and w in filter_set: continue
        if w not in seen:
            seen.add(w)
            final_words.append(w)
    return final_words

# ------------------ UI å¸ƒå±€è®¾è®¡ ------------------

# ä¾§è¾¹æ 
with st.sidebar:
    st.image("https://img.icons8.com/fluency/96/dictionary.png", width=50)
    st.markdown("### VocabMaster")
    st.caption("v2.2 Resource Edition")
    st.markdown("---")
    
    menu = st.radio(
        "é€‰æ‹©åŠŸèƒ½", 
        ["âš¡ åˆ¶ä½œç”Ÿè¯æœ¬", "ğŸŒ å…¬å…±è¯ä¹¦åº“"],
        captions=["ä»æ–‡ä»¶æå–å•è¯", "ä¸‹è½½ç°æˆçš„è¯ä¹¦"]
    )
    
    st.markdown("---")
    st.info("**å°è´´å£«**\nä½¿ç”¨ Spacy å¼•æ“å¯ä»¥è·å¾—æ›´å‡†ç¡®çš„è¯å½¢è¿˜åŸï¼ˆä¾‹å¦‚å°† 'running' è¿˜åŸä¸º 'run'ï¼‰ã€‚")

# === åŠŸèƒ½ä¸€: åˆ¶ä½œç”Ÿè¯æœ¬ ===
if menu == "âš¡ åˆ¶ä½œç”Ÿè¯æœ¬":
    st.title("âš¡ æ™ºèƒ½ç”Ÿè¯æå–å·¥åŠ")
    
    # --- æŒ‡å¼•åŒºåŸŸ & èµ„æºæ¨è (æ–°å¢) ---
    with st.expander("ğŸ“– æ–°æ‰‹æŒ‡å— & å­—å¹•èµ„æºæ¨è (ç‚¹å‡»å±•å¼€)", expanded=False):
        
        tab_guide, tab_resources = st.tabs(["ğŸ’¡ å¦‚ä½•ä½¿ç”¨", "ğŸ”— æ²¡å­—å¹•ï¼Ÿå»å“ªæ‰¾"])
        
        with tab_guide:
            st.markdown("""
            1.  **å‡†å¤‡æ–‡ä»¶**ï¼šæ‰¾åˆ°ä½ æƒ³å­¦ä¹ çš„å­—å¹•æ–‡ä»¶ (`.srt`, `.ass`) æˆ–è‹±æ–‡æ–‡æ¡£ã€‚
            2.  **è®¾ç½®è§„åˆ™**ï¼šåœ¨å·¦ä¾§è®¾ç½®è¿‡æ»¤æ¡ä»¶ï¼Œå»ºè®®ä¸Šä¼ â€œç†Ÿè¯è¡¨â€ä»¥è¿‡æ»¤æ‰ç®€å•è¯ã€‚
            3.  **ä¸Šä¼ åˆ†æ**ï¼šæ‹–å…¥æ–‡ä»¶ï¼Œç³»ç»Ÿè‡ªåŠ¨æå–é«˜é¢‘ç”Ÿè¯ã€‚
            4.  **å¯¼å‡ºåˆ†äº«**ï¼šç”Ÿæˆç»“æœåï¼Œå¯ä¸‹è½½ ZIP æˆ–å‘å¸ƒåˆ°å…¬å…±åº“ã€‚
            """)
            
        with tab_resources:
            st.markdown("è¿™é‡Œæ•´ç†äº†å¸¸ç”¨çš„å­—å¹•ä¸‹è½½ç«™ç‚¹ï¼Œæ–¹ä¾¿æ‚¨å¯»æ‰¾å­¦ä¹ ç´ æï¼š")
            c_res1, c_res2 = st.columns(2)
            with c_res1:
                st.markdown("ğŸ¯ **[ä¼ªå°„æ‰‹ç½‘ (Assrt)](https://assrt.net/)**")
                st.caption("è€ç‰Œå­—å¹•ç«™ï¼Œèµ„æºæå…¶ä¸°å¯Œï¼Œæ”¯æŒä¸­è‹±åŒè¯­ã€‚")
                
                st.markdown("ğŸ“º **[å­—å¹•åº“ (Zimuku)](http://zimuku.org/)**")
                st.caption("ç¾å‰§ã€æ—¥å‰§æ›´æ–°é€Ÿåº¦å¿«ï¼Œæœç´¢ä½“éªŒå¥½ã€‚")
            with c_res2:
                st.markdown("ğŸ’ **[SubHD](https://subhd.tv/)**")
                st.caption("ç•Œé¢æ¸…çˆ½ï¼Œé«˜æ¸…å½±è§†å­—å¹•çš„é¦–é€‰ä¹‹åœ°ã€‚")
                
                st.markdown("ğŸŒ **[OpenSubtitles](https://www.opensubtitles.org/)**")
                st.caption("å…¨çƒæœ€å¤§çš„å­—å¹•åº“ï¼Œå¯»æ‰¾çº¯è‹±æ–‡å­—å¹•çš„æœ€ä½³é€‰æ‹©ã€‚")

    # çŠ¶æ€ç®¡ç†åˆå§‹åŒ–
    if 'result_words' not in st.session_state: st.session_state.result_words = []
    if 'source_files_count' not in st.session_state: st.session_state.source_files_count = 0
    
    # --- ä¸»æ“ä½œåŒº ---
    c_config, c_upload = st.columns([1, 2], gap="large")
    
    # å·¦æ ï¼šé…ç½®
    with c_config:
        st.markdown('<div class="step-header">1ï¸âƒ£ è®¾ç½®æå–è§„åˆ™</div>', unsafe_allow_html=True)
        with st.container(border=True):
            nlp_mode = st.selectbox("AI å¤„ç†å¼•æ“", ["nltk (å¿«é€Ÿ)", "spacy (ç²¾å‡†)"])
            mode_key = "spacy" if "spacy" in nlp_mode else "nltk"
            
            min_len = st.number_input("å•è¯æœ€çŸ­é•¿åº¦", value=3, min_value=1)
            
            st.divider()
            st.markdown("**ç†Ÿè¯è¿‡æ»¤ (å¯é€‰)**")
            filter_file = st.file_uploader("ä¸Šä¼ ç†Ÿè¯è¡¨ (.txt)", type=['txt'], label_visibility="collapsed")
            filter_set = set()
            if filter_file:
                c = filter_file.getvalue().decode("utf-8", errors='ignore')
                filter_set = set(l.strip().lower() for l in c.splitlines() if l.strip())
                st.caption(f"âœ… å·²åŠ è½½ {len(filter_set)} ä¸ªç†Ÿè¯")

    # å³æ ï¼šä¸Šä¼ ä¸æ‰§è¡Œ
    with c_upload:
        st.markdown('<div class="step-header">2ï¸âƒ£ ä¸Šä¼ æ–‡ä»¶å¹¶åˆ†æ</div>', unsafe_allow_html=True)
        with st.container(border=True):
            st.markdown("<div class='info-box'>æ”¯æŒ .srt, .ass, .docx, .txt æ‰¹é‡ä¸Šä¼ </div>", unsafe_allow_html=True)
            
            uploaded_files = st.file_uploader(
                "æ–‡ä»¶ä¸Šä¼ åŒº", 
                type=['txt','srt','ass','vtt','docx'], 
                accept_multiple_files=True,
                label_visibility="collapsed"
            )
            
            st.markdown("<br>", unsafe_allow_html=True)
            
            # æŒ‰é’®åŒº
            if uploaded_files:
                if st.button("ğŸš€ å¼€å§‹æ™ºèƒ½æå–", type="primary", use_container_width=True):
                    # è¿›åº¦æ¡
                    progress_text = "æ­£åœ¨è¯»å–æ–‡ä»¶..."
                    my_bar = st.progress(0, text=progress_text)
                    
                    all_raw_text = []
                    for idx, file in enumerate(uploaded_files):
                        text = extract_text_from_bytes(file, file.name)
                        all_raw_text.append(text)
                        my_bar.progress((idx + 1) / len(uploaded_files), text=f"è§£ææ–‡ä»¶: {file.name}")
                    
                    full_text = "\n".join(all_raw_text)
                    
                    if full_text.strip():
                        my_bar.progress(100, text=f"æ­£åœ¨ä½¿ç”¨ {mode_key.upper()} å¼•æ“æ¸…æ´—æ•°æ®...")
                        words = process_words(full_text, mode_key, min_len, filter_set)
                        
                        # æ›´æ–° Session State
                        st.session_state.result_words = words
                        st.session_state.source_files_count = len(uploaded_files)
                        
                        my_bar.empty()
                        st.success(f"æå–å®Œæˆï¼å…±å‘ç° {len(words)} ä¸ªç”Ÿè¯ã€‚")
                        time.sleep(0.5)
                        st.rerun() # å¼ºåˆ¶åˆ·æ–°æ˜¾ç¤ºç»“æœ
                    else:
                        st.error("æ— æ³•ä»æ–‡ä»¶ä¸­è¯†åˆ«æ–‡å­—ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ã€‚")

    # --- ç»“æœå±•ç¤ºåŒº (Step 3) ---
    if st.session_state.result_words:
        st.divider()
        st.markdown('<div class="step-header">3ï¸âƒ£ ç»“æœé¢„è§ˆä¸å¯¼å‡º</div>', unsafe_allow_html=True)
        
        words = st.session_state.result_words
        
        # ç»“æœæ¦‚è§ˆæ 
        with st.container(border=True):
            col_stat1, col_stat2, col_stat3 = st.columns(3)
            col_stat1.metric("ğŸ“š æå–ç”Ÿè¯æ€»æ•°", f"{len(words)}")
            col_stat2.metric("â±ï¸ å»ºè®®å­¦ä¹ å¤©æ•°", f"{math.ceil(len(words)/20)} å¤©")
            col_stat3.metric("ğŸ” è¯æ±‡æ¥æº", f"{st.session_state.source_files_count} ä¸ªæ–‡ä»¶")

        col_preview, col_action = st.columns([1.5, 1], gap="medium")

        # å·¦ä¾§ï¼šåˆ—è¡¨é¢„è§ˆ
        with col_preview:
            st.subheader("ğŸ“‹ å•è¯åˆ—è¡¨")
            st.dataframe(
                [{"åºå·": i+1, "å•è¯": w} for i, w in enumerate(words)],
                use_container_width=True,
                height=400,
                hide_index=True
            )

        # å³ä¾§ï¼šå¯¼å‡ºæ“ä½œ
        with col_action:
            st.subheader("ğŸ’¾ ä¿å­˜æ–¹å¼")
            tab1, tab2 = st.tabs(["ğŸ“¥ ä¸‹è½½åˆ°æœ¬åœ°", "â˜ï¸ åˆ†äº«åˆ°äº‘ç«¯"])
            
            with tab1:
                st.caption("å°†å•è¯æ‰“åŒ…ä¸º .zip ä¸‹è½½")
                chunk_size = st.number_input("æ‹†åˆ†å¤§å° (è¯/æ–‡ä»¶)", value=5000, step=1000)
                
                # å‡†å¤‡ Zip
                zip_buffer = io.BytesIO()
                num_files = math.ceil(len(words) / chunk_size)
                with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                    for i in range(num_files):
                        s = i * chunk_size
                        e = min(s + chunk_size, len(words))
                        zf.writestr(f"word_list_{i+1}.txt", "\n".join(words[s:e]))
                
                st.download_button(
                    "ğŸ“¦ ç‚¹å‡»ä¸‹è½½ ZIP", 
                    zip_buffer.getvalue(), 
                    "my_vocabulary.zip", 
                    "application/zip", 
                    type="primary",
                    use_container_width=True
                )

            with tab2:
                st.caption("å‘å¸ƒåˆ°â€œå…¬å…±è¯ä¹¦åº“â€ï¼Œä¸ä»–äººåˆ†äº«")
                with st.form("pub_form"):
                    s_name = st.text_input("æ–‡ä»¶å (è‹±æ–‡, .txt)", value=f"vocab_{int(time.time())}.txt")
                    s_title = st.text_input("æ ‡é¢˜", placeholder="å¦‚ï¼šè€å‹è®°ç¬¬ä¸€å­£é«˜é¢‘è¯")
                    s_desc = st.text_area("ç®€ä»‹", placeholder="è¿™æœ¬è¯ä¹¦æ¥è‡ªäº...")
                    
                    if st.form_submit_button("ğŸŒ ç¡®è®¤å‘å¸ƒ", use_container_width=True):
                        if not s_name.endswith(".txt"):
                            st.warning("æ–‡ä»¶åå¿…é¡»ä»¥ .txt ç»“å°¾")
                        else:
                            with st.spinner("æ­£åœ¨ä¸Šä¼ ..."):
                                save_to_github_library(s_name, "\n".join(words), s_title, s_desc)

# === åŠŸèƒ½äºŒ: å…¬å…±è¯ä¹¦åº“ ===
elif menu == "ğŸŒ å…¬å…±è¯ä¹¦åº“":
    st.title("ğŸŒ ç¤¾åŒºå…¬å…±è¯ä¹¦åº“")
    
    st.markdown("""
    <div class="info-box">
    è¿™é‡Œæ±‡é›†äº†å¤§å®¶ä¸Šä¼ çš„ç²¾é€‰è¯ä¹¦ã€‚æ‚¨å¯ä»¥è‡ªç”±æµè§ˆã€ä¸‹è½½å­¦ä¹ ã€‚<br>
    æƒ³è¦åˆ†äº«æ‚¨çš„è¯ä¹¦ï¼Ÿè¯·å‰å¾€â€œåˆ¶ä½œç”Ÿè¯æœ¬â€é¡µé¢è¿›è¡Œå‘å¸ƒã€‚
    </div>
    """, unsafe_allow_html=True)
    
    # æœç´¢ä¸è¿‡æ»¤
    col_search, _ = st.columns([2, 1])
    with col_search:
        search_q = st.text_input("ğŸ” æœç´¢è¯ä¹¦æ ‡é¢˜...", placeholder="è¾“å…¥å…³é”®è¯æœç´¢...").lower()

    # æ•°æ®åŠ è½½
    LIBRARY_DIR = "library"
    INFO_FILE = "info.json"
    
    # ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
    if not os.path.exists(LIBRARY_DIR): 
        os.makedirs(LIBRARY_DIR)
    
    book_info = {}
    try:
        with open(os.path.join(LIBRARY_DIR, INFO_FILE), "r", encoding="utf-8") as f:
            book_info = json.load(f)
    except: pass

    try:
        files = [f for f in os.listdir(LIBRARY_DIR) if f.endswith(".txt")]
    except: files = []
    
    # è¿‡æ»¤æ–‡ä»¶
    visible_files = []
    for f in files:
        meta = book_info.get(f, {})
        t = meta.get("title", f).lower()
        if search_q in t or search_q in f.lower():
            visible_files.append(f)

    if not visible_files:
        st.warning("ğŸ“­ æš‚æ—¶æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è¯ä¹¦ã€‚å¦‚æœæ‚¨æ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œï¼Œè¯·å°è¯•å…ˆåœ¨â€œåˆ¶ä½œç”Ÿè¯æœ¬â€ä¸­ä¸Šä¼ å¹¶å‘å¸ƒä¸€ä¸ªæ–‡ä»¶ã€‚")
    else:
        st.divider()
        # å¡ç‰‡ç½‘æ ¼æ˜¾ç¤º
        cols = st.columns(3)
        for i, filename in enumerate(visible_files):
            file_path = os.path.join(LIBRARY_DIR, filename)
            try:
                with open(file_path, "r", encoding="utf-8") as f: content = f.read()
                count = len(content.splitlines())
                meta = book_info.get(filename, {})
                
                title = meta.get("title", filename)
                desc = meta.get("desc", "æš‚æ— æè¿°")
                date = meta.get("date", "")
                
                # è½®è¯¢åˆ—
                with cols[i % 3]:
                    with st.container(border=True):
                        st.subheader(f"ğŸ“„ {title}")
                        st.caption(f"ğŸ“… {date} | ğŸ“ {count} è¯")
                        st.markdown(f"<div style='height:40px;overflow:hidden;color:grey;font-size:0.9em'>{desc}</div>", unsafe_allow_html=True)
                        st.markdown("<br>", unsafe_allow_html=True)
                        st.download_button(
                            "â¬‡ï¸ ä¸‹è½½è¯è¡¨", 
                            content, 
                            filename, 
                            "text/plain",
                            key=f"btn_{i}",
                            use_container_width=True
                        )
            except: continue

import streamlit as st
import io
import re
import zipfile
import math
import chardet
import os
import json
import random
import time
import pandas as pd
import streamlit.components.v1 as components
from github import Github

# NLP Imports
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet, stopwords
from nltk import pos_tag

# DOCX Import
from docx import Document

# Optional Spacy
try:
    import spacy
    _HAS_SPACY = True
except ImportError:
    _HAS_SPACY = False

# ------------------ 0. åˆå§‹åŒ–ï¼šå‡†å¤‡é¢„ç½®è¯åº“æ•°æ® (æ–°å¢éƒ¨åˆ†) ------------------
# å®šä¹‰å­˜æ”¾è¯è¡¨çš„ç›®å½•
WORDLIST_DIR = "wordlists"

# ç¡®ä¿ç›®å½•å­˜åœ¨
if not os.path.exists(WORDLIST_DIR):
    os.makedirs(WORDLIST_DIR)
    # --- ã€é‡è¦æç¤ºã€‘è¯·ç”¨çœŸå®çš„è¯è¡¨æ›¿æ¢ä»¥ä¸‹æ¼”ç¤ºæ–‡ä»¶å†…å®¹ ---
    # åˆ›å»ºæ¼”ç¤ºç”¨çš„å°å­¦è¯è¡¨
    with open(os.path.join(WORDLIST_DIR, "primary.txt"), "w", encoding="utf-8") as f:
        f.write("a\nan\nthe\nis\nare\nam\nhello\ngood\nmorning\napple\nbanana\ncat\ndog\nbook\npen")
    # åˆ›å»ºæ¼”ç¤ºç”¨çš„ä¸­è€ƒè¯è¡¨
    with open(os.path.join(WORDLIST_DIR, "zhongkao.txt"), "w", encoding="utf-8") as f:
        f.write("ability\nabsent\naccept\naccording\nachieve\nactive\nactually\nadd\naddress\nadmit")
    # åˆ›å»ºæ¼”ç¤ºç”¨çš„é«˜è€ƒè¯è¡¨
    with open(os.path.join(WORDLIST_DIR, "gaokao.txt"), "w", encoding="utf-8") as f:
        f.write("abandon\nability\nabnormal\naboard\nabolish\nabortion\nabrupt\nabsence\nabsolute\nabsorb")
    print(f"å·²åœ¨ {WORDLIST_DIR} ç›®å½•ä¸‹åˆ›å»ºæ¼”ç¤ºè¯è¡¨æ–‡ä»¶ã€‚è¯·æ›¿æ¢ä¸ºçœŸå®æ•°æ®ã€‚")

# å®šä¹‰é¢„ç½®è¯åº“çš„æ˜¾ç¤ºåç§°å’Œæ–‡ä»¶è·¯å¾„æ˜ å°„
PRESET_WORDLISTS = {
    "ğŸ‘¶ å°å­¦æ ¸å¿ƒè¯ (æ¼”ç¤º)": os.path.join(WORDLIST_DIR, "primary.txt"),
    "ğŸ‘¦ ä¸­è€ƒå¿…å¤‡è¯ (æ¼”ç¤º)": os.path.join(WORDLIST_DIR, "zhongkao.txt"),
    "ğŸ‘¨â€ğŸ“ é«˜è€ƒ3500è¯ (æ¼”ç¤º)": os.path.join(WORDLIST_DIR, "gaokao.txt"),
    # ä½ å¯ä»¥åœ¨è¿™é‡Œç»§ç»­æ·»åŠ ï¼Œä¾‹å¦‚å››å…­çº§ã€è€ƒç ”ç­‰
    # "ğŸ“š å¤§å­¦å››çº§è¯æ±‡": os.path.join(WORDLIST_DIR, "cet4.txt"),
}

# ------------------ 1. é¡µé¢é…ç½® & æç®€ä¸»ä¹‰è®¾è®¡ç³»ç»Ÿ (UI Overhaul) ------------------
st.set_page_config(
    page_title="VocabMaster | æ™ºèƒ½è¯ä¹¦å·¥åŠ", 
    page_icon="âš¡", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# æ³¨å…¥ CSSï¼šå¼•å…¥ç°ä»£è®¾è®¡è¯­è¨€ (Glassmorphism, Soft UI)
st.markdown("""
<style>
    /* å¼•å…¥ Google Fonts: Inter */
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');

    /* å…¨å±€åŸºç¡€è®¾ç½® */
    .stApp {
        background-color: #f8fafc; /* ææµ…çš„è“ç°è‰²èƒŒæ™¯ */
        font-family: 'Inter', sans-serif;
    }
    
    h1, h2, h3, h4, h5 {
        color: #1e293b;
        font-family: 'Inter', sans-serif;
        font-weight: 700;
        letter-spacing: -0.025em;
    }
    
    p, div, span {
        color: #475569;
        line-height: 1.6;
    }

    /* ä¾§è¾¹æ ç¾åŒ– */
    section[data-testid="stSidebar"] {
        background-color: #ffffff;
        border-right: 1px solid #f1f5f9;
        box-shadow: 4px 0 24px rgba(0,0,0,0.02);
    }

    /* å¡ç‰‡åŒ–å®¹å™¨ (Expander, Form, Container) */
    [data-testid="stExpander"], [data-testid="stForm"], [data-testid="stContainer"] {
        background: #ffffff;
        border: 1px solid #e2e8f0;
        border-radius: 16px; /* æ›´å¤§çš„åœ†è§’ */
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05), 0 2px 4px -1px rgba(0, 0, 0, 0.03); /* æŸ”å’Œé˜´å½± */
        transition: all 0.3s ease;
    }
    
    /* å»é™¤ Expander é»˜è®¤çš„ä¸‘é™‹è¾¹æ¡†ï¼Œæ”¹ä¸ºçº¯å‡€å¡ç‰‡ */
    .streamlit-expanderHeader {
        background-color: transparent;
        color: #334155;
        font-weight: 600;
    }

    /* æŒ‰é’®è®¾è®¡ï¼šæ¸å˜è‰² + æ‚¬æµ®æ•ˆæœ */
    div.stButton > button {
        background: linear-gradient(135deg, #6366f1 0%, #4f46e5 100%); /* Indigo Gradient */
        color: white;
        border: none;
        border-radius: 10px;
        padding: 0.6rem 1.2rem;
        font-weight: 600;
        font-size: 0.95rem;
        box-shadow: 0 4px 12px rgba(79, 70, 229, 0.3);
        transition: transform 0.2s ease, box-shadow 0.2s ease;
    }
    
    div.stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 16px rgba(79, 70, 229, 0.4);
        color: white;
    }
    
    /* æ¬¡çº§æŒ‰é’® (Secondary Button) æ ·å¼è¦†ç›– */
    div.stButton > button[kind="secondary"] {
        background: white;
        color: #4f46e5;
        border: 1px solid #e2e8f0;
        box-shadow: 0 2px 5px rgba(0,0,0,0.05);
    }

    /* é“¾æ¥æŒ‰é’® (Link Button) */
    a[kind="primary"] {
        background: linear-gradient(135deg, #10b981 0%, #059669 100%) !important; /* ç»¿è‰²æ¸å˜åŒºåˆ† */
        border: none !important;
        box-shadow: 0 4px 12px rgba(16, 185, 129, 0.3) !important;
        border-radius: 10px !important;
        color: white !important;
        font-weight: 600 !important;
        text-decoration: none !important;
        display: flex;
        justify-content: center;
        align-items: center;
        padding: 0.6rem 1.2rem;
        transition: transform 0.2s ease !important;
    }
    a[kind="primary"]:hover {
        transform: translateY(-2px);
    }

    /* è¾“å…¥æ¡†ä¸ä¸‹æ‹‰èœå•ç¾åŒ– */
    .stTextInput > div > div, .stSelectbox > div > div, .stNumberInput > div > div, .stMultiSelect > div > div {
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 10px;
        color: #334155;
    }
    .stTextInput > div > div:focus-within, .stMultiSelect > div > div:focus-within {
        border-color: #6366f1;
        box-shadow: 0 0 0 3px rgba(99, 102, 241, 0.1);
    }
    /* MultiSelect çš„ Tag æ ·å¼ */
    .stMultiSelect [data-baseweb="tag"] {
        background-color: #e0e7ff;
        border: 1px solid #c7d2fe;
        color: #4f46e5;
    }

    /* æ­¥éª¤æ ‡é¢˜æ ·å¼ (ä¿æŒåŸé€»è¾‘ï¼Œä¼˜åŒ–è§†è§‰) */
    .step-header {
        font-size: 1.25rem;
        font-weight: 700;
        color: #334155;
        margin-bottom: 16px;
        display: flex;
        align-items: center;
        padding-bottom: 8px;
        border-bottom: 2px solid #f1f5f9;
    }
    
    /* æç¤ºæ¡† (Info Box) ç»ç’ƒæ‹Ÿæ€ */
    .info-box {
        background: rgba(239, 246, 255, 0.7);
        backdrop-filter: blur(10px);
        border: 1px solid #dbeafe;
        border-left: 4px solid #3b82f6;
        padding: 16px;
        border-radius: 8px;
        color: #1e40af;
        font-size: 0.95em;
        margin-bottom: 20px;
    }
    
    /* ä»£ç å—æ ·å¼ä¼˜åŒ–ï¼šæ›´åƒç¼–è¾‘å™¨ */
    .stCodeBlock {
        max-height: 220px !important;
        overflow-y: auto !important;
        border: 1px solid #cbd5e1;
        border-radius: 8px;
        background-color: #f1f5f9; /* æ›´æ·±çš„ç°èƒŒæ™¯ï¼Œå¢åŠ å¯¹æ¯” */
        font-family: 'JetBrains Mono', monospace;
    }
    
    /* Metric æŒ‡æ ‡å¡ç‰‡ */
    [data-testid="stMetric"] {
        background-color: white;
        padding: 15px;
        border-radius: 12px;
        border: 1px solid #f1f5f9;
        box-shadow: 0 2px 4px rgba(0,0,0,0.02);
        text-align: center;
    }
    [data-testid="stMetricLabel"] { color: #64748b; font-size: 0.9rem; }
    [data-testid="stMetricValue"] { color: #4f46e5; font-size: 1.8rem; font-weight: 700; }

    /* Tabs é€‰é¡¹å¡ç¾åŒ– */
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
        background-color: transparent;
    }
    .stTabs [data-baseweb="tab"] {
        background-color: white;
        border-radius: 8px 8px 0 0;
        border: 1px solid #e2e8f0;
        border-bottom: none;
        padding: 10px 20px;
        color: #64748b;
    }
    .stTabs [aria-selected="true"] {
        background-color: #f8fafc;
        color: #4f46e5;
        font-weight: 600;
        border-top: 2px solid #4f46e5;
    }
</style>
""", unsafe_allow_html=True)

# ------------------ 2. è‡ªå®šä¹‰å¤åˆ¶æŒ‰é’®ç»„ä»¶ (UI åŒæ­¥ä¼˜åŒ–) ------------------
def render_copy_button(text_content, unique_key):
    safe_text = json.dumps(text_content)
    # æ›´æ–° JS æŒ‰é’®æ ·å¼ä»¥åŒ¹é…æ–°çš„è®¾è®¡ç³»ç»Ÿ
    html_code = f"""
    <!DOCTYPE html>
    <html>
    <head>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@600&display=swap');
        body {{ margin: 0; padding: 0; }}
        .copy-btn {{
            width: 100%;
            padding: 14px;
            /* åŒ¹é… Python æŒ‰é’®çš„ Indigo æ¸å˜ */
            background: linear-gradient(135deg, #6366f1 0%, #4f46e5 100%);
            color: white;
            border: none;
            border-radius: 10px;
            font-family: 'Inter', sans-serif;
            font-weight: 600;
            font-size: 15px;
            cursor: pointer;
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            display: flex;
            align-items: center;
            justify-content: center;
            box-shadow: 0 4px 12px rgba(79, 70, 229, 0.3);
            letter-spacing: 0.02em;
        }}
        .copy-btn:hover {{
            transform: translateY(-2px);
            box-shadow: 0 8px 20px rgba(79, 70, 229, 0.4);
            filter: brightness(1.05);
        }}
        .copy-btn:active {{
            transform: translateY(0);
        }}
        .icon {{ margin-right: 8px; font-size: 18px; }}
    </style>
    </head>
    <body>
        <button id="btn_{unique_key}" class="copy-btn" onclick="copyText()">
            <span class="icon">ğŸ“‹</span> ä¸€é”®å¤åˆ¶ (Copy All)
        </button>
        <script>
        function copyText() {{
            const text = {safe_text};
            const textArea = document.createElement("textarea");
            textArea.value = text;
            document.body.appendChild(textArea);
            textArea.select();
            try {{
                document.execCommand('copy');
                const btn = document.getElementById("btn_{unique_key}");
                const originalHTML = btn.innerHTML;
                
                // æˆåŠŸçŠ¶æ€ - ç»¿è‰²æ¸å˜
                btn.innerHTML = '<span class="icon">âœ…</span> æˆåŠŸï¼(Copied)';
                btn.style.background = "linear-gradient(135deg, #10b981 0%, #059669 100%)";
                btn.style.boxShadow = "0 4px 12px rgba(16, 185, 129, 0.3)";
                
                setTimeout(() => {{
                    btn.innerHTML = originalHTML;
                    btn.style.background = "linear-gradient(135deg, #6366f1 0%, #4f46e5 100%)";
                    btn.style.boxShadow = "0 4px 12px rgba(79, 70, 229, 0.3)";
                }}, 2000);
            }} catch (err) {{}}
            document.body.removeChild(textArea);
        }}
        </script>
    </body>
    </html>
    """
    components.html(html_code, height=55)

# ------------------ 3. èµ„æºåŠ è½½ (åŠŸèƒ½ä¸å˜) ------------------
@st.cache_resource
def download_nltk_resources():
    resources = ["punkt", "averaged_perceptron_tagger", "averaged_perceptron_tagger_eng", "wordnet", "omw-1.4", "stopwords"]
    for r in resources:
        try: nltk.data.find(f'tokenizers/{r}')
        except LookupError: nltk.download(r, quiet=True)
        except ValueError: nltk.download(r, quiet=True)

@st.cache_resource
def load_spacy_model():
    if _HAS_SPACY:
        try: return spacy.load("en_core_web_sm", disable=["ner", "parser"])
        except Exception: return None
    return None

download_nltk_resources()
nlp_spacy = load_spacy_model()

# ------------------ 4. æ ¸å¿ƒé€»è¾‘ (åŠŸèƒ½ä¸å˜) ------------------
def save_to_github_library(filename, content, title, desc):
    try:
        if "GITHUB_TOKEN" not in st.secrets:
            st.error("ğŸ”’ ç³»ç»Ÿæœªé…ç½® GitHub Tokenã€‚")
            return

        token = st.secrets["GITHUB_TOKEN"]
        username = st.secrets["GITHUB_USERNAME"]
        repo_name = st.secrets["GITHUB_REPO"]
        
        g = Github(token)
        repo = g.get_repo(f"{username}/{repo_name}")
        
        library_path = f"library/{filename}"
        info_path = "library/info.json"
        
        try:
            contents = repo.get_contents(library_path)
            repo.update_file(library_path, f"Update {filename}", content, contents.sha)
        except:
            repo.create_file(library_path, f"Create {filename}", content)

        try:
            info_contents = repo.get_contents(info_path)
            info_data = json.loads(info_contents.decoded_content.decode("utf-8"))
        except:
            info_data = {}
            info_contents = None

        info_data[filename] = {
            "title": title,
            "desc": desc,
            "date": time.strftime("%Y-%m-%d"),
            "author": "User" 
        }
        
        new_info_str = json.dumps(info_data, indent=2, ensure_ascii=False)
        if info_contents:
            repo.update_file(info_path, "Update info.json", new_info_str, info_contents.sha)
        else:
            repo.create_file(info_path, "Create info.json", new_info_str)
            
        local_lib = "library"
        if not os.path.exists(local_lib): os.makedirs(local_lib)
        with open(os.path.join(local_lib, filename), "w", encoding="utf-8") as f:
            f.write(content)
        
        local_info_path = os.path.join(local_lib, "info.json")
        local_info = {}
        if os.path.exists(local_info_path):
            with open(local_info_path, "r", encoding="utf-8") as f:
                try: local_info = json.load(f)
                except: pass
        local_info[filename] = info_data[filename]
        with open(local_info_path, "w", encoding="utf-8") as f:
            json.dump(local_info, f, indent=2, ensure_ascii=False)

        st.toast("âœ… å‘å¸ƒæˆåŠŸï¼", icon="ğŸ‰")
        time.sleep(1.5)
        st.rerun()
        
    except Exception as e:
        st.error(f"ä¸Šä¼ å¤±è´¥: {e}")

def extract_text_from_bytes(file_obj, filename):
    if '.' in filename: ext = filename.split('.')[-1].lower()
    else: ext = 'txt'
    text = ""
    try:
        if ext == 'docx':
            doc = Document(file_obj)
            text_content = []
            for p in doc.paragraphs:
                if p.text.strip(): text_content.append(p.text)
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if cell.text.strip(): text_content.append(cell.text)
            text = "\n".join(text_content)
        elif ext == 'doc':
            return ""
        else:
            raw = file_obj.read()
            enc = chardet.detect(raw).get('encoding') or 'utf-8'
            text = raw.decode(enc, errors='ignore')
    except Exception: return ""
    
    if ext in ['srt', 'vtt', 'ass']:
        clean_text = re.sub(r"<.*?>", "", text)
        clean_text = re.sub(r"\d{2}:\d{2}:\d{2},\d{3} --> \d{2}:\d{2}:\d{2},\d{3}", "", clean_text)
        return clean_text
    return text

def process_words(all_text, mode, min_len, filter_set=None):
    TOKEN_RE = re.compile(r"[A-Za-z-]+")
    cleaned = [re.sub(r'[^a-z]', '', w.lower()) for w in TOKEN_RE.findall(all_text) if w]
    lemmatized = []
    
    if mode == "spacy" and nlp_spacy:
        chunk_size = 50000
        chunks = [cleaned[i:i + chunk_size] for i in range(0, len(cleaned), chunk_size)]
        for chunk in chunks:
            doc = nlp_spacy(" ".join(chunk))
            for token in doc:
                lw = token.lemma_.lower()
                if lw.isalpha() and wordnet.synsets(lw): lemmatized.append(lw)
    else:
        lemmatizer = WordNetLemmatizer()
        for w, tag in pos_tag(cleaned):
            wn = {'J':wordnet.ADJ,'V':wordnet.VERB,'N':wordnet.NOUN,'R':wordnet.ADV}.get(tag[0], None)
            lw = lemmatizer.lemmatize(w, wn) if wn else lemmatizer.lemmatize(w)
            if wordnet.synsets(lw): lemmatized.append(lw)

    seen = set()
    final_words = []
    sys_stopwords = set(stopwords.words('english'))
    for w in lemmatized:
        if len(w) < min_len: continue
        if w in sys_stopwords: continue
        if filter_set and w in filter_set: continue
        if w not in seen:
            seen.add(w)
            final_words.append(w)
    return final_words

# ------------------ 5. UI æ¶æ„ ------------------

with st.sidebar:
    st.image("https://img.icons8.com/fluency/96/dictionary.png", width=64) # ç¨å¾®è°ƒå¤§å›¾æ ‡
    st.markdown("### VocabMaster")
    st.caption("v11.1 Pro Edition")
    st.markdown("---")
    menu = st.radio("é€‰æ‹©åŠŸèƒ½", ["âš¡ åˆ¶ä½œç”Ÿè¯æœ¬", "ğŸŒ å…¬å…±è¯ä¹¦åº“"])
    st.markdown("---")
    st.info("**å°è´´å£«**\nä½¿ç”¨ Spacy å¼•æ“è¿˜åŸè¯å½¢æ›´å‡†ã€‚")

# === åˆ¶ä½œç”Ÿè¯æœ¬ ===
if menu == "âš¡ åˆ¶ä½œç”Ÿè¯æœ¬":
    st.title("âš¡ æ™ºèƒ½ç”Ÿè¯æå–å·¥åŠ")
    
    # èµ„æºå¯¼èˆª
    with st.expander("ğŸ“– æ–°æ‰‹æŒ‡å— & å®è—èµ„æºåº“ (ç‚¹å‡»å±•å¼€)", expanded=False):
        t1, t2, t3, t4 = st.tabs(["ğŸ’¡ æ“ä½œæŒ‡å¼•", "ğŸ¬ å½±è§†å­—å¹•", "ğŸ“š åŸè‘—é˜…è¯»", "ğŸ§ å¬åŠ›ç´ æ"])
        
        with t1:
            st.markdown("""
            <div style="padding:5px;">
            <h5 style="margin-top:0">ğŸš€ å››æ­¥åˆ¶ä½œä¸“å±è¯ä¹¦ï¼š</h5>
            <ol>
                <li><b>å‡†å¤‡ç´ æ</b>ï¼šä»å³ä¾§æ ‡ç­¾é¡µä¸‹è½½ <code>.srt</code> å­—å¹•æˆ– <code>.txt</code> ç”µå­ä¹¦ã€‚</li>
                <li><b>æ¸…æ´—è®¾ç½®</b>ï¼šåœ¨ä¸‹æ–¹ã€è®¾ç½®æå–è§„åˆ™ã€‘ä¸­ï¼Œé€‰æ‹©<b>â€œé¢„ç½®ç†Ÿè¯åº“â€</b>æˆ–ä¸Šä¼ è‡ªå®šä¹‰ç†Ÿè¯è¡¨ï¼ˆéå¸¸é‡è¦ï¼èƒ½å±è”½æ‰ is, the ç­‰ç®€å•è¯ï¼‰ã€‚</li>
                <li><b>æ™ºèƒ½æå–</b>ï¼šå°†æ–‡ä»¶æ‹–å…¥ä¸Šä¼ åŒºï¼ŒAI è‡ªåŠ¨å®Œæˆå»é‡ã€è¯å½¢è¿˜åŸï¼ˆRun/Ran/Running â†’ Runï¼‰ã€‚</li>
                <li><b>é—­ç¯å­¦ä¹ </b>ï¼šç‚¹å‡»ç”Ÿæˆçš„<b>â€œä¸€é”®å¤åˆ¶â€</b>æŒ‰é’®ï¼Œè·³è½¬æ‰‡è´ç½‘æ‰¹é‡åˆ¶å¡ï¼Œæˆ–å¯¼å‡ºè¯ä¹¦ã€‚</li>
            </ol>
            </div>
            """, unsafe_allow_html=True)
            
        with t2:
            st.info("ğŸ’¡ å­—å¹•æ–‡ä»¶æ˜¯æå–å£è¯­è¯æ±‡çš„æœ€ä½³ææ–™ã€‚ä¸‹è½½åæ— éœ€è½¬æ¢ï¼Œç›´æ¥æ‹–å…¥æœ¬å·¥å…·ã€‚")
            c1, c2 = st.columns(2)
            c1.markdown("ğŸ¯ **[ä¼ªå°„æ‰‹ç½‘ (Assrt)](https://assrt.net/)**\n<small>è€ç‰Œç«™ç‚¹ï¼Œèµ„æºæœ€å…¨ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ã€‚</small>", unsafe_allow_html=True)
            c1.markdown("ğŸ“º **[å­—å¹•åº“ (Zimuku)](http://zimuku.org/)**\n<small>ç¾å‰§ã€æ—¥å‰§æ›´æ–°é€Ÿåº¦æå¿«ã€‚</small>", unsafe_allow_html=True)
            c1.markdown("âš¡ **[Addic7ed](https://www.addic7ed.com/)**\n<small>ç¾å‰§ç”Ÿè‚‰æ›´æ–°æœ€å¿«çš„åœ°æ–¹ï¼Œé€‚åˆé«˜é˜¶å­¦ä¹ è€…ã€‚</small>", unsafe_allow_html=True)
            
            c2.markdown("ğŸ’ **[SubHD](https://subhd.tv/)**\n<small>ç•Œé¢æ¸…çˆ½ï¼Œé«˜æ¸…å½±è§†å­—å¹•é¦–é€‰ã€‚</small>", unsafe_allow_html=True)
            c2.markdown("ğŸŒ **[OpenSubtitles](https://www.opensubtitles.org/)**\n<small>å…¨çƒæœ€å¤§å­—å¹•åº“ï¼Œå¯»æ‰¾çº¯è‹±æ–‡å­—å¹•é¦–é€‰ã€‚</small>", unsafe_allow_html=True)
            c2.markdown("ğŸï¸ **[YIFY Subtitles](https://yifysubtitles.ch/)**\n<small>ä¸“é—¨é’ˆå¯¹ç”µå½±çš„é«˜è´¨é‡è‹±æ–‡å­—å¹•ã€‚</small>", unsafe_allow_html=True)

        with t3:
            st.success("ğŸ“š æ¨èä¸‹è½½ .txt æˆ– .epub (éœ€è½¬txt) æ ¼å¼ã€‚")
            c1, c2 = st.columns(2)
            c1.markdown("ğŸ›ï¸ **[Project Gutenberg](https://www.gutenberg.org/)**\n<small>æ‹¥æœ‰7ä¸‡+å…è´¹å…¬ç‰ˆç”µå­ä¹¦ï¼Œè‹±æ–‡åŸè‘—çš„å¤§å®åº“ã€‚</small>", unsafe_allow_html=True)
            c1.markdown("ğŸ“– **[ManyBooks](https://manybooks.net/)**\n<small>æ’ç‰ˆç²¾ç¾ï¼Œåˆ†ç±»è¯¦ç»†ï¼Œä¸‹è½½ä½“éªŒå¥½ã€‚</small>", unsafe_allow_html=True)
            
            c2.markdown("ğŸ“° **[Global Times](https://www.globaltimes.cn/)**\n<small>å›½äº§è‹±æ–‡åª’ä½“ï¼Œç”¨è¯è´´è¿‘æ—¶æ”¿ï¼Œé€‚åˆå¤‡è€ƒã€‚</small>", unsafe_allow_html=True)
            c2.markdown("ğŸ§  **[Scientific American](https://www.scientificamerican.com/)**\n<small>é«˜é˜¶ç§‘æ™®æ–‡ç« ï¼Œæ‰˜ç¦/é›…æ€/GRE é˜…è¯»åŒæºç´ æã€‚</small>", unsafe_allow_html=True)

        with t4:
            st.warning("ğŸ§ æŠ€å·§ï¼šä¸‹è½½ Transcript (æ–‡ç¨¿) æå–å•è¯ï¼Œå­¦å®Œå†å»å¬ã€‚")
            c1, c2 = st.columns(2)
            c1.markdown("ğŸ”´ **[TED Talks](https://www.ted.com/)**\n<small>æ€æƒ³ç››å®´ï¼Œæ¯ä¸ªè§†é¢‘éƒ½è‡ªå¸¦å¤šè¯­è¨€æ–‡ç¨¿ã€‚</small>", unsafe_allow_html=True)
            c1.markdown("ğŸ‡ºğŸ‡¸ **[VOA Learning English](https://learningenglish.voanews.com/)**\n<small>ç»å…¸åˆ†çº§å¬åŠ›ææ–™ï¼Œå«çº¯æ­£æ–‡ç¨¿ã€‚</small>", unsafe_allow_html=True)
            
            c2.markdown("ğŸ‡¬ğŸ‡§ **[BBC Learning English](https://www.bbc.co.uk/learningenglish/)**\n<small>è‹±å¼è‹±è¯­é‡‘ç‰Œæ•™ç¨‹ï¼Œ6 Minute English å¿…å¬ã€‚</small>", unsafe_allow_html=True)
            c2.markdown("ğŸ“ **[Coursera](https://www.coursera.org/)**\n<small>å­¦ä¹ ä¸“ä¸šè¯¾ï¼ˆè®¡ç®—æœº/å•†ç§‘ï¼‰çš„æœ€å¥½æ–¹å¼ã€‚</small>", unsafe_allow_html=True)

    if 'result_words' not in st.session_state: st.session_state.result_words = []
    if 'source_files_count' not in st.session_state: st.session_state.source_files_count = 0
    
    # æ“ä½œåŒº
    c_config, c_upload = st.columns([1, 2], gap="large")
    
    with c_config:
        st.markdown('<div class="step-header">1ï¸âƒ£ è®¾ç½®æå–è§„åˆ™</div>', unsafe_allow_html=True)
        with st.container(border=True):
            nlp_mode = st.selectbox("AI å¼•æ“", ["nltk (å¿«é€Ÿ)", "spacy (ç²¾å‡†)"])
            mode_key = "spacy" if "spacy" in nlp_mode else "nltk"
            
            min_len = st.number_input("å•è¯æœ€çŸ­é•¿åº¦", 3, 20, 3)
            
            st.markdown("---")
            sort_order = st.selectbox("ğŸ”€ å•è¯æ’åº", ["æŒ‰æ–‡æœ¬å‡ºç°é¡ºåº", "A-Z æ’åº", "éšæœºæ‰“ä¹±"])
            chunk_size = st.number_input("ğŸ“¥ æ–‡ä»¶æ‹†åˆ†å¤§å° (è¯/æ–‡ä»¶)", 5000, 50000, 5000, step=1000)
            
            # --- æ”¹åŠ¨å¼€å§‹ï¼šæ–°çš„ç†Ÿè¯å±è”½åŒºåŸŸ ---
            st.markdown("---")
            st.markdown("##### ğŸ›¡ï¸ ç†Ÿè¯å±è”½è®¾ç½®")
            
            # 1. é¢„ç½®è¯åº“å¤šé€‰
            selected_presets = st.multiselect(
                "é€‰æ‹©é¢„ç½®ç†Ÿè¯åº“ (å¯å¤šé€‰, å åŠ ç”Ÿæ•ˆ)",
                options=list(PRESET_WORDLISTS.keys()),
                default=[],
                help="é€‰æ‹©ä½ å·²ç»æŒæ¡çš„è¯æ±‡ç­‰çº§ï¼Œè¿™äº›è¯å°†ä¸ä¼šå‡ºç°åœ¨æœ€ç»ˆç»“æœä¸­ã€‚"
            )
            
            # 2. è‡ªå®šä¹‰ä¸Šä¼ 
            filter_file = st.file_uploader("ä¸Šä¼ è‡ªå®šä¹‰ç†Ÿè¯è¡¨ (.txt)", type=['txt'], help="å¦‚æœä½ æœ‰è‡ªå·±çš„ä¸“å±è¯è¡¨ï¼Œå¯ä»¥åœ¨è¿™é‡Œä¸Šä¼ ï¼Œå°†ä¸é¢„ç½®è¯åº“å åŠ ã€‚")
            
            # 3. åˆå¹¶è¿‡æ»¤è¯é›†åˆ
            filter_set = set()
            # å¤„ç†é¢„ç½®è¯åº“
            for preset_name in selected_presets:
                file_path = PRESET_WORDLISTS[preset_name]
                if os.path.exists(file_path):
                    try:
                        with open(file_path, "r", encoding="utf-8") as f:
                            # è¯»å–æ–‡ä»¶ï¼Œå»é™¤é¦–å°¾ç©ºæ ¼ï¼Œè½¬å°å†™ï¼ŒåŠ å…¥é›†åˆ
                            words_in_file = set(l.strip().lower() for l in f if l.strip())
                            filter_set.update(words_in_file)
                    except Exception as e:
                         st.warning(f"è¯»å–è¯åº“ {preset_name} å¤±è´¥: {e}")
                else:
                     st.warning(f"æ‰¾ä¸åˆ°è¯åº“æ–‡ä»¶: {file_path}")

            # å¤„ç†è‡ªå®šä¹‰ä¸Šä¼ 
            if filter_file:
                c = filter_file.getvalue().decode("utf-8", errors='ignore')
                custom_words = set(l.strip().lower() for l in c.splitlines() if l.strip())
                filter_set.update(custom_words)
                
            # æ˜¾ç¤ºåé¦ˆ
            if filter_set:
                st.caption(f"âœ… å·²å¯ç”¨å±è”½ï¼Œå…±è®¡ {len(filter_set)} ä¸ªç†Ÿè¯ã€‚")
            else:
                 st.caption("â„¹ï¸ æœªå¯ç”¨ä»»ä½•ç†Ÿè¯å±è”½ã€‚")
            # --- æ”¹åŠ¨ç»“æŸ ---

    with c_upload:
        st.markdown('<div class="step-header">2ï¸âƒ£ ä¸Šä¼ ä¸åˆ†æ</div>', unsafe_allow_html=True)
        with st.container(border=True):
            uploaded_files = st.file_uploader("æ”¯æŒ .srt, .docx, .txt", type=['txt','srt','ass','docx'], accept_multiple_files=True)
            st.markdown("<br>", unsafe_allow_html=True)
            if uploaded_files and st.button("ğŸš€ å¼€å§‹æå–", type="primary", use_container_width=True):
                my_bar = st.progress(0, text="è¯»å–æ–‡ä»¶...")
                all_text = []
                for i, f in enumerate(uploaded_files):
                    all_text.append(extract_text_from_bytes(f, f.name))
                    my_bar.progress((i+1)/len(uploaded_files))
                
                full_text = "\n".join(all_text)
                if full_text.strip():
                    my_bar.progress(100, text="AI åˆ†æä¸­ (å¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´)...")
                    # å°†åˆå¹¶åçš„ filter_set ä¼ é€’ç»™å¤„ç†å‡½æ•°
                    words = process_words(full_text, mode_key, min_len, filter_set)
                    
                    # åº”ç”¨æ’åº
                    if sort_order == "A-Z æ’åº": words.sort()
                    elif sort_order == "éšæœºæ‰“ä¹±": random.shuffle(words)
                    
                    st.session_state.result_words = words
                    st.session_state.source_files_count = len(uploaded_files)
                    my_bar.empty()
                    st.rerun()
                else:
                    st.error("æœªæå–åˆ°æ–‡æœ¬ã€‚")

    # ç»“æœåŒº
    if st.session_state.result_words:
        st.divider()
        st.markdown('<div class="step-header">3ï¸âƒ£ ç»“æœä¸å¯¼å‡º</div>', unsafe_allow_html=True)
        words = st.session_state.result_words
        content_str = "\n".join(words)
        
        with st.container(border=True):
            c1, c2, c3 = st.columns(3)
            c1.metric("å•è¯æ•°", len(words))
            c2.metric("æ’åº", sort_order)
            c3.metric("æ¥æº", f"{st.session_state.source_files_count} æ–‡ä»¶")
            
        col_copy, col_act = st.columns([2, 1], gap="large")
        
        with col_copy:
            st.markdown("##### ğŸ“‹ å•è¯åˆ—è¡¨ (ä¸€é”®å¤åˆ¶)")
            render_copy_button(content_str, "res_copy")
            st.code(content_str, language="text") # å¤‡ç”¨å±•ç¤º
            
        with col_act:
            st.markdown("##### ğŸš€ æ“ä½œ")
            st.markdown(
                """<a href="https://web.shanbay.com/wordsweb/#/books" target="_blank" kind="primary">ğŸ¦ å¯¼å…¥æ‰‡è´ (Web)</a>""", 
                unsafe_allow_html=True
            )
            st.markdown("<div style='height:15px'></div>", unsafe_allow_html=True)
            
            # ZIP ä¸‹è½½
            zip_buffer = io.BytesIO()
            num_files = math.ceil(len(words) / chunk_size)
            with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                for i in range(num_files):
                    s = i * chunk_size
                    e = min(s + chunk_size, len(words))
                    zf.writestr(f"word_list_{i+1}.txt", "\n".join(words[s:e]))
            st.download_button(f"ğŸ“¦ ä¸‹è½½ ZIP ({num_files}ä¸ª)", zip_buffer.getvalue(), "vocab.zip", "application/zip", use_container_width=True)
            
            st.markdown("---")
            with st.expander("â˜ï¸ å‘å¸ƒ"):
                with st.form("pub"):
                    name = st.text_input("æ–‡ä»¶å(.txt)", value=f"v_{int(time.time())}.txt")
                    title = st.text_input("æ ‡é¢˜")
                    desc = st.text_area("ç®€ä»‹")
                    if st.form_submit_button("æäº¤"):
                        if name.endswith(".txt"): save_to_github_library(name, content_str, title, desc)

# === å…¬å…±è¯ä¹¦åº“ ===
elif menu == "ğŸŒ å…¬å…±è¯ä¹¦åº“":
    st.title("ğŸŒ ç¤¾åŒºå…¬å…±è¯ä¹¦åº“")
    st.markdown("<div class='info-box'>æ±‡é›†ç²¾é€‰è¯ä¹¦ã€‚ç‚¹å‡»ä¸‹æ–¹<b>â€œå±•å¼€â€</b>æŒ‰é’®æŸ¥çœ‹è¯¦æƒ…å¹¶å¤åˆ¶ã€‚</div>", unsafe_allow_html=True)
    
    search_q = st.text_input("ğŸ” æœç´¢...", "").lower()
    
    LIBRARY_DIR = "library"
    if not os.path.exists(LIBRARY_DIR): os.makedirs(LIBRARY_DIR)
    
    try:
        with open(os.path.join(LIBRARY_DIR, "info.json"), "r", encoding="utf-8") as f: book_info = json.load(f)
    except: book_info = {}
    
    files = [f for f in os.listdir(LIBRARY_DIR) if f.endswith(".txt")]
    visible = [f for f in files if search_q in f.lower() or search_q in book_info.get(f, {}).get("title", "").lower()]
    
    if not visible:
        st.warning("ğŸ“­ æš‚æ— æ•°æ®")
    else:
        cols = st.columns(3)
        for i, f in enumerate(visible):
            path = os.path.join(LIBRARY_DIR, f)
            try:
                with open(path, "r", encoding="utf-8") as file: content = file.read()
                count = len(content.splitlines())
                meta = book_info.get(f, {})
                title = meta.get("title", f)
                desc = meta.get("desc", "æš‚æ— æè¿°")
                
                with cols[i % 3]:
                    with st.container(border=True):
                        st.subheader(f"ğŸ“„ {title}")
                        st.caption(f"ğŸ“ {count} è¯")
                        
                        # æ“ä½œæŒ‰é’®åŒº
                        c1, c2 = st.columns(2)
                        with c1: 
                             st.markdown(
                                """<a href="https://web.shanbay.com/wordsweb/#/books" target="_blank" kind="primary" style="font-size:0.8rem; padding:0.4rem;">ğŸš€ å¯¼å…¥</a>""", 
                                unsafe_allow_html=True
                            )
                        with c2: st.download_button("â¬‡ï¸ ä¸‹è½½", content, f, "text/plain", use_container_width=True)
                        
                        with st.expander("ğŸ‘€ å±•å¼€æŸ¥çœ‹ä¸å¤åˆ¶"):
                            st.caption(desc)
                            render_copy_button(content, f"lib_copy_{i}")
                            st.code(content, language="text")
            except: continue

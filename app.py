import streamlit as st
import io
import re
import zipfile
import math
import chardet
import os
import json
import random
from github import Github

# NLP Imports
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet, stopwords
from nltk import pos_tag
from docx import Document

# Optional Spacy
try:
    import spacy
    _HAS_SPACY = True
except ImportError:
    _HAS_SPACY = False

# ------------------ é¡µé¢é…ç½® ------------------
st.set_page_config(
    page_title="ä¸‡èƒ½è¯ä¹¦å¹³å°", 
    page_icon="ğŸ“˜", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰ CSS ä¼˜åŒ–æ ·å¼
st.markdown("""
<style>
    .stTabs [data-baseweb="tab-list"] { gap: 2px; }
    .stTabs [data-baseweb="tab"] { height: 50px; white-space: pre-wrap; background-color: #f0f2f6; border-radius: 4px 4px 0px 0px; gap: 1px; padding-top: 10px; padding-bottom: 10px; }
    .stTabs [aria-selected="true"] { background-color: #ffffff; border-top: 2px solid #ff4b4b; }
    .metric-card { background-color: #f9f9f9; border: 1px solid #e0e0e0; padding: 15px; border-radius: 8px; text-align: center; }
</style>
""", unsafe_allow_html=True)

# ------------------ ç¼“å­˜èµ„æºåŠ è½½ ------------------
@st.cache_resource
def download_nltk_resources():
    resources = ["punkt", "averaged_perceptron_tagger", "averaged_perceptron_tagger_eng", "wordnet", "omw-1.4", "stopwords"]
    for r in resources:
        try:
            nltk.data.find(f'tokenizers/{r}')
        except (LookupError, ValueError):
            nltk.download(r, quiet=True)

@st.cache_resource
def load_spacy_model():
    if _HAS_SPACY:
        try:
            return spacy.load("en_core_web_sm", disable=["ner", "parser"])
        except Exception:
            return None
    return None

download_nltk_resources()
nlp_spacy = load_spacy_model()

# ------------------ æ ¸å¿ƒé€»è¾‘å‡½æ•° (ä¿æŒä¸å˜) ------------------
def save_to_github_library(filename, content, title, desc):
    try:
        token = st.secrets["GITHUB_TOKEN"]
        username = st.secrets["GITHUB_USERNAME"]
        repo_name = st.secrets["GITHUB_REPO"]
        
        g = Github(token)
        repo = g.get_repo(f"{username}/{repo_name}")
        
        library_path = f"library/{filename}"
        info_path = "library/info.json"
        
        try:
            contents = repo.get_contents(library_path)
            repo.update_file(library_path, f"Update {filename}", content, contents.sha)
            st.toast(f"æ–‡ä»¶ {filename} å·²æ›´æ–°ï¼", icon="âœ…")
        except:
            repo.create_file(library_path, f"Create {filename}", content)
            st.toast(f"æ–‡ä»¶ {filename} å·²åˆ›å»ºï¼", icon="âœ…")

        try:
            info_contents = repo.get_contents(info_path)
            info_data = json.loads(info_contents.decoded_content.decode("utf-8"))
        except:
            info_data = {}
            info_contents = None

        info_data[filename] = {
            "title": title,
            "desc": desc
        }
        
        new_info_str = json.dumps(info_data, indent=2, ensure_ascii=False)
        if info_contents:
            repo.update_file(info_path, "Update info.json", new_info_str, info_contents.sha)
        else:
            repo.create_file(info_path, "Create info.json", new_info_str)
            
        st.balloons()
        st.success(f"ğŸ‰ æˆåŠŸä¿å­˜åˆ°äº‘ç«¯ï¼è¯·åˆ·æ–°é¡µé¢æŸ¥çœ‹â€œå…¬å…±è¯ä¹¦åº“â€ã€‚")
        
    except Exception as e:
        st.error(f"ä¸Šä¼ å¤±è´¥: {e}")
        st.warning("è¯·æ£€æŸ¥ .streamlit/secrets.toml é…ç½®æ˜¯å¦æ­£ç¡®ã€‚")

def extract_text_from_bytes(file_obj, filename):
    if '.' in filename: ext = filename.split('.')[-1].lower()
    else: ext = 'txt'
    text = ""
    try:
        if ext == 'docx':
            doc = Document(file_obj)
            text_content = []
            for p in doc.paragraphs:
                if p.text.strip(): text_content.append(p.text)
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if cell.text.strip(): text_content.append(cell.text)
            text = "\n".join(text_content)
        elif ext == 'doc':
            st.error(f"ä¸æ”¯æŒ .doc æ ¼å¼ ({filename})ï¼Œè¯·è½¬å­˜ä¸º .docx")
            return ""
        else:
            raw = file_obj.read()
            enc = chardet.detect(raw).get('encoding') or 'utf-8'
            text = raw.decode(enc, errors='ignore')
    except Exception as e: return ""
    
    if ext in ['srt', 'vtt', 'ass']:
        clean_text = re.sub(r"<.*?>", "", text)
        clean_text = re.sub(r"\{.*?\}", "", clean_text) # Remove ASS tags
        return clean_text
    return text

def process_words(all_text, mode, min_len, filter_set=None):
    TOKEN_RE = re.compile(r"[A-Za-z-]+")
    cleaned = [re.sub(r'[^a-z]', '', w.lower()) for w in TOKEN_RE.findall(all_text) if w]
    lemmatized = []
    
    # æ¨¡æ‹Ÿè¿›åº¦æ¡éœ€è¦å¤–éƒ¨ä¼ å…¥ callbackï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
    if mode == "spacy" and nlp_spacy:
        chunk_size = 50000
        chunks = [cleaned[i:i + chunk_size] for i in range(0, len(cleaned), chunk_size)]
        for chunk in chunks:
            doc = nlp_spacy(" ".join(chunk))
            for token in doc:
                lw = token.lemma_.lower()
                if lw.isalpha() and wordnet.synsets(lw): lemmatized.append(lw)
    else:
        lemmatizer = WordNetLemmatizer()
        for w, tag in pos_tag(cleaned):
            wn = {'J':wordnet.ADJ,'V':wordnet.VERB,'N':wordnet.NOUN,'R':wordnet.ADV}.get(tag[0], None)
            lw = lemmatizer.lemmatize(w, wn) if wn else lemmatizer.lemmatize(w)
            if wordnet.synsets(lw): lemmatized.append(lw)

    seen = set()
    final_words = []
    sys_stopwords = set(stopwords.words('english'))
    for w in lemmatized:
        if len(w) < min_len: continue
        if w in sys_stopwords: continue
        if filter_set and w in filter_set: continue
        if w not in seen:
            seen.add(w)
            final_words.append(w)
    return final_words

# ------------------ UI é€»è¾‘ ------------------

# ä¾§è¾¹æ å¯¼èˆª
with st.sidebar:
    st.title("ğŸ“˜ ä¸‡èƒ½è¯ä¹¦")
    page = st.radio("åŠŸèƒ½å¯¼èˆª", ["ğŸ› ï¸ åˆ¶ä½œç”Ÿè¯æœ¬", "ğŸ“š å…¬å…±è¯ä¹¦åº“"], label_visibility="collapsed")
    st.divider()
    st.caption("Version 2.0 | Power by NLP")

if page == "ğŸ› ï¸ åˆ¶ä½œç”Ÿè¯æœ¬":
    st.markdown("## ğŸ› ï¸ è‹±è¯­ç”Ÿè¯æå–å™¨")
    st.info("ğŸ’¡ ä¸Šä¼ å­—å¹•æˆ–æ–‡æ¡£ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨å»é™¤ç®€å•è¯ã€è¿˜åŸè¯å½¢ï¼Œç”Ÿæˆä½ çš„ä¸“å±å•è¯ä¹¦ã€‚")

    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.subheader("1. æå–è®¾ç½®")
        with st.expander("âš™ï¸ é«˜çº§é…ç½® (ç‚¹å‡»å±•å¼€)", expanded=True):
            nlp_mode = st.selectbox("NLP å¼•æ“", ["nltk (é€Ÿåº¦å¿«)", "spacy (ç²¾åº¦é«˜)"], help="Spacy å¯¹è¯æ€§è¿˜åŸæ›´å‡†ç¡®ï¼Œä½†é€Ÿåº¦ç¨æ…¢ã€‚")
            mode_key = "spacy" if "spacy" in nlp_mode else "nltk"
            min_len = st.slider("æœ€çŸ­å•è¯é•¿åº¦", 2, 10, 3)
            
            st.divider()
            st.write("ğŸš« **è¿‡æ»¤è¯è¡¨** (å¯é€‰)")
            filter_file = st.file_uploader("ä¸Šä¼ ç†Ÿè¯è¡¨ (txt)", type=['txt'], label_visibility="collapsed")
            filter_set = set()
            if filter_file:
                c = filter_file.getvalue().decode("utf-8", errors='ignore')
                filter_set = set(l.strip().lower() for l in c.splitlines() if l.strip())
                st.success(f"âœ… å·²åŠ è½½ {len(filter_set)} ä¸ªè¿‡æ»¤è¯")

    with col2:
        st.subheader("2. ä¸Šä¼ ä¸å¤„ç†")
        uploaded_files = st.file_uploader("æ‹–æ‹½æˆ–ç‚¹å‡»ä¸Šä¼ æ–‡ä»¶ (æ”¯æŒ .srt, .docx, .txt ç­‰)", type=['txt','srt','ass','vtt','docx'], accept_multiple_files=True)
        
        process_btn = st.button("ğŸš€ å¼€å§‹æ™ºèƒ½æå–", type="primary", use_container_width=True, disabled=not uploaded_files)

    if 'result_words' not in st.session_state:
        st.session_state.result_words = []

    if process_btn and uploaded_files:
        all_raw_text = []
        
        # ä½¿ç”¨ status ç»„ä»¶æ˜¾ç¤ºè¯¦ç»†çŠ¶æ€
        with st.status("æ­£åœ¨å¤„ç†æ–‡ä»¶ä¸­...", expanded=True) as status:
            st.write("ğŸ“– è¯»å–æ–‡ä»¶å†…å®¹...")
            for file in uploaded_files:
                text = extract_text_from_bytes(file, file.name)
                all_raw_text.append(text)
            
            full_text = "\n".join(all_raw_text)
            
            if full_text.strip():
                st.write(f"ğŸ§  è°ƒç”¨ {mode_key.upper()} å¼•æ“è¿›è¡Œè‡ªç„¶è¯­è¨€åˆ†æ...")
                words = process_words(full_text, mode_key, min_len, filter_set)
                st.session_state.result_words = words # é»˜è®¤æš‚ä¸æ’åºï¼Œä¿ç•™æå–é¡ºåº
                status.update(label="âœ… æå–å®Œæˆï¼", state="complete", expanded=False)
            else:
                status.update(label="âŒ æœªæå–åˆ°æ–‡æœ¬", state="error")
                st.warning("è¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹æ˜¯å¦ä¸ºç©ºã€‚")

    # ç»“æœå±•ç¤ºåŒº
    if st.session_state.result_words:
        st.divider()
        result_words = st.session_state.result_words
        
        # é¡¶éƒ¨æ•°æ®æŒ‡æ ‡
        m1, m2, m3 = st.columns(3)
        m1.metric("æå–å•è¯æ€»æ•°", len(result_words))
        m2.metric("è¿‡æ»¤è¯æ•°", len(filter_set) if filter_set else 0)
        m3.metric("é¢„ä¼°æŒæ¡ç”¨æ—¶", f"{math.ceil(len(result_words)/30)} å¤©", help="æŒ‰æ¯å¤©èƒŒ30ä¸ªå•è¯è®¡ç®—")
        
        # æ“ä½œåŒºåŸŸ Tab åˆ†é¡µ
        tab1, tab2, tab3 = st.tabs(["ğŸ‘€ åˆ—è¡¨é¢„è§ˆ", "ğŸ“¥ æœ¬åœ°å¯¼å‡º", "â˜ï¸ å‘å¸ƒåˆ°äº‘ç«¯"])
        
        with tab1:
            # è½¬æ¢ä¸º DataFrame æ–¹ä¾¿å±•ç¤º
            import pandas as pd
            df_words = pd.DataFrame(result_words, columns=["Words"])
            st.dataframe(df_words, use_container_width=True, height=300)

        with tab2:
            st.subheader("å¯¼å‡ºé€‰é¡¹")
            c1, c2 = st.columns(2)
            with c1:
                sort_order = st.radio("æ’åºæ–¹å¼", ["æŒ‰æ–‡æœ¬é¡ºåº", "A-Z æ’åº", "éšæœºæ‰“ä¹± (æ¨èå¤ä¹ )"])
            with c2:
                chunk_size = st.number_input("å•æ–‡ä»¶å•è¯ä¸Šé™", value=5000, step=1000)
            
            # ä¸´æ—¶åº”ç”¨æ’åº
            export_words = result_words.copy()
            if sort_order == "A-Z æ’åº":
                export_words.sort()
            elif sort_order == "éšæœºæ‰“ä¹± (æ¨èå¤ä¹ )":
                random.shuffle(export_words)

            zip_buffer = io.BytesIO()
            num_files = math.ceil(len(export_words) / chunk_size)
            with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                for i in range(num_files):
                    s = i * chunk_size
                    e = min(s + chunk_size, len(export_words))
                    zf.writestr(f"word_list_{i+1}.txt", "\n".join(export_words[s:e]))
            
            st.download_button(
                label=f"ğŸ“¥ ä¸‹è½½ç”Ÿè¯æœ¬ (ZIP, å…±{num_files}ä¸ªæ–‡ä»¶)", 
                data=zip_buffer.getvalue(), 
                file_name="my_vocab_book.zip", 
                mime="application/zip",
                type="primary"
            )

        with tab3:
            col_cloud_form, col_cloud_tips = st.columns([2, 1])
            with col_cloud_form:
                with st.form("upload_form"):
                    st.write("å¡«å†™è¯ä¹¦ä¿¡æ¯")
                    save_name = st.text_input("æ–‡ä»¶å (e.g. harry_potter.txt)", value="new_book.txt")
                    save_title = st.text_input("æ ‡é¢˜", value="æˆ‘çš„ç”Ÿè¯æœ¬")
                    save_desc = st.text_area("ç®€ä»‹", value="æå–è‡ª...")
                    submitted = st.form_submit_button("ğŸš€ ç¡®è®¤å‘å¸ƒ")
                    
                    if submitted:
                        if not save_name.endswith(".txt"):
                            st.error("æ–‡ä»¶åå¿…é¡»ä»¥ .txt ç»“å°¾")
                        else:
                            # é»˜è®¤å‘å¸ƒå‰æŒ‰ A-Z æ’åºï¼Œæ¯”è¾ƒæ•´é½
                            final_content = sorted(list(set(result_words)))
                            content_str = "\n".join(final_content)
                            with st.spinner("æ­£åœ¨è¿æ¥ GitHub..."):
                                save_to_github_library(save_name, content_str, save_title, save_desc)
            with col_cloud_tips:
                st.info("â„¹ï¸ è¯´æ˜ï¼š\nå‘å¸ƒåï¼Œè¯¥è¯ä¹¦å°†å‡ºç°åœ¨â€œå…¬å…±è¯ä¹¦åº“â€ä¸­ï¼Œæ‰€æœ‰äººå‡å¯ä¸‹è½½ã€‚\nè¯·ç¡®ä¿å†…å®¹ä¸åŒ…å«æ•æ„Ÿä¿¡æ¯ã€‚")

elif page == "ğŸ“š å…¬å…±è¯ä¹¦åº“":
    st.markdown("## ğŸ“š å…¬å…±è¯ä¹¦åº“")
    st.caption("è¿™é‡Œæ±‡èšäº†å¤§å®¶åˆ†äº«çš„ä¼˜è´¨ç”Ÿè¯æœ¬ï¼Œç‚¹å‡»å³å¯å…è´¹ä¸‹è½½ã€‚")
    
    col_search, _ = st.columns([1, 2])
    search_query = col_search.text_input("ğŸ” æœç´¢è¯ä¹¦", placeholder="è¾“å…¥æ ‡é¢˜æˆ–æè¿°å…³é”®å­—...").lower()
    
    LIBRARY_DIR = "library"
    INFO_FILE = "info.json"
    
    if not os.path.exists(LIBRARY_DIR):
        os.makedirs(LIBRARY_DIR)
    
    book_info = {}
    info_path = os.path.join(LIBRARY_DIR, INFO_FILE)
    if os.path.exists(info_path):
        try:
            with open(info_path, "r", encoding="utf-8") as f: book_info = json.load(f)
        except: pass

    files = [f for f in os.listdir(LIBRARY_DIR) if f.endswith(".txt")]
    
    # è¿‡æ»¤æœç´¢
    filtered_files = []
    for f in files:
        meta = book_info.get(f, {})
        title = meta.get("title", f).lower()
        desc = meta.get("desc", "").lower()
        if search_query in title or search_query in desc or search_query in f.lower():
            filtered_files.append(f)

    if not filtered_files:
        st.warning("ğŸ“­ æš‚æ— ç›¸å…³è¯ä¹¦ï¼Œå¿«å»â€œåˆ¶ä½œç”Ÿè¯æœ¬â€é‡Œä¸Šä¼ ä¸€æœ¬å§ï¼")
    else:
        # ä½¿ç”¨ Grid å¸ƒå±€ (æ¯è¡Œ3ä¸ª)
        cols = st.columns(3)
        for i, filename in enumerate(filtered_files):
            file_path = os.path.join(LIBRARY_DIR, filename)
            with open(file_path, "r", encoding="utf-8") as f: file_content = f.read()
            word_count = len(file_content.splitlines())
            
            meta = book_info.get(filename, {})
            display_title = meta.get("title", filename)
            display_desc = meta.get("desc", "æš‚æ— æè¿°")
            
            # ä½¿ç”¨ Container æ¨¡æ‹Ÿå¡ç‰‡æ ·å¼
            with cols[i % 3]:
                with st.container(border=True):
                    st.subheader(f"ğŸ“„ {display_title}")
                    st.caption(f"æ–‡ä»¶å: {filename}")
                    st.text(f"ğŸ“Š å•è¯é‡: {word_count}")
                    
                    # é™åˆ¶æè¿°æ–‡å­—é«˜åº¦ï¼Œé˜²æ­¢å¡ç‰‡å‚å·®ä¸é½
                    if len(display_desc) > 50:
                        short_desc = display_desc[:50] + "..."
                        st.markdown(f"<div style='height:45px; overflow:hidden; color:gray; font-size:0.9em'>{short_desc}</div>", unsafe_allow_html=True)
                    else:
                        st.markdown(f"<div style='height:45px; color:gray; font-size:0.9em'>{display_desc}</div>", unsafe_allow_html=True)
                    
                    st.download_button(
                        f"ğŸ“¥ ä¸‹è½½", 
                        file_content, 
                        filename, 
                        "text/plain", 
                        key=f"dl_{i}",
                        use_container_width=True
                    )

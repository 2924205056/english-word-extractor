è¿™æ˜¯ä¸€ä¸ªéå¸¸å…³é”®çš„ä¿®å¤ã€‚

**ä¸ºä»€ä¹ˆä¹‹å‰çš„ä»£ç æ— æ³•è¯»å– Library äº†ï¼Ÿ**
åŸå› æ˜¯åœ¨ä¸Šä¸€æ¬¡ UI é‡æ„ï¼ˆå˜ä¸º Teal/Slate é£æ ¼ï¼‰æ—¶ï¼Œä¸ºäº†å±•ç¤º 3D ä¹¦ç±çš„è§†è§‰æ•ˆæœï¼Œæˆ‘æš‚æ—¶ä½¿ç”¨äº†\*\*é™æ€çš„æ¨¡æ‹Ÿæ•°æ®ï¼ˆMock Dataï¼‰\*\*æ›¿æ¢äº†åŸæœ¬è¯»å–æœ¬åœ°/GitHub æ–‡ä»¶çš„é€»è¾‘ã€‚

ä¸‹é¢çš„ä»£ç **å®Œç¾èåˆ**äº†ä¸‰ä¸ªéƒ¨åˆ†ï¼š

1.  **æ–° UI é£æ ¼**ï¼ˆTeal/Slate + 3D ä¹¦ç±ï¼‰ã€‚
2.  **æ–°æ‰‹æŒ‡å¼•æ¨¡å—**ï¼ˆä½ æä¾›çš„ä»£ç ï¼‰ã€‚
3.  **åŠ¨æ€é€»è¾‘å›å½’**ï¼ˆé‡æ–°è¯»å– `library` æ–‡ä»¶å¤¹ï¼Œå¹¶æ”¯æŒä¸Šä¼ åˆ° GitHubï¼‰ã€‚

è¯·ä½¿ç”¨ä»¥ä¸‹å®Œæ•´ä»£ç è¦†ç›– `app.py`ï¼š

```python
import streamlit as st
import io
import re
import zipfile
import math
import chardet
import os
import json
import random
import time
import streamlit.components.v1 as components
from github import Github

# NLP Imports
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet, stopwords
from nltk import pos_tag
from docx import Document

# Optional Spacy
try:
    import spacy
    _HAS_SPACY = True
except ImportError:
    _HAS_SPACY = False

# ------------------ 0. åˆå§‹åŒ– & èµ„æºåŠ è½½ ------------------
WORDLIST_DIR = "wordlists"
LIBRARY_DIR = "library"

# ç¡®ä¿ç›®å½•å­˜åœ¨
for d in [WORDLIST_DIR, LIBRARY_DIR]:
    if not os.path.exists(d): os.makedirs(d)

# åˆ›å»ºæ¼”ç¤ºæ•°æ®
if not os.path.exists(os.path.join(WORDLIST_DIR, "primary.txt")):
    with open(os.path.join(WORDLIST_DIR, "primary.txt"), "w", encoding="utf-8") as f:
        f.write("a\nan\nthe\nis\nare\nam\nhello\ngood\nbook\npen")

PRESET_WORDLISTS = {
    "ğŸ‘¶ å°å­¦æ ¸å¿ƒè¯": os.path.join(WORDLIST_DIR, "primary.txt"),
    "ğŸ‘¦ ä¸­è€ƒå¿…å¤‡è¯": os.path.join(WORDLIST_DIR, "zhongkao.txt"),
    "ğŸ‘¨â€ğŸ“ é«˜è€ƒ3500è¯": os.path.join(WORDLIST_DIR, "gaokao.txt"),
}

@st.cache_resource
def download_nltk_resources():
    resources = ["punkt", "averaged_perceptron_tagger", "averaged_perceptron_tagger_eng", "wordnet", "omw-1.4", "stopwords"]
    for r in resources:
        try: nltk.data.find(f'tokenizers/{r}')
        except LookupError: nltk.download(r, quiet=True)
        except ValueError: nltk.download(r, quiet=True)

@st.cache_resource
def load_spacy_model():
    if _HAS_SPACY:
        try: return spacy.load("en_core_web_sm", disable=["ner", "parser"])
        except Exception: return None
    return None

download_nltk_resources()
nlp_spacy = load_spacy_model()

# ------------------ 1. é¡µé¢é…ç½® & CSS è®¾è®¡ç³»ç»Ÿ ------------------
st.set_page_config(
    page_title="VocabMaster", 
    page_icon="âš¡", 
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Plus+Jakarta+Sans:wght@400;500;600;700;800&family=Noto+Sans+SC:wght@400;500;700&display=swap');

    .stApp { background-color: #F8FAFC; font-family: 'Plus Jakarta Sans', 'Noto Sans SC', sans-serif; color: #1e293b; }
    h1, h2, h3, h4 { font-family: 'Plus Jakarta Sans', sans-serif; font-weight: 700; color: #0f172a; }
    
    /* ä¾§è¾¹æ  */
    section[data-testid="stSidebar"] { background-color: #ffffff; border-right: 1px solid #f1f5f9; }
    section[data-testid="stSidebar"] > div { padding-top: 2rem; }

    /* å¡ç‰‡å®¹å™¨ */
    div[data-testid="stVerticalBlock"] > div[data-testid="stVerticalBlockBorderWrapper"] > div {
        border: 1px solid #e2e8f0; border-radius: 16px; background-color: #ffffff;
        box-shadow: 0 4px 20px -2px rgba(0, 0, 0, 0.05); padding: 24px;
    }

    /* æŒ‰é’®æ ·å¼ */
    div.stButton > button[kind="primary"] {
        background: linear-gradient(135deg, #2DD4BF 0%, #0F766E 100%); color: white; border: none;
        border-radius: 12px; padding: 0.6rem 1.5rem; font-weight: 700;
        box-shadow: 0 4px 12px rgba(15, 118, 110, 0.2); transition: all 0.3s;
    }
    div.stButton > button[kind="primary"]:hover { transform: translateY(-2px); box-shadow: 0 8px 20px rgba(15, 118, 110, 0.3); }
    
    /* è¾“å…¥æ¡† */
    .stSelectbox > div > div, .stTextInput > div > div, .stNumberInput > div > div {
        background-color: #F8FAFC; border: 1px solid #cbd5e1; border-radius: 10px;
    }
    [data-testid="stFileUploader"] {
        background-color: #F8FAFC; border: 2px dashed #cbd5e1; border-radius: 16px; padding: 20px; text-align: center;
    }

    /* 3D ä¹¦ç±ç‰¹æ•ˆ */
    .book-container { perspective: 1000px; margin-bottom: 20px; }
    .book-3d {
        width: 100%; aspect-ratio: 3/4; border-radius: 4px 12px 12px 4px; position: relative;
        transform-style: preserve-3d; transition: transform 0.3s ease; box-shadow: 5px 5px 15px rgba(0,0,0,0.1);
        cursor: pointer; display: flex; flex-direction: column; justify-content: center; align-items: center;
        text-align: center; padding: 15px; overflow: hidden;
    }
    .book-3d:hover { transform: translateY(-8px) rotateY(-5deg) scale(1.02); box-shadow: 10px 15px 25px rgba(0,0,0,0.15); }
    .book-spine {
        position: absolute; left: 0; top: 0; bottom: 0; width: 12px;
        background: linear-gradient(90deg, rgba(255,255,255,0.2) 0%, rgba(255,255,255,0) 100%);
        z-index: 10; border-right: 1px solid rgba(0,0,0,0.05);
    }
    .book-badge {
        position: absolute; top: 12px; left: 16px; background: rgba(255,255,255,0.9);
        backdrop-filter: blur(4px); padding: 2px 8px; border-radius: 4px; font-size: 10px; font-weight: 800; color: #1e293b; z-index: 20;
    }
    header[data-testid="stHeader"] { background: transparent; }
    .stMain { margin-top: -60px; }
</style>
""", unsafe_allow_html=True)

# ------------------ 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ------------------

# GitHub ä¸Šä¼ åŠŸèƒ½ (ä»åŸç‰ˆæ¢å¤)
def save_to_github_library(filename, content, title, desc):
    try:
        if "GITHUB_TOKEN" not in st.secrets:
            st.error("ğŸ”’ æœªé…ç½® GitHub Tokenï¼Œæ— æ³•ä¸Šä¼ åˆ°äº‘ç«¯ã€‚ä»…ä¿å­˜åˆ°æœ¬åœ°ã€‚")
            # å³ä½¿å¤±è´¥ä¹Ÿä¿å­˜åˆ°æœ¬åœ°
            with open(os.path.join(LIBRARY_DIR, filename), "w", encoding="utf-8") as f:
                f.write(content)
            return

        token = st.secrets["GITHUB_TOKEN"]
        username = st.secrets["GITHUB_USERNAME"]
        repo_name = st.secrets["GITHUB_REPO"]
        
        g = Github(token)
        repo = g.get_repo(f"{username}/{repo_name}")
        
        # 1. ä¸Šä¼ å†…å®¹æ–‡ä»¶
        library_path = f"library/{filename}"
        try:
            contents = repo.get_contents(library_path)
            repo.update_file(library_path, f"Update {filename}", content, contents.sha)
        except:
            repo.create_file(library_path, f"Create {filename}", content)

        # 2. æ›´æ–° info.json
        info_path = "library/info.json"
        try:
            info_contents = repo.get_contents(info_path)
            info_data = json.loads(info_contents.decoded_content.decode("utf-8"))
        except:
            info_data = {}
            info_contents = None

        info_data[filename] = {
            "title": title,
            "desc": desc,
            "date": time.strftime("%Y-%m-%d"),
            "author": "User" 
        }
        
        new_info_str = json.dumps(info_data, indent=2, ensure_ascii=False)
        if info_contents:
            repo.update_file(info_path, "Update info.json", new_info_str, info_contents.sha)
        else:
            repo.create_file(info_path, "Create info.json", new_info_str)
            
        # 3. åŒæ­¥åˆ°æœ¬åœ°
        with open(os.path.join(LIBRARY_DIR, filename), "w", encoding="utf-8") as f:
            f.write(content)
        
        local_info_path = os.path.join(LIBRARY_DIR, "info.json")
        try:
            with open(local_info_path, "r", encoding="utf-8") as f: local_info = json.load(f)
        except: local_info = {}
        
        local_info[filename] = info_data[filename]
        with open(local_info_path, "w", encoding="utf-8") as f:
            json.dump(local_info, f, indent=2, ensure_ascii=False)

        st.toast("âœ… å‘å¸ƒæˆåŠŸï¼å·²åŒæ­¥è‡³äº‘ç«¯ã€‚", icon="ğŸ‰")
        time.sleep(1.5)
        st.rerun()
        
    except Exception as e:
        # é™çº§å¤„ç†ï¼šä¿å­˜åˆ°æœ¬åœ°
        with open(os.path.join(LIBRARY_DIR, filename), "w", encoding="utf-8") as f:
            f.write(content)
        st.warning(f"äº‘ç«¯åŒæ­¥å¤±è´¥ ({e})ï¼Œä½†å·²ä¿å­˜è‡³æœ¬åœ° Libraryã€‚")
        time.sleep(1.5)
        st.rerun()

def extract_text_from_bytes(file_obj, filename):
    if '.' in filename: ext = filename.split('.')[-1].lower()
    else: ext = 'txt'
    text = ""
    try:
        if ext == 'docx':
            doc = Document(file_obj)
            text = "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
        else:
            raw = file_obj.read()
            enc = chardet.detect(raw).get('encoding') or 'utf-8'
            text = raw.decode(enc, errors='ignore')
    except: return ""
    
    if ext in ['srt', 'vtt', 'ass']:
        text = re.sub(r"<.*?>", "", text)
        text = re.sub(r"\d{2}:\d{2}:\d{2},\d{3} --> \d{2}:\d{2}:\d{2},\d{3}", "", text)
    return text

def process_words(all_text, mode, min_len, filter_set=None):
    TOKEN_RE = re.compile(r"[A-Za-z-]+")
    cleaned = [re.sub(r'[^a-z]', '', w.lower()) for w in TOKEN_RE.findall(all_text) if w]
    lemmatized = []
    
    if mode == "spacy" and nlp_spacy:
        chunks = [cleaned[i:i + 50000] for i in range(0, len(cleaned), 50000)]
        for chunk in chunks:
            doc = nlp_spacy(" ".join(chunk))
            for token in doc:
                lw = token.lemma_.lower()
                if lw.isalpha() and wordnet.synsets(lw): lemmatized.append(lw)
    else:
        lemmatizer = WordNetLemmatizer()
        for w, tag in pos_tag(cleaned):
            wn = {'J':wordnet.ADJ,'V':wordnet.VERB,'N':wordnet.NOUN,'R':wordnet.ADV}.get(tag[0], None)
            lw = lemmatizer.lemmatize(w, wn) if wn else lemmatizer.lemmatize(w)
            if wordnet.synsets(lw): lemmatized.append(lw)

    seen = set()
    final_words = []
    sys_stopwords = set(stopwords.words('english'))
    for w in lemmatized:
        if len(w) < min_len: continue
        if w in sys_stopwords: continue
        if filter_set and w in filter_set: continue
        if w not in seen:
            seen.add(w)
            final_words.append(w)
    return final_words

def render_copy_button(text_content, key_suffix=""):
    safe_text = json.dumps(text_content)
    html_code = f"""
    <script>
    function copyText_{key_suffix}() {{
        const text = {safe_text};
        const el = document.createElement('textarea');
        el.value = text;
        document.body.appendChild(el);
        el.select();
        document.execCommand('copy');
        document.body.removeChild(el);
        const btn = document.getElementById("copy_btn_{key_suffix}");
        btn.innerHTML = "âœ… å·²å¤åˆ¶";
        btn.style.background = "#059669";
        setTimeout(() => {{ 
            btn.innerHTML = "ğŸ“‹ ä¸€é”®å¤åˆ¶"; 
            btn.style.background = "linear-gradient(135deg, #2DD4BF 0%, #0F766E 100%)";
        }}, 2000);
    }}
    </script>
    <button id="copy_btn_{key_suffix}" onclick="copyText_{key_suffix}()" style="
        width: 100%; padding: 10px; 
        background: linear-gradient(135deg, #2DD4BF 0%, #0F766E 100%); 
        color: white; border: none; border-radius: 8px; 
        font-family: sans-serif; font-weight: 600; cursor: pointer;
        box-shadow: 0 4px 10px rgba(15, 118, 110, 0.2); transition: all 0.3s;">
        ğŸ“‹ ä¸€é”®å¤åˆ¶
    </button>
    """
    components.html(html_code, height=50)

# ------------------ 3. ä¸»ç•Œé¢å¸ƒå±€ ------------------

with st.sidebar:
    st.markdown("""
        <div style="display:flex; align-items:center; gap:10px; margin-bottom:20px;">
            <div style="width:36px; height:36px; background:linear-gradient(135deg, #2DD4BF, #0F766E); border-radius:8px; display:flex; align-items:center; justify-content:center; color:white; font-weight:bold; box-shadow:0 0 15px rgba(45,212,191,0.4);">V</div>
            <h2 style="margin:0; font-size:1.2rem; color:#0f172a;">VocabMaster</h2>
        </div>
    """, unsafe_allow_html=True)
    
    menu = st.radio("MENU", ["âš¡ æ™ºèƒ½å·¥ä½œå°", "ğŸ“š å…¬å…±è¯ä¹¦åº“", "ğŸ‘¤ ä¸ªäººä¸­å¿ƒ"], label_visibility="collapsed")
    
    st.markdown("---")
    st.markdown("<div style='background:#f0fdfa; padding:12px; border-radius:8px; color:#0f766e; font-size:0.85rem;'><b>ğŸ’¡ Pro Tips:</b><br>ä½¿ç”¨ Spacy å¼•æ“å¯è·å¾—æ›´ç²¾å‡†çš„è¯å½¢è¿˜åŸ (Better Lemmatization).</div>", unsafe_allow_html=True)

c_title, c_user = st.columns([3, 1])
with c_title:
    if "å·¥ä½œå°" in menu:
        st.title("æ™ºèƒ½ç”Ÿè¯æå–")
        st.caption("AI èµ‹èƒ½ï¼Œä¸€é”®ç”Ÿæˆä¸“å±è¯ä¹¦ (Workbench)")
    elif "è¯ä¹¦åº“" in menu:
        st.title("å…¬å…±è¯ä¹¦åº“")
        st.caption("å‘ç°ä¼˜è´¨è¯­æ–™ï¼Œå¼€å¯å­¦ä¹ ä¹‹æ—… (Library)")
    else:
        st.title("ä¸ªäººä¸­å¿ƒ")
with c_user:
    st.markdown("""
    <div style="display:flex; justify-content:flex-end; align-items:center; gap:10px; padding-top:10px;">
        <span style="background:white; padding:4px 10px; border-radius:20px; border:1px solid #e2e8f0; font-size:12px; font-weight:bold; color:#475569;">ğŸš€ Free Plan</span>
        <img src="https://api.dicebear.com/7.x/notionists/svg?seed=Felix" style="width:40px; height:40px; border-radius:50%; border:2px solid white; box-shadow:0 2px 5px rgba(0,0,0,0.1);">
    </div>
    """, unsafe_allow_html=True)
st.markdown("<br>", unsafe_allow_html=True)

# === âš¡ æ™ºèƒ½å·¥ä½œå° ===
if "å·¥ä½œå°" in menu:
    
    # --- æ’å…¥ï¼šèµ„æºå¯¼èˆª (ç”¨æˆ·æä¾›ä»£ç ) ---
    with st.expander("ğŸ“– æ–°æ‰‹æŒ‡å— & å®è—èµ„æºåº“ (ç‚¹å‡»å±•å¼€)", expanded=False):
        t1, t2, t3, t4 = st.tabs(["ğŸ’¡ æ“ä½œæŒ‡å¼•", "ğŸ¬ å½±è§†å­—å¹•", "ğŸ“š åŸè‘—é˜…è¯»", "ğŸ§ å¬åŠ›ç´ æ"])
        
        with t1:
            st.markdown("""
            <div style="padding:5px;">
            <h5 style="margin-top:0">ğŸš€ å››æ­¥åˆ¶ä½œä¸“å±è¯ä¹¦ï¼š</h5>
            <ol>
                <li><b>å‡†å¤‡ç´ æ</b>ï¼šä»å³ä¾§æ ‡ç­¾é¡µä¸‹è½½ <code>.srt</code> å­—å¹•æˆ– <code>.txt</code> ç”µå­ä¹¦ã€‚</li>
                <li><b>æ¸…æ´—è®¾ç½®</b>ï¼šåœ¨ä¸‹æ–¹ã€è®¾ç½®æå–è§„åˆ™ã€‘ä¸­ï¼Œé€‰æ‹©<b>â€œé¢„ç½®ç†Ÿè¯åº“â€</b>æˆ–ä¸Šä¼ è‡ªå®šä¹‰ç†Ÿè¯è¡¨ï¼ˆéå¸¸é‡è¦ï¼èƒ½å±è”½æ‰ is, the ç­‰ç®€å•è¯ï¼‰ã€‚</li>
                <li><b>æ™ºèƒ½æå–</b>ï¼šå°†æ–‡ä»¶æ‹–å…¥ä¸Šä¼ åŒºï¼ŒAI è‡ªåŠ¨å®Œæˆå»é‡ã€è¯å½¢è¿˜åŸï¼ˆRun/Ran/Running â†’ Runï¼‰ã€‚</li>
                <li><b>é—­ç¯å­¦ä¹ </b>ï¼šç‚¹å‡»ç”Ÿæˆçš„<b>â€œä¸€é”®å¤åˆ¶â€</b>æŒ‰é’®ï¼Œè·³è½¬æ‰‡è´ç½‘æ‰¹é‡åˆ¶å¡ï¼Œæˆ–å¯¼å‡ºè¯ä¹¦ã€‚</li>
            </ol>
            </div>
            """, unsafe_allow_html=True)
            
        with t2:
            st.info("ğŸ’¡ å­—å¹•æ–‡ä»¶æ˜¯æå–å£è¯­è¯æ±‡çš„æœ€ä½³ææ–™ã€‚ä¸‹è½½åæ— éœ€è½¬æ¢ï¼Œç›´æ¥æ‹–å…¥æœ¬å·¥å…·ã€‚")
            c1, c2 = st.columns(2)
            c1.markdown("ğŸ¯ **[ä¼ªå°„æ‰‹ç½‘ (Assrt)](https://assrt.net/)**\n<small>è€ç‰Œç«™ç‚¹ï¼Œèµ„æºæœ€å…¨ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ã€‚</small>", unsafe_allow_html=True)
            c1.markdown("ğŸ“º **[å­—å¹•åº“ (Zimuku)](http://zimuku.org/)**\n<small>ç¾å‰§ã€æ—¥å‰§æ›´æ–°é€Ÿåº¦æå¿«ã€‚</small>", unsafe_allow_html=True)
            c1.markdown("âš¡ **[Addic7ed](https://www.addic7ed.com/)**\n<small>ç¾å‰§ç”Ÿè‚‰æ›´æ–°æœ€å¿«çš„åœ°æ–¹ï¼Œé€‚åˆé«˜é˜¶å­¦ä¹ è€…ã€‚</small>", unsafe_allow_html=True)
            
            c2.markdown("ğŸ’ **[SubHD](https://subhd.tv/)**\n<small>ç•Œé¢æ¸…çˆ½ï¼Œé«˜æ¸…å½±è§†å­—å¹•é¦–é€‰ã€‚</small>", unsafe_allow_html=True)
            c2.markdown("ğŸŒ **[OpenSubtitles](https://www.opensubtitles.org/)**\n<small>å…¨çƒæœ€å¤§å­—å¹•åº“ï¼Œå¯»æ‰¾çº¯è‹±æ–‡å­—å¹•é¦–é€‰ã€‚</small>", unsafe_allow_html=True)
            c2.markdown("ğŸï¸ **[YIFY Subtitles](https://yifysubtitles.ch/)**\n<small>ä¸“é—¨é’ˆå¯¹ç”µå½±çš„é«˜è´¨é‡è‹±æ–‡å­—å¹•ã€‚</small>", unsafe_allow_html=True)

        with t3:
            st.success("ğŸ“š æ¨èä¸‹è½½ .txt æˆ– .epub (éœ€è½¬txt) æ ¼å¼ã€‚")
            c1, c2 = st.columns(2)
            c1.markdown("ğŸ›ï¸ **[Project Gutenberg](https://www.gutenberg.org/)**\n<small>æ‹¥æœ‰7ä¸‡+å…è´¹å…¬ç‰ˆç”µå­ä¹¦ï¼Œè‹±æ–‡åŸè‘—çš„å¤§å®åº“ã€‚</small>", unsafe_allow_html=True)
            c1.markdown("ğŸ“– **[ManyBooks](https://manybooks.net/)**\n<small>æ’ç‰ˆç²¾ç¾ï¼Œåˆ†ç±»è¯¦ç»†ï¼Œä¸‹è½½ä½“éªŒå¥½ã€‚</small>", unsafe_allow_html=True)
            
            c2.markdown("ğŸ“° **[Global Times](https://www.globaltimes.cn/)**\n<small>å›½äº§è‹±æ–‡åª’ä½“ï¼Œç”¨è¯è´´è¿‘æ—¶æ”¿ï¼Œé€‚åˆå¤‡è€ƒã€‚</small>", unsafe_allow_html=True)
            c2.markdown("ğŸ§  **[Scientific American](https://www.scientificamerican.com/)**\n<small>é«˜é˜¶ç§‘æ™®æ–‡ç« ï¼Œæ‰˜ç¦/é›…æ€/GRE é˜…è¯»åŒæºç´ æã€‚</small>", unsafe_allow_html=True)

        with t4:
            st.warning("ğŸ§ æŠ€å·§ï¼šä¸‹è½½ Transcript (æ–‡ç¨¿) æå–å•è¯ï¼Œå­¦å®Œå†å»å¬ã€‚")
            c1, c2 = st.columns(2)
            c1.markdown("ğŸ”´ **[TED Talks](https://www.ted.com/)**\n<small>æ€æƒ³ç››å®´ï¼Œæ¯ä¸ªè§†é¢‘éƒ½è‡ªå¸¦å¤šè¯­è¨€æ–‡ç¨¿ã€‚</small>", unsafe_allow_html=True)
            c1.markdown("ğŸ‡ºğŸ‡¸ **[VOA Learning English](https://learningenglish.voanews.com/)**\n<small>ç»å…¸åˆ†çº§å¬åŠ›ææ–™ï¼Œå«çº¯æ­£æ–‡ç¨¿ã€‚</small>", unsafe_allow_html=True)
            
            c2.markdown("ğŸ‡¬ğŸ‡§ **[BBC Learning English](https://www.bbc.co.uk/learningenglish/)**\n<small>è‹±å¼è‹±è¯­é‡‘ç‰Œæ•™ç¨‹ï¼Œ6 Minute English å¿…å¬ã€‚</small>", unsafe_allow_html=True)
            c2.markdown("ğŸ“ **[Coursera](https://www.coursera.org/)**\n<small>å­¦ä¹ ä¸“ä¸šè¯¾ï¼ˆè®¡ç®—æœº/å•†ç§‘ï¼‰çš„æœ€å¥½æ–¹å¼ã€‚</small>", unsafe_allow_html=True)
    # ------------------------------------------------

    if 'result_words' not in st.session_state: st.session_state.result_words = []
    
    col_config, col_main = st.columns([1, 2.5], gap="medium")
    
    with col_config:
        with st.container(border=True):
            st.markdown("##### ğŸ› ï¸ æå–é…ç½®")
            st.caption("AI å¼•æ“ (ENGINE)")
            nlp_mode = st.selectbox("Engine", ["nltk (å¿«é€Ÿ)", "spacy (ç²¾å‡†)"], label_visibility="collapsed")
            st.markdown("<br>", unsafe_allow_html=True)
            st.caption("æ’åºæ–¹å¼ (SORT)")
            sort_order = st.selectbox("Sort", ["æŒ‰æ–‡æœ¬å‡ºç°é¡ºåº", "æŒ‰å­—æ¯ A-Z", "éšæœºæ‰“ä¹±"], label_visibility="collapsed")
            st.markdown("<br>", unsafe_allow_html=True)
            st.caption(f"æœ€çŸ­è¯é•¿ (MIN LENGTH)")
            min_len = st.slider("Min Len", 2, 15, 3, label_visibility="collapsed")
            st.markdown("---")
            st.markdown("##### ğŸ›¡ï¸ ç†Ÿè¯å±è”½")
            selected_presets = st.multiselect("é€‰æ‹©é¢„ç½®åº“", options=list(PRESET_WORDLISTS.keys()), default=[], label_visibility="collapsed")
            st.markdown("<div style='height:8px'></div>", unsafe_allow_html=True)
            filter_file = st.file_uploader("ä¸Šä¼ è‡ªå®šä¹‰å±è”½è¡¨ (.txt)", type=['txt'], label_visibility="collapsed")

            filter_set = set()
            for p in selected_presets:
                if os.path.exists(PRESET_WORDLISTS[p]):
                    with open(PRESET_WORDLISTS[p],'r',encoding='utf-8') as f: filter_set.update(f.read().splitlines())
            if filter_file:
                filter_set.update(filter_file.getvalue().decode('utf-8', errors='ignore').splitlines())

    with col_main:
        with st.container(border=True):
            st.markdown("""<div style="display:flex; justify-content:space-between; margin-bottom:10px;"><span style="font-size:12px; font-weight:bold; color:#94a3b8; letter-spacing:1px;">INPUT SOURCE</span></div>""", unsafe_allow_html=True)
            input_text = st.text_area("Input", height=200, placeholder="åœ¨æ­¤ç²˜è´´æ–‡ç« ã€å­—å¹•æ–‡æœ¬ã€æ­Œè¯...\næˆ–è€…ç‚¹å‡»ä¸‹æ–¹è™šçº¿æ¡†ä¸Šä¼ æ–‡ä»¶", label_visibility="collapsed")
            uploaded_files = st.file_uploader("æˆ–æ‹–æ‹½æ–‡ä»¶åˆ°æ­¤å¤„ (æ”¯æŒ .srt, .docx, .txt)", type=['txt','srt','ass','docx'], accept_multiple_files=True, label_visibility="collapsed")

            col_act_1, col_act_2 = st.columns([3, 1])
            with col_act_2:
                st.markdown("<br>", unsafe_allow_html=True)
                start_btn = st.button("ğŸš€ å¼€å§‹æ™ºèƒ½æå–", type="primary", use_container_width=True)

        if start_btn:
            full_text = input_text
            if uploaded_files:
                for f in uploaded_files:
                    full_text += "\n" + extract_text_from_bytes(f, f.name)
            
            if not full_text.strip():
                st.warning("âš ï¸ è¯·å…ˆè¾“å…¥æ–‡æœ¬æˆ–ä¸Šä¼ æ–‡ä»¶")
            else:
                with st.spinner("AI æ­£åœ¨åˆ†æè¯­ä¹‰ä¸è¯å½¢..."):
                    mode_key = "spacy" if "spacy" in nlp_mode else "nltk"
                    words = process_words(full_text, mode_key, min_len, filter_set)
                    if sort_order == "æŒ‰å­—æ¯ A-Z": words.sort()
                    elif sort_order == "éšæœºæ‰“ä¹±": random.shuffle(words)
                    st.session_state.result_words = words
                    st.rerun()

    if st.session_state.result_words:
        st.markdown("<br>", unsafe_allow_html=True)
        with st.container(border=True):
            words = st.session_state.result_words
            content_str = "\n".join(words)
            
            st.markdown(f"""
            <div style="display:flex; justify-content:space-between; align-items:center; margin-bottom:15px;">
                <h3 style="margin:0;">ğŸ‰ æå–ç»“æœ</h3>
                <span style="background:#dcfce7; color:#166534; padding:2px 10px; border-radius:12px; font-size:12px; font-weight:bold;">å…± {len(words)} è¯</span>
            </div>
            """, unsafe_allow_html=True)
            
            st.text_area("Result", value=content_str, height=150, label_visibility="collapsed")
            
            c_copy, c_dl, c_pub = st.columns([1, 1, 1])
            with c_copy: render_copy_button(content_str)
            with c_dl: st.download_button("ğŸ“¦ ä¸‹è½½ç»“æœ (.txt)", content_str, "vocab.txt", "text/plain", use_container_width=True)
            with c_pub:
                with st.popover("â˜ï¸ å‘å¸ƒåˆ°ç¤¾åŒºåº“"):
                    st.markdown("**åˆ†äº«ä½ çš„è¯ä¹¦**")
                    with st.form("pub_form"):
                        name = st.text_input("æ–‡ä»¶å (è‹±æ–‡, e.g. friends_s1.txt)", value=f"list_{int(time.time())}.txt")
                        title = st.text_input("æ ‡é¢˜ (e.g. è€å‹è®°ç¬¬ä¸€å­£)")
                        desc = st.text_area("ç®€ä»‹ (e.g. åŒ…å«å‰10é›†ç”Ÿè¯)")
                        if st.form_submit_button("ç¡®è®¤å‘å¸ƒ"):
                            if name.endswith(".txt"): 
                                save_to_github_library(name, content_str, title, desc)
                            else:
                                st.error("æ–‡ä»¶åå¿…é¡»ä»¥ .txt ç»“å°¾")

# === ğŸ“š å…¬å…±è¯ä¹¦åº“ ===
elif "è¯ä¹¦åº“" in menu:
    
    search_col, _ = st.columns([1, 2])
    with search_col:
        search_q = st.text_input("Search", placeholder="ğŸ” æœç´¢è¯ä¹¦...", label_visibility="collapsed").lower()

    # --- ä¿®å¤ï¼šé‡æ–°è¯»å–æœ¬åœ°/äº‘ç«¯æ–‡ä»¶ (ä¸å†ä½¿ç”¨é™æ€æ•°æ®) ---
    try:
        with open(os.path.join(LIBRARY_DIR, "info.json"), "r", encoding="utf-8") as f: book_info = json.load(f)
    except: book_info = {}
    
    files = [f for f in os.listdir(LIBRARY_DIR) if f.endswith(".txt")]
    visible = [f for f in files if search_q in f.lower() or search_q in book_info.get(f, {}).get("title", "").lower()]
    
    # é¢œè‰²æ± ï¼Œç”¨äºå¾ªç¯åˆ†é…ç»™ä¹¦ç±
    palettes = [
        {"bg": "#FDE68A", "txt": "#451a03"}, # Amber
        {"bg": "#A7F3D0", "txt": "#064e3b"}, # Emerald
        {"bg": "#BFDBFE", "txt": "#1e3a8a"}, # Blue
        {"bg": "#FECACA", "txt": "#7f1d1d"}, # Red
        {"bg": "#DDD6FE", "txt": "#4c1d95"}, # Violet
        {"bg": "#E2E8F0", "txt": "#0f172a"}, # Slate
    ]

    if not visible:
        st.info("ğŸ“­ æš‚æ— æ•°æ®ï¼Œå»å·¥ä½œå°åˆ¶ä½œç¬¬ä¸€ä¸ªè¯ä¹¦å§ï¼")
    else:
        st.markdown("<br>", unsafe_allow_html=True)
        cols = st.columns(5)
        for i, f in enumerate(visible):
            meta = book_info.get(f, {})
            title = meta.get("title", f)
            desc = meta.get("desc", "æš‚æ— æè¿°")
            
            # è¯»å–è¯æ•°
            try:
                with open(os.path.join(LIBRARY_DIR, f), 'r', encoding='utf-8') as _f:
                    cnt = len(_f.read().splitlines())
            except: cnt = "?"
            
            color = palettes[i % len(palettes)]
            
            with cols[i % 5]:
                # 3D ä¹¦ç±æ¸²æŸ“ (Dynamic)
                st.markdown(f"""
                <div class="book-container">
                    <div class="book-3d" style="background-color: {color['bg']}; color: {color['txt']};">
                        <div class="book-spine"></div>
                        <div class="book-badge">TXT</div>
                        <h3 style="font-size:1.1rem; margin-top:20px; line-height:1.2; color:{color['txt']}; overflow:hidden; text-overflow:ellipsis; display:-webkit-box; -webkit-line-clamp:2; -webkit-box-orient:vertical;">{title}</h3>
                        <p style="font-size:0.75rem; opacity:0.8; margin-top:5px; color:{color['txt']}; overflow:hidden; white-space:nowrap; text-overflow:ellipsis;">{desc}</p>
                        <div style="margin-top:auto; font-size:0.75rem; font-weight:bold; opacity:0.6;">{cnt} è¯</div>
                    </div>
                </div>
                """, unsafe_allow_html=True)
                
                # æ“ä½œæŒ‰é’®
                with st.popover("ä¸‹è½½/é¢„è§ˆ", use_container_width=True):
                    st.markdown(f"**{title}**")
                    try:
                        with open(os.path.join(LIBRARY_DIR, f), 'r', encoding='utf-8') as _f: c = _f.read()
                        render_copy_button(c, f"lib_{i}")
                        st.download_button("â¬‡ï¸ ä¸‹è½½", c, f, "text/plain")
                        st.text_area("é¢„è§ˆ", c, height=200)
                    except: st.error("æ–‡ä»¶è¯»å–å¤±è´¥")

# === ä¸ªäººä¸­å¿ƒ ===
else:
    st.info("ğŸš§ ä¸ªäººä¸­å¿ƒæ­£åœ¨æ–½å·¥ä¸­... (Coming Soon)")
```

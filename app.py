import streamlit as st
import io
import re
import zipfile
import math
import chardet
import os
import json
import random
import time
import pandas as pd
import streamlit.components.v1 as components # å¼•å…¥ç»„ä»¶åº“ç”¨äºè‡ªå®šä¹‰æŒ‰é’®
from github import Github

# NLP Imports
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet, stopwords
from nltk import pos_tag

# DOCX Import
from docx import Document

# Optional Spacy
try:
    import spacy
    _HAS_SPACY = True
except ImportError:
    _HAS_SPACY = False

# ------------------ 1. é¡µé¢é…ç½® & ç°ä»£ CSS æ³¨å…¥ ------------------
st.set_page_config(
    page_title="VocabMaster | æ™ºèƒ½è¯ä¹¦å·¥åŠ", 
    page_icon="âš¡", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# æ³¨å…¥ CSS
st.markdown("""
<style>
    .stApp { background-color: #fcfdfe; }
    h1, h2, h3 { font-family: 'Segoe UI', sans-serif; color: #2c3e50; }
    .step-header { font-size: 1.1rem; font-weight: 700; color: #4f46e5; margin-bottom: 10px; display: flex; align-items: center; }
    [data-testid="stExpander"], [data-testid="stForm"] { background: white; border-radius: 12px; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05); border: 1px solid #e5e7eb; }
    div.stButton > button { border-radius: 8px; padding: 0.5rem 1rem; font-weight: 600; transition: all 0.2s; }
    div.stButton > button:hover { transform: translateY(-1px); }
    .info-box { background-color: #eff6ff; border-left: 4px solid #3b82f6; padding: 10px 15px; border-radius: 4px; color: #1e3a8a; font-size: 0.9em; margin-bottom: 15px; }
    a { color: #0366d6; text-decoration: none; }
    a:hover { text-decoration: underline; }
    
    /* ä»£ç å—æ ·å¼ï¼šä½œä¸ºå¤‡ç”¨å±•ç¤ºï¼Œç¨å¾®æ·¡åŒ– */
    .stCodeBlock {
        max-height: 200px !important;
        overflow-y: auto !important;
        border: 1px solid #e2e8f0;
        border-radius: 8px;
        background-color: #f8fafc;
        opacity: 0.9;
    }
</style>
""", unsafe_allow_html=True)

# ------------------ 2. å·¥å…·å‡½æ•°ï¼šè‡ªå®šä¹‰å¤åˆ¶æŒ‰é’® ------------------
def render_copy_button(text_content, unique_key):
    """
    æ¸²æŸ“ä¸€ä¸ªé†’ç›®çš„è‡ªå®šä¹‰ HTML/JS å¤åˆ¶æŒ‰é’®
    """
    # å®‰å…¨è½¬ä¹‰æ–‡æœ¬å†…å®¹
    safe_text = json.dumps(text_content)
    
    html_code = f"""
    <!DOCTYPE html>
    <html>
    <head>
    <style>
        .copy-btn {{
            width: 100%;
            padding: 12px;
            background-color: #4f46e5; /* é†’ç›®è“ç´«è‰² */
            color: white;
            border: none;
            border-radius: 8px;
            font-family: 'Segoe UI', sans-serif;
            font-weight: 600;
            font-size: 16px;
            cursor: pointer;
            transition: all 0.3s ease;
            display: flex;
            align-items: center;
            justify-content: center;
            box-shadow: 0 4px 6px rgba(79, 70, 229, 0.2);
        }}
        .copy-btn:hover {{
            background-color: #4338ca;
            transform: translateY(-2px);
            box-shadow: 0 6px 8px rgba(79, 70, 229, 0.3);
        }}
        .copy-btn:active {{
            transform: translateY(0);
        }}
        .icon {{ margin-right: 8px; font-size: 18px; }}
    </style>
    </head>
    <body>
        <button id="btn_{unique_key}" class="copy-btn" onclick="copyText()">
            <span class="icon">ğŸ“‹</span> ç‚¹å‡»ä¸€é”®å¤åˆ¶æ‰€æœ‰å•è¯ (Copy All)
        </button>

        <script>
        function copyText() {{
            const text = {safe_text};
            const textArea = document.createElement("textarea");
            textArea.value = text;
            document.body.appendChild(textArea);
            textArea.select();
            try {{
                document.execCommand('copy');
                const btn = document.getElementById("btn_{unique_key}");
                const originalText = btn.innerHTML;
                
                // æˆåŠŸåé¦ˆ
                btn.innerHTML = '<span class="icon">âœ…</span> å¤åˆ¶æˆåŠŸï¼(Copied)';
                btn.style.backgroundColor = "#10b981"; // ç»¿è‰²
                
                // 2ç§’åæ¢å¤
                setTimeout(() => {{
                    btn.innerHTML = originalText;
                    btn.style.backgroundColor = "#4f46e5"; // æ¢å¤è“ç´«è‰²
                }}, 2000);
            }} catch (err) {{
                console.error('Fallback: Oops, unable to copy', err);
            }}
            document.body.removeChild(textArea);
        }}
        </script>
    </body>
    </html>
    """
    # æ¸²æŸ“ HTML ç»„ä»¶ï¼Œè®¾å®šå›ºå®šé«˜åº¦
    components.html(html_code, height=60)


# ------------------ 3. ç¼“å­˜èµ„æºåŠ è½½ ------------------
@st.cache_resource
def download_nltk_resources():
    resources = ["punkt", "averaged_perceptron_tagger", "averaged_perceptron_tagger_eng", "wordnet", "omw-1.4", "stopwords"]
    for r in resources:
        try:
            nltk.data.find(f'tokenizers/{r}')
        except LookupError:
            nltk.download(r, quiet=True)
        except ValueError:
            nltk.download(r, quiet=True)

@st.cache_resource
def load_spacy_model():
    if _HAS_SPACY:
        try:
            return spacy.load("en_core_web_sm", disable=["ner", "parser"])
        except Exception:
            return None
    return None

download_nltk_resources()
nlp_spacy = load_spacy_model()

# ------------------ 4. æ ¸å¿ƒé€»è¾‘å‡½æ•° ------------------
def save_to_github_library(filename, content, title, desc):
    """GitHub ä¸Šä¼ é€»è¾‘"""
    try:
        if "GITHUB_TOKEN" not in st.secrets:
            st.error("ğŸ”’ ç³»ç»Ÿæœªé…ç½® GitHub Tokenï¼Œæ— æ³•è¿æ¥äº‘ç«¯ã€‚è¯·åœ¨ .streamlit/secrets.toml ä¸­é…ç½®ã€‚")
            return

        token = st.secrets["GITHUB_TOKEN"]
        username = st.secrets["GITHUB_USERNAME"]
        repo_name = st.secrets["GITHUB_REPO"]
        
        g = Github(token)
        repo = g.get_repo(f"{username}/{repo_name}")
        
        library_path = f"library/{filename}"
        info_path = "library/info.json"
        
        try:
            contents = repo.get_contents(library_path)
            repo.update_file(library_path, f"Update {filename}", content, contents.sha)
        except:
            repo.create_file(library_path, f"Create {filename}", content)

        try:
            info_contents = repo.get_contents(info_path)
            info_data = json.loads(info_contents.decoded_content.decode("utf-8"))
        except:
            info_data = {}
            info_contents = None

        info_data[filename] = {
            "title": title,
            "desc": desc,
            "date": time.strftime("%Y-%m-%d"),
            "author": "User" 
        }
        
        new_info_str = json.dumps(info_data, indent=2, ensure_ascii=False)
        if info_contents:
            repo.update_file(info_path, "Update info.json", new_info_str, info_contents.sha)
        else:
            repo.create_file(info_path, "Create info.json", new_info_str)
            
        local_lib = "library"
        if not os.path.exists(local_lib): os.makedirs(local_lib)
        with open(os.path.join(local_lib, filename), "w", encoding="utf-8") as f:
            f.write(content)
        
        local_info_path = os.path.join(local_lib, "info.json")
        local_info = {}
        if os.path.exists(local_info_path):
            with open(local_info_path, "r", encoding="utf-8") as f:
                try: local_info = json.load(f)
                except: pass
        local_info[filename] = info_data[filename]
        with open(local_info_path, "w", encoding="utf-8") as f:
            json.dump(local_info, f, indent=2, ensure_ascii=False)

        st.toast("âœ… å‘å¸ƒæˆåŠŸï¼", icon="ğŸ‰")
        time.sleep(1.5)
        st.rerun()
        
    except Exception as e:
        st.error(f"è¿æ¥äº‘ç«¯å¤±è´¥: {e}")

def extract_text_from_bytes(file_obj, filename):
    if '.' in filename: ext = filename.split('.')[-1].lower()
    else: ext = 'txt'
    text = ""
    try:
        if ext == 'docx':
            doc = Document(file_obj)
            text_content = []
            for p in doc.paragraphs:
                if p.text.strip(): text_content.append(p.text)
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if cell.text.strip(): text_content.append(cell.text)
            text = "\n".join(text_content)
        elif ext == 'doc':
            return ""
        else:
            raw = file_obj.read()
            enc = chardet.detect(raw).get('encoding') or 'utf-8'
            text = raw.decode(enc, errors='ignore')
    except Exception: return ""
    
    if ext in ['srt', 'vtt', 'ass']:
        clean_text = re.sub(r"<.*?>", "", text)
        clean_text = re.sub(r"\d{2}:\d{2}:\d{2},\d{3} --> \d{2}:\d{2}:\d{2},\d{3}", "", clean_text)
        return clean_text
    return text

def process_words(all_text, mode, min_len, filter_set=None):
    TOKEN_RE = re.compile(r"[A-Za-z-]+")
    cleaned = [re.sub(r'[^a-z]', '', w.lower()) for w in TOKEN_RE.findall(all_text) if w]
    lemmatized = []
    
    if mode == "spacy" and nlp_spacy:
        chunk_size = 50000
        chunks = [cleaned[i:i + chunk_size] for i in range(0, len(cleaned), chunk_size)]
        for chunk in chunks:
            doc = nlp_spacy(" ".join(chunk))
            for token in doc:
                lw = token.lemma_.lower()
                if lw.isalpha() and wordnet.synsets(lw): lemmatized.append(lw)
    else:
        lemmatizer = WordNetLemmatizer()
        for w, tag in pos_tag(cleaned):
            wn = {'J':wordnet.ADJ,'V':wordnet.VERB,'N':wordnet.NOUN,'R':wordnet.ADV}.get(tag[0], None)
            lw = lemmatizer.lemmatize(w, wn) if wn else lemmatizer.lemmatize(w)
            if wordnet.synsets(lw): lemmatized.append(lw)

    seen = set()
    final_words = []
    sys_stopwords = set(stopwords.words('english'))
    for w in lemmatized:
        if len(w) < min_len: continue
        if w in sys_stopwords: continue
        if filter_set and w in filter_set: continue
        if w not in seen:
            seen.add(w)
            final_words.append(w)
    return final_words

# ------------------ 5. UI å¸ƒå±€è®¾è®¡ ------------------

# === ä¾§è¾¹æ  ===
with st.sidebar:
    st.image("https://img.icons8.com/fluency/96/dictionary.png", width=50)
    st.markdown("### VocabMaster")
    st.caption("v8.0 Pro Copy Edition")
    st.markdown("---")
    
    menu = st.radio(
        "é€‰æ‹©åŠŸèƒ½", 
        ["âš¡ åˆ¶ä½œç”Ÿè¯æœ¬", "ğŸŒ å…¬å…±è¯ä¹¦åº“"],
        captions=["ä»æ–‡ä»¶æå–å•è¯", "ä¸‹è½½ç°æˆçš„è¯ä¹¦"]
    )
    
    st.markdown("---")
    st.info("**å°è´´å£«**\nä½¿ç”¨ Spacy å¼•æ“å¯ä»¥è·å¾—æ›´å‡†ç¡®çš„è¯å½¢è¿˜åŸï¼ˆä¾‹å¦‚å°† 'running' è¿˜åŸä¸º 'run'ï¼‰ã€‚")

# === åŠŸèƒ½ä¸€: åˆ¶ä½œç”Ÿè¯æœ¬ ===
if menu == "âš¡ åˆ¶ä½œç”Ÿè¯æœ¬":
    st.title("âš¡ æ™ºèƒ½ç”Ÿè¯æå–å·¥åŠ")
    
    # --- æŒ‡å¼•åŒºåŸŸ ---
    with st.expander("ğŸ“– æ–°æ‰‹æŒ‡å— & å®è—èµ„æºå¯¼èˆª (ç‚¹å‡»å±•å¼€)", expanded=False):
        tab_guide, tab_subs, tab_books, tab_learn = st.tabs(["ğŸ’¡ æ“ä½œæŒ‡å¼•", "ğŸ¬ å½±è§†å­—å¹•", "ğŸ“š åè‘— & é˜…è¯»", "ğŸ§ åå¸ˆ & å¬åŠ›"])
        
        with tab_guide:
            st.markdown("""
            <div style="padding: 10px; background: #f8f9fa; border-radius: 8px;">
            <h4 style="margin-top:0">ğŸš€ å¿«é€Ÿä¸Šæ‰‹æµç¨‹</h4>
            <ol>
                <li><b>å®šè§„åˆ™</b>ï¼šè®¾ç½®æå–è§„åˆ™ï¼ŒåŒ…æ‹¬æ–‡ä»¶æ‹†åˆ†å¤§å°ã€‚</li>
                <li><b>ä¼ æ–‡ä»¶</b>ï¼šå°†å­—å¹•æˆ–æ–‡æ¡£æ‹–å…¥ä¸Šä¼ åŒºï¼Œç‚¹å‡»æå–ã€‚</li>
                <li><b>å»èƒŒè¯µ</b>ï¼šä½¿ç”¨<b>é†’ç›®çš„è“è‰²æŒ‰é’®</b>ä¸€é”®å¤åˆ¶ï¼Œè·³è½¬æ‰‡è´ç½‘æ‰¹é‡å¯¼å…¥ã€‚</li>
            </ol>
            </div>
            """, unsafe_allow_html=True)
            
        with tab_subs:
            st.info("ğŸ’¡ æç¤ºï¼šä¸‹è½½ .srt æˆ– .ass æ ¼å¼çš„å­—å¹•æ–‡ä»¶ï¼Œç›´æ¥æ‹–å…¥æœ¬å·¥å…·å³å¯æå–ç”Ÿè¯ã€‚")
            c1, c2 = st.columns(2)
            with c1:
                st.markdown("ğŸ¯ **[ä¼ªå°„æ‰‹ç½‘ (Assrt)](https://assrt.net/)**")
                st.markdown("ğŸ“º **[å­—å¹•åº“ (Zimuku)](http://zimuku.org/)**")
            with c2:
                st.markdown("ğŸ’ **[SubHD](https://subhd.tv/)**")
                st.markdown("ğŸŒ **[OpenSubtitles](https://www.opensubtitles.org/)**")

        with tab_books:
            c1, c2 = st.columns(2)
            with c1:
                st.markdown("ğŸ›ï¸ **[Project Gutenberg](https://www.gutenberg.org/)**")
                st.markdown("ğŸ“– **[Standard Ebooks](https://standardebooks.org/)**")
            with c2:
                st.markdown("ğŸ“° **[The Economist](https://www.economist.com/)**")
                st.markdown("ğŸ² **[China Daily](https://language.chinadaily.com.cn/)**")

        with tab_learn:
            c1, c2 = st.columns(2)
            with c1:
                st.markdown("ğŸ”´ **[TED Talks](https://www.ted.com/)**")
                st.markdown("ğŸ‡¬ğŸ‡§ **[BBC Learning English](https://www.bbc.co.uk/learningenglish/)**")
            with c2:
                st.markdown("ğŸ“ **[Coursera](https://www.coursera.org/)**")
                st.markdown("ğŸ‡ºğŸ‡¸ **[NPR News](https://www.npr.org/)**")

    # çŠ¶æ€åˆå§‹åŒ–
    if 'result_words' not in st.session_state: st.session_state.result_words = []
    if 'source_files_count' not in st.session_state: st.session_state.source_files_count = 0
    
    # --- ä¸»æ“ä½œåŒº ---
    c_config, c_upload = st.columns([1, 2], gap="large")
    
    with c_config:
        st.markdown('<div class="step-header">1ï¸âƒ£ è®¾ç½®æå–è§„åˆ™</div>', unsafe_allow_html=True)
        with st.container(border=True):
            nlp_mode = st.selectbox("AI å¤„ç†å¼•æ“", ["nltk (å¿«é€Ÿ)", "spacy (ç²¾å‡†)"])
            mode_key = "spacy" if "spacy" in nlp_mode else "nltk"
            
            min_len = st.number_input("å•è¯æœ€çŸ­é•¿åº¦", value=3, min_value=1)
            
            st.markdown("---")
            chunk_size = st.number_input(
                "ğŸ“¥ æ–‡ä»¶æ‹†åˆ†å¤§å° (è¯/æ–‡ä»¶)", 
                value=5000, 
                step=1000,
                help="å½“ä¸‹è½½ ZIP æ—¶ï¼Œä¼šå°†å•è¯è¡¨åˆ‡å‰²æˆå¤šä¸ªæ–‡ä»¶ã€‚"
            )
            
            st.markdown("---")
            filter_file = st.file_uploader("å±è”½è¯è¡¨ (.txt)", type=['txt'], label_visibility="visible")
            filter_set = set()
            if filter_file:
                c = filter_file.getvalue().decode("utf-8", errors='ignore')
                filter_set = set(l.strip().lower() for l in c.splitlines() if l.strip())
                st.caption(f"âœ… å·²åŠ è½½ {len(filter_set)} ä¸ªç†Ÿè¯")

    with c_upload:
        st.markdown('<div class="step-header">2ï¸âƒ£ ä¸Šä¼ æ–‡ä»¶å¹¶åˆ†æ</div>', unsafe_allow_html=True)
        with st.container(border=True):
            st.markdown("<div class='info-box'>æ”¯æŒ .srt, .ass, .docx, .txt æ‰¹é‡ä¸Šä¼ </div>", unsafe_allow_html=True)
            
            uploaded_files = st.file_uploader(
                "æ–‡ä»¶ä¸Šä¼ åŒº", 
                type=['txt','srt','ass','vtt','docx'], 
                accept_multiple_files=True,
                label_visibility="collapsed"
            )
            
            st.markdown("<br>", unsafe_allow_html=True)
            
            if uploaded_files:
                if st.button("ğŸš€ å¼€å§‹æ™ºèƒ½æå–", type="primary", use_container_width=True):
                    progress_text = "æ­£åœ¨è¯»å–æ–‡ä»¶..."
                    my_bar = st.progress(0, text=progress_text)
                    
                    all_raw_text = []
                    for idx, file in enumerate(uploaded_files):
                        text = extract_text_from_bytes(file, file.name)
                        all_raw_text.append(text)
                        my_bar.progress((idx + 1) / len(uploaded_files), text=f"è§£ææ–‡ä»¶: {file.name}")
                    
                    full_text = "\n".join(all_raw_text)
                    
                    if full_text.strip():
                        my_bar.progress(100, text=f"æ­£åœ¨ä½¿ç”¨ {mode_key.upper()} å¼•æ“æ¸…æ´—æ•°æ®...")
                        words = process_words(full_text, mode_key, min_len, filter_set)
                        st.session_state.result_words = words
                        st.session_state.source_files_count = len(uploaded_files)
                        my_bar.empty()
                        st.success(f"æå–å®Œæˆï¼å…±å‘ç° {len(words)} ä¸ªç”Ÿè¯ã€‚")
                        time.sleep(0.5)
                        st.rerun() 
                    else:
                        st.error("æ— æ³•ä»æ–‡ä»¶ä¸­è¯†åˆ«æ–‡å­—ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ã€‚")

    # --- ç»“æœå±•ç¤ºåŒº ---
    if st.session_state.result_words:
        st.divider()
        st.markdown('<div class="step-header">3ï¸âƒ£ ç»“æœé¢„è§ˆä¸å¯¼å…¥</div>', unsafe_allow_html=True)
        
        words = st.session_state.result_words
        content_str = "\n".join(words)
        
        with st.container(border=True):
            col_stat1, col_stat2, col_stat3 = st.columns(3)
            col_stat1.metric("ğŸ“š æå–ç”Ÿè¯æ€»æ•°", f"{len(words)}")
            col_stat2.metric("â±ï¸ å»ºè®®å­¦ä¹ å¤©æ•°", f"{math.ceil(len(words)/20)} å¤©")
            col_stat3.metric("ğŸ” è¯æ±‡æ¥æº", f"{st.session_state.source_files_count} ä¸ªæ–‡ä»¶")

        # å¸ƒå±€ï¼šå·¦ä¾§(å¤åˆ¶åŒº) + å³ä¾§(æ“ä½œæŒ‰é’®)
        col_copy, col_actions = st.columns([2, 1], gap="large")

        # å·¦ä¾§ï¼šé†’ç›®å¤åˆ¶åŒº
        with col_copy:
            st.markdown("##### ğŸ“‹ å•è¯åˆ—è¡¨ (ä¸€é”®å¤åˆ¶)")
            # 1. æ¸²æŸ“è‡ªå®šä¹‰çš„å¤§æŒ‰é’®
            render_copy_button(content_str, "result_area")
            
            # 2. å¤‡ç”¨å±•ç¤ºåŒº (ä»£ç å—)
            st.caption("ğŸ‘‡ ä¸‹æ–¹ä¸ºæ–‡æœ¬é¢„è§ˆ (Preview)")
            st.code(content_str, language="text")

        # å³ä¾§ï¼šæ“ä½œæŒ‰é’®ç¾¤
        with col_actions:
            st.markdown("##### ğŸš€ å¿«é€Ÿæ“ä½œ")
            
            st.link_button(
                "ğŸ¦ å¯¼å…¥æ‰‡è´ç½‘ (Webç«¯)", 
                "https://web.shanbay.com/wordsweb/#/books", 
                help="ç‚¹å‡»è·³è½¬ï¼Œç™»å½•åé€‰æ‹©'ä¸Šä¼ è¯ä¹¦'ï¼Œç²˜è´´å·¦ä¾§å¤åˆ¶çš„å•è¯ã€‚",
                type="primary", 
                use_container_width=True
            )
            
            st.markdown("<div style='height:10px'></div>", unsafe_allow_html=True)

            zip_buffer = io.BytesIO()
            num_files = math.ceil(len(words) / chunk_size)
            with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                for i in range(num_files):
                    s = i * chunk_size
                    e = min(s + chunk_size, len(words))
                    zf.writestr(f"word_list_{i+1}.txt", "\n".join(words[s:e]))
            
            st.download_button(
                f"ğŸ“¦ ä¸‹è½½ ZIP ({num_files}ä¸ªæ–‡ä»¶)", 
                zip_buffer.getvalue(), 
                "my_vocabulary.zip", 
                "application/zip", 
                use_container_width=True
            )
            
            st.markdown("---")
            
            with st.expander("â˜ï¸ å‘å¸ƒåˆ°å…¬å…±åº“", expanded=False):
                with st.form("pub_form"):
                    s_name = st.text_input("æ–‡ä»¶å (è‹±æ–‡)", value=f"vocab_{int(time.time())}.txt")
                    s_title = st.text_input("æ ‡é¢˜", placeholder="å¦‚ï¼šè€å‹è®°ç¬¬ä¸€å­£")
                    s_desc = st.text_area("ç®€ä»‹")
                    if st.form_submit_button("å‘å¸ƒ"):
                        if not s_name.endswith(".txt"):
                            st.warning("éœ€ .txt ç»“å°¾")
                        else:
                            save_to_github_library(s_name, content_str, s_title, s_desc)

# === åŠŸèƒ½äºŒ: å…¬å…±è¯ä¹¦åº“ ===
elif menu == "ğŸŒ å…¬å…±è¯ä¹¦åº“":
    st.title("ğŸŒ ç¤¾åŒºå…¬å…±è¯ä¹¦åº“")
    
    st.markdown("""
    <div class="info-box">
    æ±‡é›†ç¤¾åŒºç²¾é€‰è¯ä¹¦ã€‚<b>ç‚¹å‡»è“è‰²å¤§æŒ‰é’®å¤åˆ¶</b>ï¼Œå³å¯å»æ‰‡è´ç½‘å¯¼å…¥å­¦ä¹ ã€‚
    </div>
    """, unsafe_allow_html=True)
    
    col_search, _ = st.columns([2, 1])
    with col_search:
        search_q = st.text_input("ğŸ” æœç´¢è¯ä¹¦...", placeholder="è¾“å…¥å…³é”®è¯...").lower()

    LIBRARY_DIR = "library"
    INFO_FILE = "info.json"
    if not os.path.exists(LIBRARY_DIR): os.makedirs(LIBRARY_DIR)
    
    book_info = {}
    try:
        with open(os.path.join(LIBRARY_DIR, INFO_FILE), "r", encoding="utf-8") as f:
            book_info = json.load(f)
    except: pass

    try:
        files = [f for f in os.listdir(LIBRARY_DIR) if f.endswith(".txt")]
    except: files = []
    
    visible_files = []
    for f in files:
        meta = book_info.get(f, {})
        t = meta.get("title", f).lower()
        if search_q in t or search_q in f.lower():
            visible_files.append(f)

    if not visible_files:
        st.warning("ğŸ“­ æš‚æ— å…¬å…±è¯ä¹¦ã€‚")
    else:
        st.divider()
        cols = st.columns(3)
        for i, filename in enumerate(visible_files):
            file_path = os.path.join(LIBRARY_DIR, filename)
            try:
                with open(file_path, "r", encoding="utf-8") as f: content = f.read()
                count = len(content.splitlines())
                meta = book_info.get(filename, {})
                title = meta.get("title", filename)
                desc = meta.get("desc", "æš‚æ— æè¿°")
                
                with cols[i % 3]:
                    with st.container(border=True):
                        st.subheader(f"ğŸ“„ {title}")
                        st.caption(f"ğŸ“ {count} è¯")
                        
                        # æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨é†’ç›®çš„å¤§æŒ‰é’®ä»£æ›¿ç®€å•çš„ä»£ç å±•ç¤º
                        render_copy_button(content, f"lib_{i}")
                        
                        # ä»£ç å—ä½œä¸ºé¢„è§ˆï¼Œé«˜åº¦å—é™
                        st.code(content, language="text")
                        
                        c_imp, c_dl = st.columns(2)
                        with c_imp:
                            st.link_button(
                                "ğŸš€ å¯¼å…¥æ‰‡è´", 
                                "https://web.shanbay.com/wordsweb/#/books", 
                                use_container_width=True
                            )
                        with c_dl:
                            st.download_button(
                                "â¬‡ï¸ ä¸‹è½½", content, filename, "text/plain",
                                key=f"dl_{i}", use_container_width=True
                            )
            except: continue

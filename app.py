import streamlit as st
import io
import re
import zipfile
import math
import chardet
import os

# NLP Imports
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet, stopwords
from nltk import pos_tag

# DOCX Import
from docx import Document

# Optional Spacy
try:
    import spacy
    _HAS_SPACY = True
except ImportError:
    _HAS_SPACY = False

# ------------------ é¡µé¢é…ç½® ------------------
st.set_page_config(page_title="ä¸‡èƒ½è¯ä¹¦å¹³å°", page_icon="ğŸ“˜", layout="wide")

# ------------------ ç¼“å­˜èµ„æºåŠ è½½ ------------------
@st.cache_resource
def download_nltk_resources():
    """é™é»˜ä¸‹è½½ NLTK èµ„æº"""
    resources = [
        "punkt", 
        "averaged_perceptron_tagger", 
        "averaged_perceptron_tagger_eng", 
        "wordnet", 
        "omw-1.4", 
        "stopwords"
    ]
    for r in resources:
        try:
            nltk.data.find(f'tokenizers/{r}')
        except LookupError:
            nltk.download(r, quiet=True)
        except ValueError:
            nltk.download(r, quiet=True)

@st.cache_resource
def load_spacy_model():
    if _HAS_SPACY:
        try:
            return spacy.load("en_core_web_sm", disable=["ner", "parser"])
        except Exception:
            return None
    return None

download_nltk_resources()
nlp_spacy = load_spacy_model()

# ------------------ æ ¸å¿ƒé€»è¾‘å‡½æ•° (ä¿æŒä¸å˜) ------------------
# (ä¸ºäº†èŠ‚çœç¯‡å¹…ï¼Œè¿™é‡Œçœç•¥äº†å…·ä½“çš„æå–å‡½æ•°é€»è¾‘ï¼Œå®é™…è¿è¡Œæ—¶å®ƒä»¬æ˜¯å¿…é¡»çš„)
# ... è¿™é‡Œçš„ extract_english_from_srt ç­‰å‡½æ•°ä¸ä¹‹å‰çš„ä»£ç å®Œå…¨ä¸€è‡´ ...
# ä¸ºäº†ä¿è¯ä»£ç å®Œæ•´è¿è¡Œï¼Œæˆ‘è¿™é‡Œå†æ¬¡ç®€å†™ä¸€éå…³é”®å‡½æ•°ï¼Œä½ å¯ä»¥ç›´æ¥ä½¿ç”¨ä¹‹å‰å®Œæ•´çš„é€»è¾‘

def extract_text_from_bytes(file_obj, filename):
    if '.' in filename:
        ext = filename.split('.')[-1].lower()
    else:
        ext = 'txt'
    text = ""
    try:
        if ext == 'docx':
            doc = Document(file_obj)
            text_content = []
            for p in doc.paragraphs:
                if p.text.strip(): text_content.append(p.text)
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if cell.text.strip(): text_content.append(cell.text)
            text = "\n".join(text_content)
        elif ext == 'doc':
            st.error("ä¸æ”¯æŒ .docï¼Œè¯·è½¬å­˜ä¸º .docx")
            return ""
        else:
            raw = file_obj.read()
            enc = chardet.detect(raw).get('encoding') or 'utf-8'
            text = raw.decode(enc, errors='ignore')
    except Exception as e:
        return ""
    
    # ç®€å•æ¸…æ´—
    if ext in ['srt', 'vtt', 'ass']:
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å»ºè®®ä¿ç•™ä¹‹å‰å®Œæ•´çš„æ¸…æ´—é€»è¾‘
        clean_text = re.sub(r"<.*?>", "", text)
        return clean_text
    return text

def get_wordnet_pos(tag):
    if tag.startswith('J'): return wordnet.ADJ
    if tag.startswith('V'): return wordnet.VERB
    if tag.startswith('N'): return wordnet.NOUN
    if tag.startswith('R'): return wordnet.ADV
    return None

def process_words(all_text, mode, min_len, filter_set=None):
    TOKEN_RE = re.compile(r"[A-Za-z-]+")
    raw_tokens = TOKEN_RE.findall(all_text)
    cleaned = [re.sub(r'[^a-z]', '', w.lower()) for w in raw_tokens]
    cleaned = [w for w in cleaned if w]
    lemmatized = []
    
    # ç®€åŒ–æ˜¾ç¤ºï¼Œä¸ä½¿ç”¨è¿›åº¦æ¡ä»¥å…å†²çª
    if mode == "spacy" and nlp_spacy:
        chunk_size = 50000
        chunks = [cleaned[i:i + chunk_size] for i in range(0, len(cleaned), chunk_size)]
        for chunk in chunks:
            doc = nlp_spacy(" ".join(chunk))
            for token in doc:
                lw = token.lemma_.lower()
                if lw.isalpha() and wordnet.synsets(lw): lemmatized.append(lw)
    else:
        lemmatizer = WordNetLemmatizer()
        tagged = pos_tag(cleaned)
        for w, tag in tagged:
            wn = get_wordnet_pos(tag)
            lw = lemmatizer.lemmatize(w, wn) if wn else lemmatizer.lemmatize(w)
            if wordnet.synsets(lw): lemmatized.append(lw)

    seen = set()
    final_words = []
    sys_stopwords = set(stopwords.words('english'))
    for w in lemmatized:
        if len(w) < min_len: continue
        if w in sys_stopwords: continue
        if filter_set and w in filter_set: continue
        if w not in seen:
            seen.add(w)
            final_words.append(w)
    return final_words

# ------------------ ä¸»ç•Œé¢å¯¼èˆª ------------------

# ä¾§è¾¹æ å¯¼èˆª
st.sidebar.title("åŠŸèƒ½å¯¼èˆª")
page = st.sidebar.radio("é€‰æ‹©æ¨¡å¼:", ["ğŸ› ï¸ åˆ¶ä½œç”Ÿè¯æœ¬", "ğŸ“š å…¬å…±è¯ä¹¦åº“"])

# ==================== é¡µé¢ 1: åˆ¶ä½œç”Ÿè¯æœ¬ ====================
if page == "ğŸ› ï¸ åˆ¶ä½œç”Ÿè¯æœ¬":
    st.title("ğŸ› ï¸ è‹±è¯­ç”Ÿè¯æå–å™¨")
    st.markdown("ä¸Šä¼ æ–‡æ¡£æˆ–å­—å¹•ï¼Œä¸€é”®æå–ç”Ÿè¯ã€‚")

    with st.sidebar:
        st.divider()
        st.header("âš™ï¸ æå–è®¾ç½®")
        nlp_mode = st.selectbox("å¼•æ“", ["nltk (å¿«)", "spacy (å‡†)"], index=0)
        mode_key = "spacy" if "spacy" in nlp_mode else "nltk"
        min_len = st.number_input("æœ€çŸ­è¯é•¿", value=3)
        chunk_size = st.number_input("åˆ‡åˆ†å¤§å°", value=5000)
        filter_file = st.file_uploader("è¿‡æ»¤è¯è¡¨", type=['txt'])
        filter_set = set()
        if filter_file:
            c = filter_file.getvalue().decode("utf-8", errors='ignore')
            filter_set = set(l.strip().lower() for l in c.splitlines() if l.strip())
            st.success(f"å·²åŠ è½½ {len(filter_set)} ä¸ªè¿‡æ»¤è¯")

    uploaded_files = st.file_uploader("ä¸Šä¼ æ–‡ä»¶", type=['txt','srt','ass','vtt','docx'], accept_multiple_files=True)

    if uploaded_files and st.button("ğŸš€ å¼€å§‹æå–", type="primary"):
        all_raw_text = []
        for file in uploaded_files:
            text = extract_text_from_bytes(file, file.name)
            all_raw_text.append(text)
        
        full_text = "\n".join(all_raw_text)
        if full_text.strip():
            with st.spinner("æ­£åœ¨åˆ†æå•è¯..."):
                result_words = process_words(full_text, mode_key, min_len, filter_set)
            
            st.success(f"æå–æˆåŠŸï¼å…± {len(result_words)} ä¸ªå•è¯ã€‚")
            
            # é¢„è§ˆä¸ä¸‹è½½
            with st.expander("ğŸ‘€ é¢„è§ˆç»“æœ"):
                st.write(", ".join(result_words[:100]))
            
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                zf.writestr("word_list.txt", "\n".join(result_words))
            
            st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ (TXT)", zip_buffer.getvalue(), "words.zip", "application/zip")
        else:
            st.warning("æœªæå–åˆ°æ–‡æœ¬ã€‚")

# ==================== é¡µé¢ 2: å…¬å…±è¯ä¹¦åº“ ====================
elif page == "ğŸ“š å…¬å…±è¯ä¹¦åº“":
    st.title("ğŸ“š å…¬å…±è¯ä¹¦åº“")
    st.markdown("è¿™é‡Œå­˜æ”¾äº†ç«™é•¿ç²¾é€‰çš„ç”Ÿè¯æœ¬ï¼Œå¤§å®¶å¯ä»¥å…è´¹ä¸‹è½½ã€‚")
    
    # å®šä¹‰ä¹¦æ¶æ–‡ä»¶å¤¹è·¯å¾„
    LIBRARY_DIR = "library"
    
    # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(LIBRARY_DIR):
        os.makedirs(LIBRARY_DIR)
        st.info(f"ä¹¦æ¶ä¸ºç©ºã€‚è¯·åœ¨ GitHub ä»“åº“ä¸­åˆ›å»º '{LIBRARY_DIR}' æ–‡ä»¶å¤¹å¹¶ä¸Šä¼  .txt æ–‡ä»¶ã€‚")
    
    # è¯»å–æ–‡ä»¶å¤¹é‡Œçš„æ–‡ä»¶
    files = [f for f in os.listdir(LIBRARY_DIR) if f.endswith(".txt")]
    
    if not files:
        st.warning("ğŸ“­ ä¹¦æ¶ç›®å‰æ˜¯ç©ºçš„ï¼Œè¯·ç¨åå†æ¥ï¼")
    else:
        # ç”¨ä¸¤åˆ—å¸ƒå±€å±•ç¤º
        col1, col2 = st.columns(2)
        for i, filename in enumerate(files):
            file_path = os.path.join(LIBRARY_DIR, filename)
            
            # è¯»å–æ–‡ä»¶å†…å®¹ç”¨äºä¸‹è½½
            with open(file_path, "r", encoding="utf-8") as f:
                file_content = f.read()
            
            # è®¡ç®—å•è¯æ•°
            word_count = len(file_content.splitlines())
            
            # åœ¨åˆ—ä¸­å±•ç¤º
            with (col1 if i % 2 == 0 else col2):
                with st.container(border=True):
                    st.subheader(f"ğŸ“„ {filename}")
                    st.caption(f"åŒ…å«å•è¯æ•°: {word_count}")
                    st.download_button(
                        label=f"ğŸ“¥ ä¸‹è½½ {filename}",
                        data=file_content,
                        file_name=filename,
                        mime="text/plain"
                    )

import streamlit as st
import io
import re
import zipfile
import math
import chardet
import os
import json
import random
import time
from github import Github

# NLP Imports
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet, stopwords
from nltk import pos_tag

# DOCX Import
from docx import Document

# Optional Spacy
try:
    import spacy
    _HAS_SPACY = True
except ImportError:
    _HAS_SPACY = False

# ------------------ é¡µé¢é…ç½® & ç°ä»£ CSS æ³¨å…¥ ------------------
st.set_page_config(
    page_title="VocabMaster | æ™ºèƒ½è¯ä¹¦å·¥åŠ", 
    page_icon="âš¡", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# æ³¨å…¥è‡ªå®šä¹‰ CSS ä»¥æå‡è´¨æ„Ÿ
st.markdown("""
<style>
    /* å…¨å±€å­—ä½“ä¸èƒŒæ™¯ä¼˜åŒ– */
    .stApp {
        background-color: #f8f9fa;
    }
    
    /* æ ‡é¢˜æ ·å¼ */
    h1 {
        font-family: 'Inter', sans-serif;
        font-weight: 700;
        color: #1e293b;
        letter-spacing: -0.5px;
    }
    h2, h3 {
        color: #334155;
    }

    /* æŒ‰é’®æ ·å¼é‡æ„ - æ›´æœ‰è§¦æ„Ÿ */
    div.stButton > button {
        border-radius: 12px;
        height: 3em;
        font-weight: 600;
        border: none;
        box-shadow: 0 4px 6px rgba(0,0,0,0.05);
        transition: all 0.3s ease;
    }
    div.stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 12px rgba(0,0,0,0.1);
    }
    /* ä¸»æŒ‰é’®ç‰¹æ®Šæ ·å¼ */
    div.stButton > button[kind="primary"] {
        background: linear-gradient(135deg, #6366f1 0%, #4f46e5 100%);
        color: white;
    }

    /* å¡ç‰‡å®¹å™¨æ ·å¼ */
    [data-testid="stExpander"] {
        border: none;
        box-shadow: 0 2px 5px rgba(0,0,0,0.03);
        background-color: white;
        border-radius: 10px;
    }
    
    /* Metric æŒ‡æ ‡å¡ç‰‡ */
    [data-testid="stMetric"] {
        background-color: white;
        padding: 15px;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        border: 1px solid #e2e8f0;
    }
    
    /* ä¾§è¾¹æ å¾®è°ƒ */
    section[data-testid="stSidebar"] {
        background-color: #ffffff;
        border-right: 1px solid #f1f5f9;
    }
</style>
""", unsafe_allow_html=True)

# ------------------ ç¼“å­˜èµ„æºåŠ è½½ (ä¿æŒä¸å˜) ------------------
@st.cache_resource
def download_nltk_resources():
    resources = ["punkt", "averaged_perceptron_tagger", "averaged_perceptron_tagger_eng", "wordnet", "omw-1.4", "stopwords"]
    for r in resources:
        try:
            nltk.data.find(f'tokenizers/{r}')
        except LookupError:
            nltk.download(r, quiet=True)
        except ValueError:
            nltk.download(r, quiet=True)

@st.cache_resource
def load_spacy_model():
    if _HAS_SPACY:
        try:
            return spacy.load("en_core_web_sm", disable=["ner", "parser"])
        except Exception:
            return None
    return None

download_nltk_resources()
nlp_spacy = load_spacy_model()

# ------------------ æ ¸å¿ƒé€»è¾‘å‡½æ•° (ä¿æŒä¸å˜) ------------------
def save_to_github_library(filename, content, title, desc):
    """ä¸Šä¼ åˆ° GitHub"""
    try:
        if "GITHUB_TOKEN" not in st.secrets:
            st.error("ğŸ”’ æœªé…ç½® GitHub Tokenï¼Œæ— æ³•è¿æ¥äº‘ç«¯æ•°æ®åº“ã€‚")
            return

        token = st.secrets["GITHUB_TOKEN"]
        username = st.secrets["GITHUB_USERNAME"]
        repo_name = st.secrets["GITHUB_REPO"]
        
        g = Github(token)
        repo = g.get_repo(f"{username}/{repo_name}")
        
        library_path = f"library/{filename}"
        info_path = "library/info.json"
        
        try:
            contents = repo.get_contents(library_path)
            repo.update_file(library_path, f"Update {filename}", content, contents.sha)
        except:
            repo.create_file(library_path, f"Create {filename}", content)

        try:
            info_contents = repo.get_contents(info_path)
            info_data = json.loads(info_contents.decoded_content.decode("utf-8"))
        except:
            info_data = {}
            info_contents = None

        info_data[filename] = {
            "title": title,
            "desc": desc,
            "date": time.strftime("%Y-%m-%d"),
            "author": "User" 
        }
        
        new_info_str = json.dumps(info_data, indent=2, ensure_ascii=False)
        if info_contents:
            repo.update_file(info_path, "Update info.json", new_info_str, info_contents.sha)
        else:
            repo.create_file(info_path, "Create info.json", new_info_str)
            
        st.toast("âœ… å‘å¸ƒæˆåŠŸï¼å·²åŒæ­¥è‡³å…¨çƒå…¬å…±åº“", icon="ğŸŒ")
        time.sleep(1)
        st.rerun() # åˆ·æ–°é¡µé¢
        
    except Exception as e:
        st.error(f"è¿æ¥äº‘ç«¯å¤±è´¥: {e}")

def extract_text_from_bytes(file_obj, filename):
    if '.' in filename: ext = filename.split('.')[-1].lower()
    else: ext = 'txt'
    text = ""
    try:
        if ext == 'docx':
            doc = Document(file_obj)
            text_content = []
            for p in doc.paragraphs:
                if p.text.strip(): text_content.append(p.text)
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if cell.text.strip(): text_content.append(cell.text)
            text = "\n".join(text_content)
        elif ext == 'doc':
            return ""
        else:
            raw = file_obj.read()
            enc = chardet.detect(raw).get('encoding') or 'utf-8'
            text = raw.decode(enc, errors='ignore')
    except Exception: return ""
    
    if ext in ['srt', 'vtt', 'ass']:
        clean_text = re.sub(r"<.*?>", "", text)
        clean_text = re.sub(r"\d{2}:\d{2}:\d{2},\d{3} --> \d{2}:\d{2}:\d{2},\d{3}", "", clean_text)
        return clean_text
    return text

def process_words(all_text, mode, min_len, filter_set=None):
    TOKEN_RE = re.compile(r"[A-Za-z-]+")
    cleaned = [re.sub(r'[^a-z]', '', w.lower()) for w in TOKEN_RE.findall(all_text) if w]
    lemmatized = []
    
    if mode == "spacy" and nlp_spacy:
        chunk_size = 50000
        chunks = [cleaned[i:i + chunk_size] for i in range(0, len(cleaned), chunk_size)]
        for chunk in chunks:
            doc = nlp_spacy(" ".join(chunk))
            for token in doc:
                lw = token.lemma_.lower()
                if lw.isalpha() and wordnet.synsets(lw): lemmatized.append(lw)
    else:
        lemmatizer = WordNetLemmatizer()
        for w, tag in pos_tag(cleaned):
            wn = {'J':wordnet.ADJ,'V':wordnet.VERB,'N':wordnet.NOUN,'R':wordnet.ADV}.get(tag[0], None)
            lw = lemmatizer.lemmatize(w, wn) if wn else lemmatizer.lemmatize(w)
            if wordnet.synsets(lw): lemmatized.append(lw)

    seen = set()
    final_words = []
    sys_stopwords = set(stopwords.words('english'))
    for w in lemmatized:
        if len(w) < min_len: continue
        if w in sys_stopwords: continue
        if filter_set and w in filter_set: continue
        if w not in seen:
            seen.add(w)
            final_words.append(w)
    return final_words

# ------------------ ç°ä»£ UI æ¶æ„ ------------------

# ä¾§è¾¹æ ï¼šæç®€é£æ ¼
with st.sidebar:
    st.image("https://img.icons8.com/fluency/96/dictionary.png", width=60)
    st.title("VocabMaster")
    st.caption("AI é©±åŠ¨çš„è¯æ±‡æ„å»ºå·¥å…·")
    st.markdown("---")
    
    menu = st.radio(
        "å¯¼èˆª", 
        ["âš¡ æ™ºèƒ½æå– (Extract)", "ğŸŒ æ¢ç´¢è¯åº“ (Explore)"],
        label_visibility="collapsed"
    )
    
    st.markdown("---")
    st.markdown("""
    <div style='background: #e0e7ff; padding: 10px; border-radius: 8px; color: #3730a3; font-size: 0.85em;'>
    <b>ğŸ’¡ Pro Tip:</b><br>ä½¿ç”¨ Spacy å¼•æ“å¯ä»¥è·å¾—æ›´å‡†ç¡®çš„è¯å½¢è¿˜åŸæ•ˆæœã€‚
    </div>
    """, unsafe_allow_html=True)

# === é¡µé¢ 1: åˆ¶ä½œç”Ÿè¯æœ¬ ===
if "æå–" in menu:
    st.title("âš¡ æ™ºèƒ½ç”Ÿè¯æå–")
    st.markdown("ä¸Šä¼ å­—å¹•æˆ–æ–‡æ¡£ï¼ŒAI è‡ªåŠ¨æ¸…æ´—ã€è¿˜åŸå¹¶ç”Ÿæˆé«˜é¢‘ç”Ÿè¯è¡¨ã€‚")
    
    # çŠ¶æ€åˆå§‹åŒ–
    if 'result_words' not in st.session_state: st.session_state.result_words = []
    
    # --- æ­¥éª¤ 1: é…ç½®ä¸ä¸Šä¼  (å¡ç‰‡å¼å¸ƒå±€) ---
    with st.container():
        c1, c2 = st.columns([1.5, 3], gap="large")
        
        with c1:
            st.subheader("1ï¸âƒ£ å‚æ•°é…ç½®")
            with st.expander("ğŸ› ï¸ é«˜çº§è®¾ç½®", expanded=True):
                nlp_mode = st.selectbox("AI å¼•æ“", ["nltk (æé€Ÿç‰ˆ)", "spacy (ç²¾å‡†ç‰ˆ)"])
                mode_key = "spacy" if "spacy" in nlp_mode else "nltk"
                min_len = st.slider("æœ€çŸ­è¯é•¿", 2, 8, 3)
                sort_order = st.selectbox("æ’åºé€»è¾‘", ["æŒ‰æ–‡æœ¬å‡ºç°é¡ºåº", "A-Z æ’åº", "éšæœºä¹±åº"])
                
            st.markdown("ğŸš« **ç†Ÿè¯è¿‡æ»¤**")
            filter_file = st.file_uploader("ä¸Šä¼ ç†Ÿè¯è¡¨ (.txt)", type=['txt'], label_visibility="collapsed")
            filter_set = set()
            if filter_file:
                c = filter_file.getvalue().decode("utf-8", errors='ignore')
                filter_set = set(l.strip().lower() for l in c.splitlines() if l.strip())
                st.success(f"å·²æ¿€æ´» {len(filter_set)} ä¸ªç†Ÿè¯è¿‡æ»¤")

        with c2:
            st.subheader("2ï¸âƒ£ æ–‡ä»¶æŠ•å–‚")
            upload_zone = st.container(border=True)
            with upload_zone:
                uploaded_files = st.file_uploader(
                    "æ”¯æŒ .srt, .ass, .docx, .txt (æ”¯æŒæ‰¹é‡)", 
                    type=['txt','srt','ass','vtt','docx'], 
                    accept_multiple_files=True
                )
                
                if uploaded_files:
                    st.markdown("<div style='height: 10px'></div>", unsafe_allow_html=True)
                    if st.button("ğŸš€ å¼€å§‹ AI åˆ†æ", type="primary", use_container_width=True):
                        # å¤„ç†é€»è¾‘
                        all_raw_text = []
                        progress_bar = st.progress(0)
                        
                        for idx, file in enumerate(uploaded_files):
                            text = extract_text_from_bytes(file, file.name)
                            all_raw_text.append(text)
                            progress_bar.progress((idx + 1) / len(uploaded_files))
                        
                        full_text = "\n".join(all_raw_text)
                        
                        if full_text.strip():
                            with st.spinner(f"æ­£åœ¨ä½¿ç”¨ {mode_key.upper()} å¼•æ“æ·±åº¦æ¸…æ´—ä¸­..."):
                                words = process_words(full_text, mode_key, min_len, filter_set)
                                if sort_order == "A-Z æ’åº": words.sort()
                                elif sort_order == "éšæœºä¹±åº": random.shuffle(words)
                                st.session_state.result_words = words
                                st.toast(f"å¤„ç†å®Œæˆï¼æå–äº† {len(words)} ä¸ªç”Ÿè¯", icon="âœ…")
                        else:
                            st.error("æœªèƒ½è¯†åˆ«æœ‰æ•ˆæ–‡æœ¬ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶ç¼–ç ã€‚")

    # --- æ­¥éª¤ 2: ç»“æœä»ªè¡¨ç›˜ (ä»…åœ¨æœ‰ç»“æœæ—¶æ˜¾ç¤º) ---
    if st.session_state.result_words:
        st.markdown("---")
        st.subheader("3ï¸âƒ£ åˆ†ææŠ¥å‘Š & å¯¼å‡º")
        
        words = st.session_state.result_words
        
        # ä»ªè¡¨ç›˜æŒ‡æ ‡
        m1, m2, m3, m4 = st.columns(4)
        m1.metric("ğŸ“š ç”Ÿè¯æ€»é‡", f"{len(words)}")
        m2.metric("ğŸ“„ æ¥æºæ–‡ä»¶", f"{len(uploaded_files)}")
        m3.metric("â±ï¸ é¢„ä¼°å­¦ä¹ æ—¶é•¿", f"{math.ceil(len(words)/30)} min")
        m4.metric("ğŸ›¡ï¸ è¿‡æ»¤æ•ˆç‡", "High" if filter_set else "Normal")
        
        # å†…å®¹å±•ç¤ºåŒº
        row_content = st.columns([2, 1])
        
        with row_content[0]:
            st.markdown("##### ğŸ“‹ å•è¯é¢„è§ˆ")
            # ä½¿ç”¨ dataframe å±•ç¤ºæ›´ç¾è§‚
            st.dataframe(
                [{"No.": i+1, "Word": w} for i, w in enumerate(words)],
                use_container_width=True,
                height=350,
                hide_index=True
            )
            
        with row_content[1]:
            st.markdown("##### ğŸ’¾ åŠ¨ä½œé¢æ¿")
            
            # é€‰é¡¹å¡åˆ‡æ¢æ“ä½œ
            tab_local, tab_cloud = st.tabs(["ğŸ“¥ æœ¬åœ°ä¸‹è½½", "â˜ï¸ äº‘ç«¯å‘å¸ƒ"])
            
            with tab_local:
                st.info("ç”Ÿæˆ ZIP åŒ…ä¸‹è½½åˆ°æœ¬åœ°è®¾å¤‡ã€‚")
                chunk_size = st.number_input("æ–‡ä»¶åˆ‡åˆ† (è¯/æ–‡ä»¶)", value=5000, step=1000)
                
                zip_buffer = io.BytesIO()
                num_files = math.ceil(len(words) / chunk_size)
                with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                    for i in range(num_files):
                        s = i * chunk_size
                        e = min(s + chunk_size, len(words))
                        zf.writestr(f"word_list_{i+1}.txt", "\n".join(words[s:e]))
                
                st.download_button(
                    "ğŸ“¦ ç«‹å³ä¸‹è½½", 
                    zip_buffer.getvalue(), 
                    "vocab_pack.zip", 
                    "application/zip", 
                    type="primary",
                    use_container_width=True
                )
                
            with tab_cloud:
                st.success("åˆ†äº«åˆ°å…¬å…±åº“ï¼Œå¸®åŠ©æ›´å¤šäººã€‚")
                with st.form("pub_form"):
                    s_name = st.text_input("æ–‡ä»¶å", value=f"vocab_{int(time.time())}.txt")
                    s_title = st.text_input("æ ‡é¢˜", placeholder="ä¾‹å¦‚ï¼šè€å‹è®°ç¬¬ä¸€å­£é«˜é¢‘è¯")
                    s_desc = st.text_area("æè¿°", placeholder="ç®€è¦ä»‹ç»è¯ä¹¦æ¥æº...")
                    if st.form_submit_button("ğŸŒ å‘å¸ƒ", use_container_width=True):
                        if not s_name.endswith(".txt"):
                            st.warning("æ–‡ä»¶åéœ€ä»¥ .txt ç»“å°¾")
                        else:
                            save_to_github_library(s_name, "\n".join(words), s_title, s_desc)

# === é¡µé¢ 2: å…¬å…±è¯ä¹¦åº“ ===
elif "æ¢ç´¢" in menu:
    st.title("ğŸŒ ç¤¾åŒºå…¬å…±è¯ä¹¦åº“")
    
    # æœç´¢æ 
    col_search, _ = st.columns([2, 1])
    with col_search:
        search_query = st.text_input("ğŸ” æœç´¢è¯ä¹¦...", placeholder="è¾“å…¥æ ‡é¢˜æˆ–å…³é”®è¯").lower()

    # è¯»å–æ•°æ®
    LIBRARY_DIR = "library"
    INFO_FILE = "info.json"
    if not os.path.exists(LIBRARY_DIR): os.makedirs(LIBRARY_DIR)
    
    book_info = {}
    info_path = os.path.join(LIBRARY_DIR, INFO_FILE)
    if os.path.exists(info_path):
        try:
            with open(info_path, "r", encoding="utf-8") as f: book_info = json.load(f)
        except: pass

    try:
        files = [f for f in os.listdir(LIBRARY_DIR) if f.endswith(".txt")]
    except: files = []
    
    # è¿‡æ»¤ä¸æ’åº
    filtered_files = []
    for f in files:
        meta = book_info.get(f, {})
        title = meta.get("title", f).lower()
        if search_query in title or search_query in f.lower():
            filtered_files.append(f)

    if not filtered_files:
        st.container().warning("ğŸ“­ æš‚æ— åŒ¹é…çš„è¯ä¹¦ï¼Œå»ä¸Šä¼ ç¬¬ä¸€ä¸ªå§ï¼")
    else:
        # ç½‘æ ¼å¸ƒå±€å±•ç¤º
        cols = st.columns(3)
        for i, filename in enumerate(filtered_files):
            file_path = os.path.join(LIBRARY_DIR, filename)
            try:
                with open(file_path, "r", encoding="utf-8") as f: file_content = f.read()
                word_count = len(file_content.splitlines())
                
                meta = book_info.get(filename, {})
                display_title = meta.get("title", filename)
                display_desc = meta.get("desc", "æš‚æ— æè¿°")
                pub_date = meta.get("date", "Unknown")
                
                # éšæœºç”Ÿæˆå°é¢è‰²æ¡é¢œè‰²
                colors = ["#FF6B6B", "#4ECDC4", "#45B7D1", "#96CEB4", "#FFEEAD"]
                card_color = colors[i % len(colors)]
                
                with cols[i % 3]:
                    # å¡ç‰‡å®¹å™¨
                    with st.container(border=True):
                        # è£…é¥°è‰²æ¡
                        st.markdown(f"<div style='height:8px; background-color:{card_color}; border-radius: 4px 4px 0 0; margin-bottom: 10px;'></div>", unsafe_allow_html=True)
                        
                        st.subheader(display_title)
                        st.caption(f"ğŸ“… {pub_date} | ğŸ“š {word_count} è¯")
                        
                        # æè¿°åŒºåŸŸå®šé«˜ï¼Œé˜²æ­¢å‚å·®ä¸é½
                        st.markdown(
                            f"<div style='height: 60px; overflow: hidden; color: #666; font-size: 0.9em; margin-bottom: 10px;'>{display_desc}</div>", 
                            unsafe_allow_html=True
                        )
                        
                        st.download_button(
                            f"â¬‡ï¸ ä¸‹è½½", 
                            file_content, 
                            filename, 
                            "text/plain",
                            key=f"dl_{i}",
                            use_container_width=True
                        )
            except Exception:
                continue

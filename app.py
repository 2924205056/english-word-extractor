import streamlit as st
import io
import re
import zipfile
import math
import chardet
import os
import json
import random
import time
from github import Github

# NLP Imports
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet, stopwords
from nltk import pos_tag

# DOCX Import
from docx import Document

# Optional Spacy
try:
    import spacy
    _HAS_SPACY = True
except ImportError:
    _HAS_SPACY = False

# ------------------ é¡µé¢é…ç½® ------------------
st.set_page_config(
    page_title="ä¸‡èƒ½è¯ä¹¦å¹³å° | Vocabulary Builder", 
    page_icon="ğŸ“˜", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰ç®€å•çš„ CSS ä¼˜åŒ–è§†è§‰ä½“éªŒ
st.markdown("""
<style>
    .stButton>button {
        width: 100%;
        border-radius: 8px;
        font-weight: bold;
    }
    .block-container {
        padding-top: 2rem;
        padding-bottom: 2rem;
    }
    [data-testid="stMetricValue"] {
        font-size: 24px;
        color: #4CAF50;
    }
</style>
""", unsafe_allow_html=True)

# ------------------ ç¼“å­˜èµ„æºåŠ è½½ ------------------
@st.cache_resource
def download_nltk_resources():
    resources = ["punkt", "averaged_perceptron_tagger", "averaged_perceptron_tagger_eng", "wordnet", "omw-1.4", "stopwords"]
    for r in resources:
        try:
            nltk.data.find(f'tokenizers/{r}')
        except LookupError:
            nltk.download(r, quiet=True)
        except ValueError:
            nltk.download(r, quiet=True)

@st.cache_resource
def load_spacy_model():
    if _HAS_SPACY:
        try:
            return spacy.load("en_core_web_sm", disable=["ner", "parser"])
        except Exception:
            return None
    return None

download_nltk_resources()
nlp_spacy = load_spacy_model()

# ------------------ æ ¸å¿ƒé€»è¾‘ä¿æŒä¸å˜ ------------------
def save_to_github_library(filename, content, title, desc):
    """å°†ç”Ÿæˆçš„è¯ä¹¦ä¸Šä¼ åˆ° GitHub ä»“åº“"""
    try:
        # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢æœªé…ç½® secrets æŠ¥é”™
        if "GITHUB_TOKEN" not in st.secrets:
            st.error("æœªé…ç½® GITHUB_TOKENï¼Œæ— æ³•è¿æ¥äº‘ç«¯ã€‚")
            return

        token = st.secrets["GITHUB_TOKEN"]
        username = st.secrets["GITHUB_USERNAME"]
        repo_name = st.secrets["GITHUB_REPO"]
        
        g = Github(token)
        repo = g.get_repo(f"{username}/{repo_name}")
        
        library_path = f"library/{filename}"
        info_path = "library/info.json"
        
        try:
            contents = repo.get_contents(library_path)
            repo.update_file(library_path, f"Update {filename}", content, contents.sha)
            st.toast(f"æ–‡ä»¶ {filename} å·²æ›´æ–°ï¼", icon="âœ…")
        except:
            repo.create_file(library_path, f"Create {filename}", content)
            st.toast(f"æ–‡ä»¶ {filename} å·²åˆ›å»ºï¼", icon="âœ…")

        try:
            info_contents = repo.get_contents(info_path)
            info_data = json.loads(info_contents.decoded_content.decode("utf-8"))
        except:
            info_data = {}
            info_contents = None

        info_data[filename] = {
            "title": title,
            "desc": desc,
            "timestamp": time.time() # å¢åŠ æ—¶é—´æˆ³
        }
        
        new_info_str = json.dumps(info_data, indent=2, ensure_ascii=False)
        if info_contents:
            repo.update_file(info_path, "Update info.json", new_info_str, info_contents.sha)
        else:
            repo.create_file(info_path, "Create info.json", new_info_str)
            
        st.balloons()
        st.success(f"ğŸ‰ æˆåŠŸä¿å­˜åˆ°äº‘ç«¯ï¼è¯·åˆ·æ–°é¡µé¢æŸ¥çœ‹â€œå…¬å…±è¯ä¹¦åº“â€ã€‚")
        
    except Exception as e:
        st.error(f"ä¸Šä¼ å¤±è´¥: {e}")
        st.info("è¯·æ£€æŸ¥ Streamlit Secrets é…ç½®ã€‚")

def extract_text_from_bytes(file_obj, filename):
    if '.' in filename: ext = filename.split('.')[-1].lower()
    else: ext = 'txt'
    text = ""
    try:
        if ext == 'docx':
            doc = Document(file_obj)
            text_content = []
            for p in doc.paragraphs:
                if p.text.strip(): text_content.append(p.text)
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if cell.text.strip(): text_content.append(cell.text)
            text = "\n".join(text_content)
        elif ext == 'doc':
            st.error("ä¸æ”¯æŒ .doc æ ¼å¼ï¼Œè¯·å¦å­˜ä¸º .docx åä¸Šä¼ ")
            return ""
        else:
            raw = file_obj.read()
            enc = chardet.detect(raw).get('encoding') or 'utf-8'
            text = raw.decode(enc, errors='ignore')
    except Exception as e: return ""
    
    if ext in ['srt', 'vtt', 'ass']:
        clean_text = re.sub(r"<.*?>", "", text) # åŸºç¡€æ¸…æ´—
        # é’ˆå¯¹ srt æ—¶é—´è½´çš„é¢å¤–ç®€å•æ¸…æ´—ï¼ˆå¯é€‰ï¼‰
        clean_text = re.sub(r"\d{2}:\d{2}:\d{2},\d{3} --> \d{2}:\d{2}:\d{2},\d{3}", "", clean_text)
        return clean_text
    return text

def process_words(all_text, mode, min_len, filter_set=None):
    TOKEN_RE = re.compile(r"[A-Za-z-]+")
    cleaned = [re.sub(r'[^a-z]', '', w.lower()) for w in TOKEN_RE.findall(all_text) if w]
    lemmatized = []
    
    if mode == "spacy" and nlp_spacy:
        chunk_size = 50000
        chunks = [cleaned[i:i + chunk_size] for i in range(0, len(cleaned), chunk_size)]
        for chunk in chunks:
            doc = nlp_spacy(" ".join(chunk))
            for token in doc:
                lw = token.lemma_.lower()
                if lw.isalpha() and wordnet.synsets(lw): lemmatized.append(lw)
    else:
        lemmatizer = WordNetLemmatizer()
        for w, tag in pos_tag(cleaned):
            wn = {'J':wordnet.ADJ,'V':wordnet.VERB,'N':wordnet.NOUN,'R':wordnet.ADV}.get(tag[0], None)
            lw = lemmatizer.lemmatize(w, wn) if wn else lemmatizer.lemmatize(w)
            if wordnet.synsets(lw): lemmatized.append(lw)

    seen = set()
    final_words = []
    sys_stopwords = set(stopwords.words('english'))
    for w in lemmatized:
        if len(w) < min_len: continue
        if w in sys_stopwords: continue
        if filter_set and w in filter_set: continue
        if w not in seen:
            seen.add(w)
            final_words.append(w)
    return final_words

# ------------------ UI é€»è¾‘ ------------------

# ä¾§è¾¹æ å¯¼èˆª
with st.sidebar:
    st.title("ğŸ“˜ ä¸‡èƒ½è¯ä¹¦å¹³å°")
    st.caption("æå– Â· æ•´ç† Â· åˆ†äº«")
    st.divider()
    page = st.radio("åŠŸèƒ½å¯¼èˆª:", ["ğŸ› ï¸ åˆ¶ä½œç”Ÿè¯æœ¬", "ğŸ“š å…¬å…±è¯ä¹¦åº“"], index=0)
    st.divider()
    st.markdown("ğŸ’¡ **Tips:**\næ”¯æŒå­—å¹•æ–‡ä»¶ã€æ–‡æ¡£ï¼Œè‡ªåŠ¨æå–ç”Ÿè¯å¹¶è¿˜åŸè¯å½¢ã€‚")

if page == "ğŸ› ï¸ åˆ¶ä½œç”Ÿè¯æœ¬":
    # é¡¶éƒ¨ Hero åŒºåŸŸ
    st.title("ğŸ› ï¸ è‹±è¯­ç”Ÿè¯æå–å™¨")
    st.markdown("""
    <div style='background-color: #f0f2f6; padding: 15px; border-radius: 10px; margin-bottom: 20px;'>
        ä¸Šä¼ æ‚¨çš„ <b>å­—å¹•æ–‡ä»¶</b> (.srt, .vtt) æˆ– <b>æ–‡æ¡£</b> (.docx, .txt)ï¼Œ
        AI å¼•æ“å°†è‡ªåŠ¨ä¸ºæ‚¨æå–é«˜é¢‘ç”Ÿè¯ï¼Œè¿‡æ»¤ç®€å•è¯æ±‡ã€‚
    </div>
    """, unsafe_allow_html=True)

    # å¸ƒå±€ï¼šå·¦ä¾§è®¾ç½®ï¼Œå³ä¾§ä¸»æ“ä½œ
    col_settings, col_main = st.columns([1, 3])

    with col_settings:
        st.subheader("âš™ï¸ æå–è®¾ç½®")
        
        with st.expander("ğŸ§  å¤„ç†å¼•æ“", expanded=True):
            nlp_mode = st.selectbox("é€‰æ‹©å¼•æ“", ["nltk (å¿«é€Ÿ)", "spacy (ç²¾å‡†)"], help="Spacy è¿˜åŸè¯å½¢æ›´å‡†ï¼Œä½†é€Ÿåº¦è¾ƒæ…¢")
            mode_key = "spacy" if "spacy" in nlp_mode else "nltk"
        
        with st.expander("ğŸ“ è¿‡æ»¤è§„åˆ™", expanded=True):
            min_len = st.number_input("å•è¯æœ€çŸ­é•¿åº¦", value=3, min_value=1)
            filter_file = st.file_uploader("ä¸Šä¼ ç†Ÿè¯è¡¨ (txt)", type=['txt'], help="ä¸Šä¼ åŒ…å«æ‚¨å·²è®¤è¯†å•è¯çš„txtæ–‡ä»¶ï¼Œä¸€è¡Œä¸€ä¸ª")
        
        with st.expander("ğŸ“‚ è¾“å‡ºæ ¼å¼"):
            chunk_size = st.number_input("å•æ–‡ä»¶è¯æ•°é™åˆ¶", value=5000)
            sort_order = st.radio("æ’åºæ–¹å¼", ["æŒ‰æ–‡æœ¬å‡ºç°é¡ºåº", "A-Z æ’åº", "éšæœºæ‰“ä¹±"])

        filter_set = set()
        if filter_file:
            c = filter_file.getvalue().decode("utf-8", errors='ignore')
            filter_set = set(l.strip().lower() for l in c.splitlines() if l.strip())
            st.caption(f"âœ… å·²åŠ è½½ {len(filter_set)} ä¸ªç†Ÿè¯")

    with col_main:
        st.subheader("ğŸ“‚ æ–‡ä»¶ä¸Šä¼ ")
        uploaded_files = st.file_uploader(
            "æ‹–æ‹½æ–‡ä»¶åˆ°æ­¤å¤„", 
            type=['txt','srt','ass','vtt','docx'], 
            accept_multiple_files=True,
            label_visibility="collapsed"
        )

        if 'result_words' not in st.session_state:
            st.session_state.result_words = []

        # æ“ä½œæŒ‰é’®åŒºåŸŸ
        if uploaded_files:
            if st.button("ğŸš€ å¼€å§‹æ™ºèƒ½æå–", type="primary", use_container_width=True):
                all_raw_text = []
                for file in uploaded_files:
                    text = extract_text_from_bytes(file, file.name)
                    all_raw_text.append(text)
                
                full_text = "\n".join(all_raw_text)
                if full_text.strip():
                    with st.status("æ­£åœ¨åˆ†ææ–‡æœ¬...", expanded=True) as status:
                        st.write("æ­£åœ¨è¯»å–æ–‡ä»¶å†…å®¹...")
                        time.sleep(0.5)
                        st.write(f"æ­£åœ¨ä½¿ç”¨ {mode_key.upper()} å¼•æ“æå–å•è¯...")
                        words = process_words(full_text, mode_key, min_len, filter_set)
                        
                        st.write("æ­£åœ¨åº”ç”¨æ’åºè§„åˆ™...")
                        if sort_order == "A-Z æ’åº":
                            words.sort()
                        elif sort_order == "éšæœºæ‰“ä¹±":
                            random.shuffle(words)
                        
                        st.session_state.result_words = words
                        status.update(label="âœ… æå–å®Œæˆï¼", state="complete", expanded=False)
                else:
                    st.warning("âš ï¸ æœªèƒ½ä»æ–‡ä»¶ä¸­æå–åˆ°æœ‰æ•ˆæ–‡æœ¬ã€‚")

    # ç»“æœå±•ç¤ºåŒºåŸŸï¼ˆå…¨å®½ï¼‰
    if st.session_state.result_words:
        result_words = st.session_state.result_words
        st.divider()
        
        # ç»Ÿè®¡ä»ªè¡¨ç›˜
        m1, m2, m3 = st.columns(3)
        m1.metric("æå–å•è¯æ€»æ•°", len(result_words))
        m2.metric("æ¥æºæ–‡ä»¶æ•°", len(uploaded_files) if uploaded_files else 0)
        m3.metric("è¿‡æ»¤æ¨¡å¼", "æ™ºèƒ½è¿‡æ»¤ + ç†Ÿè¯è¡¨" if filter_set else "æ™ºèƒ½è¿‡æ»¤")

        # é¢„è§ˆä¸å¯¼å‡º åˆ†æ 
        st.subheader("ğŸ‘€ ç»“æœé¢„è§ˆä¸å¯¼å‡º")
        
        # ä½¿ç”¨ Dataframe å±•ç¤ºï¼Œæ¯”çº¯æ–‡æœ¬æ›´å¥½çœ‹
        with st.expander("å±•å¼€æŸ¥çœ‹å•è¯åˆ—è¡¨", expanded=False):
            # ç®€å•çš„åˆ—è¡¨è½¬DataFrameï¼Œæ–¹ä¾¿å±•ç¤º
            st.dataframe(
                [{"Index": i+1, "Word": w} for i, w in enumerate(result_words)],
                use_container_width=True,
                height=300,
                hide_index=True
            )

        # ä½¿ç”¨ Tabs ä¼˜åŒ–å¯¼å‡ºåŒºåŸŸï¼ŒèŠ‚çœç©ºé—´
        tab1, tab2 = st.tabs(["ğŸ“¥ æœ¬åœ°ä¸‹è½½ (Download)", "â˜ï¸ å‘å¸ƒåˆ°äº‘ç«¯ (Publish)"])
        
        with tab1:
            st.info("å°†å•è¯è¡¨æ‰“åŒ…ä¸‹è½½åˆ°æœ¬åœ°ã€‚")
            zip_buffer = io.BytesIO()
            num_files = math.ceil(len(result_words) / chunk_size)
            
            with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                for i in range(num_files):
                    s = i * chunk_size
                    e = min(s + chunk_size, len(result_words))
                    zf.writestr(f"word_list_{i+1}.txt", "\n".join(result_words[s:e]))
            
            col_dl_btn, _ = st.columns([1, 2])
            with col_dl_btn:
                st.download_button(
                    "ğŸ“¦ ä¸‹è½½ ZIP å‹ç¼©åŒ…", 
                    zip_buffer.getvalue(), 
                    "vocabulary_words.zip", 
                    "application/zip",
                    type="primary"
                )

        with tab2:
            st.success("å°†æ‚¨çš„ç”Ÿè¯æœ¬åˆ†äº«ç»™æ‰€æœ‰äººã€‚")
            with st.form("upload_form"):
                f_col1, f_col2 = st.columns(2)
                with f_col1:
                    save_name = st.text_input("æ–‡ä»¶å (å¿…é¡»åŒ…å« .txt)", value=f"vocab_{int(time.time())}.txt")
                    save_title = st.text_input("è¯ä¹¦æ ‡é¢˜", value="æˆ‘çš„ä¸“å±ç”Ÿè¯æœ¬")
                with f_col2:
                    save_desc = st.text_area("è¯ä¹¦æè¿° / æ¥æºè¯´æ˜", value="æå–è‡ª...", height=103)
                
                submitted = st.form_submit_button("ğŸš€ ç¡®è®¤ä¸Šä¼ å¹¶å‘å¸ƒ")
                
                if submitted:
                    if not save_name.endswith(".txt"):
                        st.error("âŒ æ–‡ä»¶åå¿…é¡»ä»¥ .txt ç»“å°¾")
                    else:
                        content_str = "\n".join(result_words)
                        with st.spinner("æ­£åœ¨è¿æ¥ GitHub ä»“åº“..."):
                            save_to_github_library(save_name, content_str, save_title, save_desc)

elif page == "ğŸ“š å…¬å…±è¯ä¹¦åº“":
    st.title("ğŸ“š å…¬å…±è¯ä¹¦åº“")
    st.markdown("è¿™é‡Œå­˜æ”¾äº†ç¤¾åŒºåˆ†äº«çš„ç²¾é€‰ç”Ÿè¯æœ¬ï¼Œæ‚¨å¯ä»¥ **å…è´¹é¢„è§ˆ** æˆ– **ä¸‹è½½**ã€‚")
    st.divider()
    
    LIBRARY_DIR = "library"
    INFO_FILE = "info.json"
    
    if not os.path.exists(LIBRARY_DIR):
        try:
            os.makedirs(LIBRARY_DIR)
        except:
            pass
    
    book_info = {}
    info_path = os.path.join(LIBRARY_DIR, INFO_FILE)
    if os.path.exists(info_path):
        try:
            with open(info_path, "r", encoding="utf-8") as f: book_info = json.load(f)
        except: pass

    try:
        files = [f for f in os.listdir(LIBRARY_DIR) if f.endswith(".txt")]
    except FileNotFoundError:
        files = []
    
    if not files:
        st.container().warning("ğŸ“­ æš‚æ— å…¬å…±è¯ä¹¦ï¼Œå¿«å»â€œåˆ¶ä½œç”Ÿè¯æœ¬â€é‡Œä¸Šä¼ æ‚¨çš„ç¬¬ä¸€æœ¬å§ï¼")
    else:
        # ä¼˜åŒ–ï¼šä½¿ç”¨ Grid å¸ƒå±€æ˜¾ç¤ºå¡ç‰‡ï¼Œè€Œä¸æ˜¯ç®€å•çš„ä¸¤åˆ—
        cols = st.columns(3) # 3åˆ—å¸ƒå±€
        for i, filename in enumerate(files):
            file_path = os.path.join(LIBRARY_DIR, filename)
            try:
                with open(file_path, "r", encoding="utf-8") as f: file_content = f.read()
                word_count = len(file_content.splitlines())
                
                meta = book_info.get(filename, {})
                display_title = meta.get("title", filename)
                display_desc = meta.get("desc", "æš‚æ— æè¿°")
                
                # è½®è¯¢æ”¾å…¥ 3 åˆ—ä¸­
                with cols[i % 3]:
                    with st.container(border=True):
                        st.subheader(f"ğŸ“„ {display_title}")
                        st.markdown(f"**å•è¯æ•°:** `{word_count}`")
                        st.caption(display_desc)
                        
                        # é¢„è§ˆå‰å‡ ä¸ªè¯
                        preview_words = file_content.splitlines()[:5]
                        st.text("Preview: " + ", ".join(preview_words) + "...")
                        
                        st.download_button(
                            f"ğŸ“¥ ä¸‹è½½", 
                            file_content, 
                            filename, 
                            "text/plain",
                            key=f"dl_{i}",
                            use_container_width=True
                        )
            except Exception as e:
                continue

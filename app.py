import streamlit as st
import io
import re
import zipfile
import math
import chardet
import os
import json
import random # å¿…é¡»å¯¼å…¥ random æ‰èƒ½æ‰“ä¹±é¡ºåº
from github import Github

# NLP Imports
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet, stopwords
from nltk import pos_tag

# DOCX Import
from docx import Document

# Optional Spacy
try:
    import spacy
    _HAS_SPACY = True
except ImportError:
    _HAS_SPACY = False

# ------------------ é¡µé¢é…ç½® ------------------
st.set_page_config(page_title="ä¸‡èƒ½è¯ä¹¦å¹³å°", page_icon="ğŸ“˜", layout="wide")

# ------------------ ç¼“å­˜èµ„æºåŠ è½½ ------------------
@st.cache_resource
def download_nltk_resources():
    resources = ["punkt", "averaged_perceptron_tagger", "averaged_perceptron_tagger_eng", "wordnet", "omw-1.4", "stopwords"]
    for r in resources:
        try:
            nltk.data.find(f'tokenizers/{r}')
        except LookupError:
            nltk.download(r, quiet=True)
        except ValueError:
            nltk.download(r, quiet=True)

@st.cache_resource
def load_spacy_model():
    if _HAS_SPACY:
        try:
            return spacy.load("en_core_web_sm", disable=["ner", "parser"])
        except Exception:
            return None
    return None

download_nltk_resources()
nlp_spacy = load_spacy_model()

# ------------------ GitHub ä¸Šä¼ å‡½æ•° ------------------
def save_to_github_library(filename, content, title, desc):
    """å°†ç”Ÿæˆçš„è¯ä¹¦ä¸Šä¼ åˆ° GitHub ä»“åº“"""
    try:
        token = st.secrets["GITHUB_TOKEN"]
        username = st.secrets["GITHUB_USERNAME"]
        repo_name = st.secrets["GITHUB_REPO"]
        
        g = Github(token)
        repo = g.get_repo(f"{username}/{repo_name}")
        
        library_path = f"library/{filename}"
        info_path = "library/info.json"
        
        # åˆ›å»º/æ›´æ–°è¯ä¹¦æ–‡ä»¶
        try:
            contents = repo.get_contents(library_path)
            repo.update_file(library_path, f"Update {filename}", content, contents.sha)
            st.toast(f"æ–‡ä»¶ {filename} å·²æ›´æ–°ï¼", icon="âœ…")
        except:
            repo.create_file(library_path, f"Create {filename}", content)
            st.toast(f"æ–‡ä»¶ {filename} å·²åˆ›å»ºï¼", icon="âœ…")

        # æ›´æ–° info.json
        try:
            info_contents = repo.get_contents(info_path)
            info_data = json.loads(info_contents.decoded_content.decode("utf-8"))
        except:
            info_data = {}
            info_contents = None

        info_data[filename] = {
            "title": title,
            "desc": desc
        }
        
        new_info_str = json.dumps(info_data, indent=2, ensure_ascii=False)
        if info_contents:
            repo.update_file(info_path, "Update info.json", new_info_str, info_contents.sha)
        else:
            repo.create_file(info_path, "Create info.json", new_info_str)
            
        st.success(f"ğŸ‰ æˆåŠŸä¿å­˜åˆ°äº‘ç«¯ï¼è¯·åˆ·æ–°é¡µé¢æŸ¥çœ‹â€œå…¬å…±è¯ä¹¦åº“â€ã€‚")
        
    except Exception as e:
        st.error(f"ä¸Šä¼ å¤±è´¥: {e}")
        st.error("è¯·æ£€æŸ¥ Streamlit Secrets é…ç½®æ˜¯å¦æ­£ç¡®ã€‚")

# ------------------ æ–‡æœ¬å¤„ç†é€»è¾‘ ------------------
def extract_text_from_bytes(file_obj, filename):
    if '.' in filename: ext = filename.split('.')[-1].lower()
    else: ext = 'txt'
    text = ""
    try:
        if ext == 'docx':
            doc = Document(file_obj)
            text_content = []
            for p in doc.paragraphs:
                if p.text.strip(): text_content.append(p.text)
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if cell.text.strip(): text_content.append(cell.text)
            text = "\n".join(text_content)
        elif ext == 'doc':
            st.error("ä¸æ”¯æŒ .docï¼Œè¯·è½¬å­˜ä¸º .docx")
            return ""
        else:
            raw = file_obj.read()
            enc = chardet.detect(raw).get('encoding') or 'utf-8'
            text = raw.decode(enc, errors='ignore')
    except Exception as e: return ""
    
    if ext in ['srt', 'vtt', 'ass']:
        clean_text = re.sub(r"<.*?>", "", text)
        return clean_text
    return text

def process_words(all_text, mode, min_len, filter_set=None):
    TOKEN_RE = re.compile(r"[A-Za-z-]+")
    cleaned = [re.sub(r'[^a-z]', '', w.lower()) for w in TOKEN_RE.findall(all_text) if w]
    lemmatized = []
    
    if mode == "spacy" and nlp_spacy:
        chunk_size = 50000
        chunks = [cleaned[i:i + chunk_size] for i in range(0, len(cleaned), chunk_size)]
        for chunk in chunks:
            doc = nlp_spacy(" ".join(chunk))
            for token in doc:
                lw = token.lemma_.lower()
                if lw.isalpha() and wordnet.synsets(lw): lemmatized.append(lw)
    else:
        lemmatizer = WordNetLemmatizer()
        for w, tag in pos_tag(cleaned):
            wn = {'J':wordnet.ADJ,'V':wordnet.VERB,'N':wordnet.NOUN,'R':wordnet.ADV}.get(tag[0], None)
            lw = lemmatizer.lemmatize(w, wn) if wn else lemmatizer.lemmatize(w)
            if wordnet.synsets(lw): lemmatized.append(lw)

    seen = set()
    final_words = []
    sys_stopwords = set(stopwords.words('english'))
    for w in lemmatized:
        if len(w) < min_len: continue
        if w in sys_stopwords: continue
        if filter_set and w in filter_set: continue
        if w not in seen:
            seen.add(w)
            final_words.append(w)
    return final_words

# ------------------ UI é€»è¾‘ ------------------

st.sidebar.title("åŠŸèƒ½å¯¼èˆª")
page = st.sidebar.radio("é€‰æ‹©æ¨¡å¼:", ["ğŸ› ï¸ åˆ¶ä½œç”Ÿè¯æœ¬", "ğŸ“š å…¬å…±è¯ä¹¦åº“"])

if page == "ğŸ› ï¸ åˆ¶ä½œç”Ÿè¯æœ¬":
    st.title("ğŸ› ï¸ è‹±è¯­ç”Ÿè¯æå–å™¨")
    # æ¢å¤å¼•å¯¼è¯­
    st.markdown("""
    ä¸Šä¼ å­—å¹•æ–‡ä»¶ (`.srt`, `.ass`, `.vtt`) æˆ–æ–‡æ¡£ (`.docx`, `.txt`)ï¼Œ
    ç³»ç»Ÿå°†è‡ªåŠ¨æå–å•è¯ã€è¿˜åŸè¯å½¢ã€å»é™¤ç®€å•è¯ï¼Œç”Ÿæˆå•è¯åˆ—è¡¨ã€‚
    """)

    with st.sidebar:
        st.divider()
        st.header("âš™ï¸ æå–è®¾ç½®")
        
        nlp_mode = st.selectbox("å¼•æ“", ["nltk (å¿«)", "spacy (å‡†)"])
        mode_key = "spacy" if "spacy" in nlp_mode else "nltk"
        
        min_len = st.number_input("æœ€çŸ­è¯é•¿", value=3)
        
        # æ¢å¤åˆ‡åˆ†å’Œæ’åºé€‰é¡¹
        chunk_size = st.number_input("è¾“å‡ºæ–‡ä»¶åˆ‡åˆ†å¤§å°", value=5000)
        sort_order = st.radio("æ’åºæ–¹å¼", ["æŒ‰æ–‡æœ¬å‡ºç°é¡ºåº", "A-Z æ’åº", "éšæœºæ‰“ä¹±"])
        
        st.divider()
        
        filter_file = st.file_uploader("è¿‡æ»¤è¯è¡¨", type=['txt'])
        filter_set = set()
        if filter_file:
            c = filter_file.getvalue().decode("utf-8", errors='ignore')
            filter_set = set(l.strip().lower() for l in c.splitlines() if l.strip())
            st.success(f"å·²åŠ è½½ {len(filter_set)} ä¸ªè¿‡æ»¤è¯")

    uploaded_files = st.file_uploader("ä¸Šä¼ æ–‡ä»¶", type=['txt','srt','ass','vtt','docx'], accept_multiple_files=True)

    if 'result_words' not in st.session_state:
        st.session_state.result_words = []

    if uploaded_files and st.button("ğŸš€ å¼€å§‹æå–", type="primary"):
        all_raw_text = []
        for file in uploaded_files:
            text = extract_text_from_bytes(file, file.name)
            all_raw_text.append(text)
        
        full_text = "\n".join(all_raw_text)
        if full_text.strip():
            with st.spinner("åˆ†æä¸­..."):
                words = process_words(full_text, mode_key, min_len, filter_set)
                
                # åº”ç”¨æ’åºé€»è¾‘
                if sort_order == "A-Z æ’åº":
                    words.sort()
                elif sort_order == "éšæœºæ‰“ä¹±":
                    random.shuffle(words)
                
                st.session_state.result_words = words
                
            st.success(f"æå–æˆåŠŸï¼å…± {len(st.session_state.result_words)} ä¸ªå•è¯ã€‚")
        else:
            st.warning("æœªæå–åˆ°æ–‡æœ¬ã€‚")

    if st.session_state.result_words:
        result_words = st.session_state.result_words
        
        with st.expander("ğŸ‘€ é¢„è§ˆç»“æœ", expanded=False):
            st.write(", ".join(result_words[:100]))

        st.divider()
        col_local, col_cloud = st.columns(2)
        
        with col_local:
            st.subheader("ğŸ“¥ æœ¬åœ°ä¸‹è½½")
            zip_buffer = io.BytesIO()
            num_files = math.ceil(len(result_words) / chunk_size)
            
            with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                for i in range(num_files):
                    s = i * chunk_size
                    e = min(s + chunk_size, len(result_words))
                    zf.writestr(f"word_list_{i+1}.txt", "\n".join(result_words[s:e]))
                    
            st.download_button("ä¸‹è½½ ZIP å‹ç¼©åŒ…", zip_buffer.getvalue(), "words.zip", "application/zip")

        with col_cloud:
            st.subheader("â˜ï¸ ä¿å­˜åˆ°å…¬å…±åº“")
            st.caption("å°†ç”Ÿæˆçš„å•è¯æœ¬ä¸Šä¼ åˆ° GitHubï¼Œåˆ†äº«ç»™æ‰€æœ‰äººã€‚")
            with st.form("upload_form"):
                save_name = st.text_input("æ–‡ä»¶å (å¿…é¡»ä»¥ .txt ç»“å°¾)", value="my_new_book.txt")
                save_title = st.text_input("è¯ä¹¦æ ‡é¢˜", value="æˆ‘çš„ç”Ÿè¯æœ¬")
                save_desc = st.text_area("è¯ä¹¦æè¿°", value="è¿™æ˜¯ä¸€æœ¬å…³äº...")
                submitted = st.form_submit_button("ç¡®è®¤ä¸Šä¼ å¹¶å‘å¸ƒ")
                
                if submitted:
                    if not save_name.endswith(".txt"):
                        st.error("æ–‡ä»¶åå¿…é¡»åŒ…å« .txt")
                    else:
                        content_str = "\n".join(result_words)
                        with st.spinner("æ­£åœ¨è¿æ¥ GitHub ä¸Šä¼ ä¸­..."):
                            save_to_github_library(save_name, content_str, save_title, save_desc)

elif page == "ğŸ“š å…¬å…±è¯ä¹¦åº“":
    st.title("ğŸ“š å…¬å…±è¯ä¹¦åº“")
    st.markdown("è¿™é‡Œå­˜æ”¾äº†ç«™é•¿ç²¾é€‰çš„ç”Ÿè¯æœ¬ï¼Œå¤§å®¶å¯ä»¥å…è´¹ä¸‹è½½ã€‚")
    
    LIBRARY_DIR = "library"
    INFO_FILE = "info.json"
    
    if not os.path.exists(LIBRARY_DIR):
        os.makedirs(LIBRARY_DIR)
    
    book_info = {}
    info_path = os.path.join(LIBRARY_DIR, INFO_FILE)
    if os.path.exists(info_path):
        try:
            with open(info_path, "r", encoding="utf-8") as f: book_info = json.load(f)
        except: pass

    files = [f for f in os.listdir(LIBRARY_DIR) if f.endswith(".txt")]
    
    if not files:
        st.warning("ğŸ“­ æš‚æ— è¯ä¹¦ï¼Œå¿«å»â€œåˆ¶ä½œç”Ÿè¯æœ¬â€é‡Œä¸Šä¼ ä¸€æœ¬å§ï¼")
    else:
        col1, col2 = st.columns(2)
        for i, filename in enumerate(files):
            file_path = os.path.join(LIBRARY_DIR, filename)
            with open(file_path, "r", encoding="utf-8") as f: file_content = f.read()
            word_count = len(file_content.splitlines())
            
            meta = book_info.get(filename, {})
            display_title = meta.get("title", filename)
            display_desc = meta.get("desc", "æš‚æ— æè¿°")
            
            with (col1 if i % 2 == 0 else col2):
                with st.container(border=True):
                    st.subheader(f"ğŸ“„ {display_title}")
                    if display_desc != "æš‚æ— æè¿°": st.info(display_desc)
                    else: st.caption("æ— è¯¦ç»†æè¿°")
                    st.caption(f"ğŸ“š å•è¯æ•°: **{word_count}**")
                    st.download_button(f"ğŸ“¥ ä¸‹è½½ {filename}", file_content, filename, "text/plain")

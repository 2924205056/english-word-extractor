import streamlit as st
import io
import re
import zipfile
import math
import chardet
import os
import json  # Êñ∞Â¢ûÔºöÁî®‰∫éËØªÂèñÊèèËø∞Êñá‰ª∂

# NLP Imports
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet, stopwords
from nltk import pos_tag

# DOCX Import
from docx import Document

# Optional Spacy
try:
    import spacy
    _HAS_SPACY = True
except ImportError:
    _HAS_SPACY = False

# ------------------ È°µÈù¢ÈÖçÁΩÆ ------------------
st.set_page_config(page_title="‰∏áËÉΩËØç‰π¶Âπ≥Âè∞", page_icon="üìò", layout="wide")

# ------------------ ÁºìÂ≠òËµÑÊ∫êÂä†ËΩΩ ------------------
@st.cache_resource
def download_nltk_resources():
    """ÈùôÈªò‰∏ãËΩΩ NLTK ËµÑÊ∫ê"""
    resources = [
        "punkt", 
        "averaged_perceptron_tagger", 
        "averaged_perceptron_tagger_eng", 
        "wordnet", 
        "omw-1.4", 
        "stopwords"
    ]
    for r in resources:
        try:
            nltk.data.find(f'tokenizers/{r}')
        except LookupError:
            nltk.download(r, quiet=True)
        except ValueError:
            nltk.download(r, quiet=True)

@st.cache_resource
def load_spacy_model():
    if _HAS_SPACY:
        try:
            return spacy.load("en_core_web_sm", disable=["ner", "parser"])
        except Exception:
            return None
    return None

download_nltk_resources()
nlp_spacy = load_spacy_model()

# ------------------ Ê†∏ÂøÉÈÄªËæëÂáΩÊï∞ ------------------

def extract_text_from_bytes(file_obj, filename):
    if '.' in filename:
        ext = filename.split('.')[-1].lower()
    else:
        ext = 'txt'
    text = ""
    try:
        if ext == 'docx':
            doc = Document(file_obj)
            text_content = []
            for p in doc.paragraphs:
                if p.text.strip(): text_content.append(p.text)
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if cell.text.strip(): text_content.append(cell.text)
            text = "\n".join(text_content)
        elif ext == 'doc':
            st.error("‰∏çÊîØÊåÅ .docÔºåËØ∑ËΩ¨Â≠ò‰∏∫ .docx")
            return ""
        else:
            raw = file_obj.read()
            enc = chardet.detect(raw).get('encoding') or 'utf-8'
            text = raw.decode(enc, errors='ignore')
    except Exception as e:
        return ""
    
    # ÁÆÄÂçïÊ∏ÖÊ¥ó
    if ext in ['srt', 'vtt', 'ass']:
        clean_text = re.sub(r"<.*?>", "", text)
        return clean_text
    return text

def get_wordnet_pos(tag):
    if tag.startswith('J'): return wordnet.ADJ
    if tag.startswith('V'): return wordnet.VERB
    if tag.startswith('N'): return wordnet.NOUN
    if tag.startswith('R'): return wordnet.ADV
    return None

def process_words(all_text, mode, min_len, filter_set=None):
    TOKEN_RE = re.compile(r"[A-Za-z-]+")
    raw_tokens = TOKEN_RE.findall(all_text)
    cleaned = [re.sub(r'[^a-z]', '', w.lower()) for w in raw_tokens]
    cleaned = [w for w in cleaned if w]
    lemmatized = []
    
    if mode == "spacy" and nlp_spacy:
        chunk_size = 50000
        chunks = [cleaned[i:i + chunk_size] for i in range(0, len(cleaned), chunk_size)]
        for chunk in chunks:
            doc = nlp_spacy(" ".join(chunk))
            for token in doc:
                lw = token.lemma_.lower()
                if lw.isalpha() and wordnet.synsets(lw): lemmatized.append(lw)
    else:
        lemmatizer = WordNetLemmatizer()
        tagged = pos_tag(cleaned)
        for w, tag in tagged:
            wn = get_wordnet_pos(tag)
            lw = lemmatizer.lemmatize(w, wn) if wn else lemmatizer.lemmatize(w)
            if wordnet.synsets(lw): lemmatized.append(lw)

    seen = set()
    final_words = []
    sys_stopwords = set(stopwords.words('english'))
    for w in lemmatized:
        if len(w) < min_len: continue
        if w in sys_stopwords: continue
        if filter_set and w in filter_set: continue
        if w not in seen:
            seen.add(w)
            final_words.append(w)
    return final_words

# ------------------ ‰∏ªÁïåÈù¢ÂØºËà™ ------------------

st.sidebar.title("ÂäüËÉΩÂØºËà™")
page = st.sidebar.radio("ÈÄâÊã©Ê®°Âºè:", ["üõ†Ô∏è Âà∂‰ΩúÁîüËØçÊú¨", "üìö ÂÖ¨ÂÖ±ËØç‰π¶Â∫ì"])

# ==================== È°µÈù¢ 1: Âà∂‰ΩúÁîüËØçÊú¨ ====================
if page == "üõ†Ô∏è Âà∂‰ΩúÁîüËØçÊú¨":
    st.title("üõ†Ô∏è Ëã±ËØ≠ÁîüËØçÊèêÂèñÂô®")
    st.markdown("‰∏ä‰º†ÊñáÊ°£ÊàñÂ≠óÂπïÔºå‰∏ÄÈîÆÊèêÂèñÁîüËØç„ÄÇ")

    with st.sidebar:
        st.divider()
        st.header("‚öôÔ∏è ÊèêÂèñËÆæÁΩÆ")
        nlp_mode = st.selectbox("ÂºïÊìé", ["nltk (Âø´)", "spacy (ÂáÜ)"], index=0)
        mode_key = "spacy" if "spacy" in nlp_mode else "nltk"
        min_len = st.number_input("ÊúÄÁü≠ËØçÈïø", value=3)
        chunk_size = st.number_input("ÂàáÂàÜÂ§ßÂ∞è", value=5000)
        filter_file = st.file_uploader("ËøáÊª§ËØçË°®", type=['txt'])
        filter_set = set()
        if filter_file:
            c = filter_file.getvalue().decode("utf-8", errors='ignore')
            filter_set = set(l.strip().lower() for l in c.splitlines() if l.strip())
            st.success(f"Â∑≤Âä†ËΩΩ {len(filter_set)} ‰∏™ËøáÊª§ËØç")

    uploaded_files = st.file_uploader("‰∏ä‰º†Êñá‰ª∂", type=['txt','srt','ass','vtt','docx'], accept_multiple_files=True)

    if uploaded_files and st.button("üöÄ ÂºÄÂßãÊèêÂèñ", type="primary"):
        all_raw_text = []
        for file in uploaded_files:
            text = extract_text_from_bytes(file, file.name)
            all_raw_text.append(text)
        
        full_text = "\n".join(all_raw_text)
        if full_text.strip():
            with st.spinner("Ê≠£Âú®ÂàÜÊûêÂçïËØç..."):
                result_words = process_words(full_text, mode_key, min_len, filter_set)
            
            st.success(f"ÊèêÂèñÊàêÂäüÔºÅÂÖ± {len(result_words)} ‰∏™ÂçïËØç„ÄÇ")
            
            with st.expander("üëÄ È¢ÑËßàÁªìÊûú"):
                st.write(", ".join(result_words[:100]))
            
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                zf.writestr("word_list.txt", "\n".join(result_words))
            
            st.download_button("üì• ‰∏ãËΩΩÁªìÊûú (TXT)", zip_buffer.getvalue(), "words.zip", "application/zip")
        else:
            st.warning("Êú™ÊèêÂèñÂà∞ÊñáÊú¨„ÄÇ")

# ==================== È°µÈù¢ 2: ÂÖ¨ÂÖ±ËØç‰π¶Â∫ì (Â∑≤Êõ¥Êñ∞ÊîØÊåÅÊèèËø∞) ====================
elif page == "üìö ÂÖ¨ÂÖ±ËØç‰π¶Â∫ì":
    st.title("üìö ÂÖ¨ÂÖ±ËØç‰π¶Â∫ì")
    st.markdown("ËøôÈáåÂ≠òÊîæ‰∫ÜÁ´ôÈïøÁ≤æÈÄâÁöÑÁîüËØçÊú¨ÔºåÂ§ßÂÆ∂ÂèØ‰ª•ÂÖçË¥π‰∏ãËΩΩ„ÄÇ")
    
    LIBRARY_DIR = "library"
    INFO_FILE = "info.json" # ÊèèËø∞Êñá‰ª∂ÁöÑÂêçÂ≠ó
    
    if not os.path.exists(LIBRARY_DIR):
        os.makedirs(LIBRARY_DIR)
        st.info(f"ËØ∑Âú® GitHub ÂàõÂª∫ '{LIBRARY_DIR}' Êñá‰ª∂Â§π„ÄÇ")
    
    # 1. Â∞ùËØïËØªÂèñ info.json ÈáåÁöÑÊèèËø∞‰ø°ÊÅØ
    book_info = {}
    info_path = os.path.join(LIBRARY_DIR, INFO_FILE)
    if os.path.exists(info_path):
        try:
            with open(info_path, "r", encoding="utf-8") as f:
                book_info = json.load(f)
        except Exception as e:
            st.error(f"ÊèèËø∞Êñá‰ª∂ËØªÂèñÂ§±Ë¥• (JsonÊ†ºÂºèÈîôËØØ): {e}")

    files = [f for f in os.listdir(LIBRARY_DIR) if f.endswith(".txt")]
    
    if not files:
        st.warning("üì≠ ‰π¶Êû∂ÁõÆÂâçÊòØÁ©∫ÁöÑÔºåËØ∑‰∏ä‰º† .txt Êñá‰ª∂Âà∞ GitHub ÁöÑ library Êñá‰ª∂Â§πÔºÅ")
    else:
        col1, col2 = st.columns(2)
        for i, filename in enumerate(files):
            file_path = os.path.join(LIBRARY_DIR, filename)
            
            with open(file_path, "r", encoding="utf-8") as f:
                file_content = f.read()
            word_count = len(file_content.splitlines())
            
            # Ëé∑ÂèñËØ•Êñá‰ª∂ÁöÑÊèèËø∞‰ø°ÊÅØ (Â¶ÇÊûúÊ≤°ÂÜôÔºåÂ∞±Áî®ÈªòËÆ§ÂÄº)
            meta = book_info.get(filename, {})
            display_title = meta.get("title", filename) # Â¶ÇÊûúÊúâÊ†áÈ¢òÂ∞±Áî®Ê†áÈ¢òÔºåÊ≤°ÊúâÂ∞±Áî®Êñá‰ª∂Âêç
            display_desc = meta.get("desc", "ÊöÇÊó†ÊèèËø∞")   # Ëé∑ÂèñÊèèËø∞
            
            with (col1 if i % 2 == 0 else col2):
                with st.container(border=True):
                    # ÊòæÁ§∫Â∏¶ emoji ÁöÑÊ†áÈ¢ò
                    st.subheader(f"üìÑ {display_title}")
                    
                    # ÊòæÁ§∫ÊèèËø∞‰ø°ÊÅØ (ÁÅ∞Ëâ≤Â∞èÂ≠ó)
                    if display_desc != "ÊöÇÊó†ÊèèËø∞":
                        st.info(display_desc)
                    else:
                        st.caption("Êó†ËØ¶ÁªÜÊèèËø∞")
                        
                    st.caption(f"üìö ÂçïËØçÊï∞: **{word_count}**")
                    
                    st.download_button(
                        label=f"üì• ‰∏ãËΩΩ {filename}",
                        data=file_content,
                        file_name=filename,
                        mime="text/plain"
                    )

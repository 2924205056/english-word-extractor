import streamlit as st
import io
import re
import zipfile
import math
import chardet
import os
import json
import random
import time
from github import Github

# NLP Imports
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet, stopwords
from nltk import pos_tag

# DOCX Import
from docx import Document

# Optional Spacy
try:
    import spacy
    _HAS_SPACY = True
except ImportError:
    _HAS_SPACY = False

# ------------------ é¡µé¢é…ç½® & æ ·å¼ä¼˜åŒ– ------------------
st.set_page_config(
    page_title="VocabMaster | æ™ºèƒ½è¯ä¹¦å·¥åŠ", 
    page_icon="âš¡", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# æ³¨å…¥ CSSï¼šå¢åŠ å‘¼å¸æ„Ÿï¼Œä¼˜åŒ–é˜…è¯»ä½“éªŒ
st.markdown("""
<style>
    /* å…¨å±€èƒŒæ™¯è‰²å¾®è°ƒ */
    .stApp { background-color: #fcfdfe; }
    
    /* æ ‡é¢˜å¢å¼º */
    h1, h2, h3 { font-family: 'Segoe UI', sans-serif; color: #2c3e50; }
    
    /* æ­¥éª¤æ ‡é¢˜æ ·å¼ */
    .step-header {
        font-size: 1.1rem;
        font-weight: 700;
        color: #4f46e5;
        margin-bottom: 10px;
        display: flex;
        align-items: center;
    }
    
    /* å¡ç‰‡å®¹å™¨ä¼˜åŒ– */
    [data-testid="stExpander"], [data-testid="stForm"] {
        background: white;
        border-radius: 12px;
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05);
        border: 1px solid #e5e7eb;
    }
    
    /* æŒ‰é’®ä¼˜åŒ– */
    div.stButton > button {
        border-radius: 8px;
        padding: 0.5rem 1rem;
        font-weight: 600;
        transition: all 0.2s;
    }
    div.stButton > button:hover { transform: translateY(-1px); }
    
    /* æç¤ºæ¡†æ ·å¼ */
    .info-box {
        background-color: #eff6ff;
        border-left: 4px solid #3b82f6;
        padding: 10px 15px;
        border-radius: 4px;
        color: #1e3a8a;
        font-size: 0.9em;
        margin-bottom: 15px;
    }
</style>
""", unsafe_allow_html=True)

# ------------------ èµ„æºåŠ è½½ (æ ¸å¿ƒåŠŸèƒ½ä¸å˜) ------------------
@st.cache_resource
def download_nltk_resources():
    resources = ["punkt", "averaged_perceptron_tagger", "averaged_perceptron_tagger_eng", "wordnet", "omw-1.4", "stopwords"]
    for r in resources:
        try:
            nltk.data.find(f'tokenizers/{r}')
        except LookupError:
            nltk.download(r, quiet=True)
        except ValueError:
            nltk.download(r, quiet=True)

@st.cache_resource
def load_spacy_model():
    if _HAS_SPACY:
        try:
            return spacy.load("en_core_web_sm", disable=["ner", "parser"])
        except Exception:
            return None
    return None

download_nltk_resources()
nlp_spacy = load_spacy_model()

# ------------------ æ ¸å¿ƒé€»è¾‘å‡½æ•° ------------------
def save_to_github_library(filename, content, title, desc):
    """GitHub ä¸Šä¼ é€»è¾‘"""
    try:
        if "GITHUB_TOKEN" not in st.secrets:
            st.error("ğŸ”’ ç³»ç»Ÿæœªé…ç½® GitHub Tokenï¼Œæ— æ³•è¿æ¥äº‘ç«¯ã€‚è¯·è”ç³»ç®¡ç†å‘˜ã€‚")
            return

        token = st.secrets["GITHUB_TOKEN"]
        username = st.secrets["GITHUB_USERNAME"]
        repo_name = st.secrets["GITHUB_REPO"]
        
        g = Github(token)
        repo = g.get_repo(f"{username}/{repo_name}")
        
        library_path = f"library/{filename}"
        info_path = "library/info.json"
        
        try:
            contents = repo.get_contents(library_path)
            repo.update_file(library_path, f"Update {filename}", content, contents.sha)
        except:
            repo.create_file(library_path, f"Create {filename}", content)

        try:
            info_contents = repo.get_contents(info_path)
            info_data = json.loads(info_contents.decoded_content.decode("utf-8"))
        except:
            info_data = {}
            info_contents = None

        info_data[filename] = {
            "title": title,
            "desc": desc,
            "date": time.strftime("%Y-%m-%d"),
            "author": "User" 
        }
        
        new_info_str = json.dumps(info_data, indent=2, ensure_ascii=False)
        if info_contents:
            repo.update_file(info_path, "Update info.json", new_info_str, info_contents.sha)
        else:
            repo.create_file(info_path, "Create info.json", new_info_str)
            
        st.toast("âœ… å‘å¸ƒæˆåŠŸï¼", icon="ğŸ‰")
        time.sleep(1.5)
        st.rerun()
        
    except Exception as e:
        st.error(f"è¿æ¥äº‘ç«¯å¤±è´¥: {e}")

def extract_text_from_bytes(file_obj, filename):
    if '.' in filename: ext = filename.split('.')[-1].lower()
    else: ext = 'txt'
    text = ""
    try:
        if ext == 'docx':
            doc = Document(file_obj)
            text_content = []
            for p in doc.paragraphs:
                if p.text.strip(): text_content.append(p.text)
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if cell.text.strip(): text_content.append(cell.text)
            text = "\n".join(text_content)
        elif ext == 'doc':
            return ""
        else:
            raw = file_obj.read()
            enc = chardet.detect(raw).get('encoding') or 'utf-8'
            text = raw.decode(enc, errors='ignore')
    except Exception: return ""
    
    if ext in ['srt', 'vtt', 'ass']:
        clean_text = re.sub(r"<.*?>", "", text)
        clean_text = re.sub(r"\d{2}:\d{2}:\d{2},\d{3} --> \d{2}:\d{2}:\d{2},\d{3}", "", clean_text)
        return clean_text
    return text

def process_words(all_text, mode, min_len, filter_set=None):
    TOKEN_RE = re.compile(r"[A-Za-z-]+")
    cleaned = [re.sub(r'[^a-z]', '', w.lower()) for w in TOKEN_RE.findall(all_text) if w]
    lemmatized = []
    
    if mode == "spacy" and nlp_spacy:
        chunk_size = 50000
        chunks = [cleaned[i:i + chunk_size] for i in range(0, len(cleaned), chunk_size)]
        for chunk in chunks:
            doc = nlp_spacy(" ".join(chunk))
            for token in doc:
                lw = token.lemma_.lower()
                if lw.isalpha() and wordnet.synsets(lw): lemmatized.append(lw)
    else:
        lemmatizer = WordNetLemmatizer()
        for w, tag in pos_tag(cleaned):
            wn = {'J':wordnet.ADJ,'V':wordnet.VERB,'N':wordnet.NOUN,'R':wordnet.ADV}.get(tag[0], None)
            lw = lemmatizer.lemmatize(w, wn) if wn else lemmatizer.lemmatize(w)
            if wordnet.synsets(lw): lemmatized.append(lw)

    seen = set()
    final_words = []
    sys_stopwords = set(stopwords.words('english'))
    for w in lemmatized:
        if len(w) < min_len: continue
        if w in sys_stopwords: continue
        if filter_set and w in filter_set: continue
        if w not in seen:
            seen.add(w)
            final_words.append(w)
    return final_words

# ------------------ UI å¸ƒå±€è®¾è®¡ ------------------

# ä¾§è¾¹æ 
with st.sidebar:
    st.image("https://img.icons8.com/fluency/96/dictionary.png", width=50)
    st.markdown("### VocabMaster")
    st.caption("v2.0 Enhanced Edition")
    st.markdown("---")
    
    menu = st.radio(
        "é€‰æ‹©åŠŸèƒ½", 
        ["âš¡ åˆ¶ä½œç”Ÿè¯æœ¬", "ğŸŒ å…¬å…±è¯ä¹¦åº“"],
        captions=["ä»æ–‡ä»¶æå–å•è¯", "ä¸‹è½½ç°æˆçš„è¯ä¹¦"]
    )
    
    st.markdown("---")
    st.info("**å°è´´å£«**\nä½¿ç”¨ Spacy å¼•æ“å¯ä»¥è·å¾—æ›´å‡†ç¡®çš„è¯å½¢è¿˜åŸï¼ˆä¾‹å¦‚å°† 'running' è¿˜åŸä¸º 'run'ï¼‰ã€‚")

# === åŠŸèƒ½ä¸€: åˆ¶ä½œç”Ÿè¯æœ¬ ===
if menu == "âš¡ åˆ¶ä½œç”Ÿè¯æœ¬":
    st.title("âš¡ æ™ºèƒ½ç”Ÿè¯æå–å·¥åŠ")
    
    # --- æŒ‡å¼•åŒºåŸŸ (å¯æŠ˜å ï¼Œä¿æŒé¡µé¢æ•´æ´) ---
    with st.expander("ğŸ“– æ–°æ‰‹æŒ‡å—ï¼šå¦‚ä½•åˆ¶ä½œä¸€æœ¬ç”Ÿè¯æœ¬ï¼Ÿ(ç‚¹å‡»å±•å¼€)", expanded=False):
        st.markdown("""
        1.  **å‡†å¤‡æ–‡ä»¶**ï¼šæ‰¾åˆ°ä½ æƒ³å­¦ä¹ çš„å­—å¹•æ–‡ä»¶ (`.srt`) æˆ–æ–‡ç«  (`.docx`, `.txt`)ã€‚
        2.  **è®¾ç½®è§„åˆ™**ï¼šåœ¨å·¦ä¾§è®¾ç½®è¿‡æ»¤æ¡ä»¶ï¼Œæ¯”å¦‚è¿‡æ»¤æ‰å¤ªçŸ­çš„å•è¯ï¼Œæˆ–ä¸Šä¼ â€œç†Ÿè¯è¡¨â€è¿‡æ»¤æ‰ä½ å·²ç»è®¤è¯†çš„è¯ã€‚
        3.  **ä¸Šä¼ åˆ†æ**ï¼šæ‹–å…¥æ–‡ä»¶ï¼Œç‚¹å‡»å¼€å§‹ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æå–é«˜é¢‘ç”Ÿè¯ã€‚
        4.  **å¯¼å‡ºåˆ†äº«**ï¼šå°†ç»“æœä¸‹è½½ä¸º ZIPï¼Œæˆ–å‘å¸ƒåˆ°å…¬å…±åº“åˆ†äº«ç»™ä»–äººã€‚
        """)

    # çŠ¶æ€ç®¡ç†
    if 'result_words' not in st.session_state: st.session_state.result_words = []
    
    # --- ä¸»æ“ä½œåŒºï¼šå·¦å³åˆ†æ  ---
    c_config, c_upload = st.columns([1, 2], gap="large")
    
    # å·¦æ ï¼šé…ç½® (Step 1)
    with c_config:
        st.markdown('<div class="step-header">1ï¸âƒ£ è®¾ç½®æå–è§„åˆ™</div>', unsafe_allow_html=True)
        with st.container(border=True):
            st.markdown("**åŸºç¡€è®¾ç½®**")
            nlp_mode = st.selectbox(
                "AI å¤„ç†å¼•æ“", 
                ["nltk (å¿«é€Ÿ)", "spacy (ç²¾å‡†)"],
                help="NLTK é€Ÿåº¦æå¿«é€‚åˆå¤§æ–‡ä»¶ï¼›Spacy è¯­æ³•åˆ†ææ›´å‡†ï¼Œé€‚åˆç²¾å‡†å­¦ä¹ ã€‚"
            )
            mode_key = "spacy" if "spacy" in nlp_mode else "nltk"
            
            min_len = st.number_input(
                "æœ€çŸ­å•è¯é•¿åº¦", 
                value=3, min_value=1,
                help="è‡ªåŠ¨è¿‡æ»¤æ‰é•¿åº¦å°äºæ­¤å€¼çš„å•è¯ï¼ˆå¦‚ a, is, to ç­‰ï¼‰ã€‚"
            )
            
            st.divider()
            
            st.markdown("**ç†Ÿè¯è¿‡æ»¤ (å¯é€‰)**")
            filter_file = st.file_uploader(
                "ä¸Šä¼ ç†Ÿè¯è¡¨ (.txt)", 
                type=['txt'],
                help="ä¸Šä¼ ä¸€ä¸ªåŒ…å«ä½ å·²è®¤è¯†å•è¯çš„txtæ–‡ä»¶ï¼ˆä¸€è¡Œä¸€ä¸ªï¼‰ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨è·³è¿‡è¿™äº›è¯ã€‚"
            )
            filter_set = set()
            if filter_file:
                c = filter_file.getvalue().decode("utf-8", errors='ignore')
                filter_set = set(l.strip().lower() for l in c.splitlines() if l.strip())
                st.caption(f"âœ… å·²åŠ è½½ {len(filter_set)} ä¸ªç†Ÿè¯")

    # å³æ ï¼šä¸Šä¼  (Step 2)
    with c_upload:
        st.markdown('<div class="step-header">2ï¸âƒ£ ä¸Šä¼ æ–‡ä»¶å¹¶åˆ†æ</div>', unsafe_allow_html=True)
        with st.container(border=True):
            # å¼•å¯¼æ–‡æ¡ˆ
            st.markdown("""
            <div class="info-box">
                æ”¯æŒæ‰¹é‡ä¸Šä¼ å­—å¹• (.srt, .ass) æˆ–æ–‡æ¡£ (.docx, .txt)ã€‚<br>
                ç³»ç»Ÿä¼šè‡ªåŠ¨å»é™¤æ—¶é—´è½´å’Œæ ¼å¼æ ‡ç­¾ã€‚
            </div>
            """, unsafe_allow_html=True)
            
            uploaded_files = st.file_uploader(
                "æ‹–æ‹½æ–‡ä»¶åˆ°è¿™é‡Œï¼Œæˆ–ç‚¹å‡»æµè§ˆ", 
                type=['txt','srt','ass','vtt','docx'], 
                accept_multiple_files=True
            )
            
            # æ“ä½œæŒ‰é’®ä¸ç©ºéš™

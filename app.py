import streamlit as st
import io
import re
import zipfile
import math
import chardet

# NLP Imports
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet, stopwords
from nltk import pos_tag

# DOCX Import
from docx import Document

# Optional Spacy
try:
    import spacy
    _HAS_SPACY = True
except ImportError:
    _HAS_SPACY = False

# ------------------ é¡µé¢é…ç½® ------------------
st.set_page_config(page_title="å•è¯æå–å™¨ Webç‰ˆ", page_icon="ğŸ“˜", layout="wide")

# ------------------ ç¼“å­˜èµ„æºåŠ è½½ (æ ¸å¿ƒä¿®å¤éƒ¨åˆ†) ------------------
@st.cache_resource
def download_nltk_resources():
    """é™é»˜ä¸‹è½½ NLTK èµ„æº"""
    # è¿™é‡Œçš„åˆ—è¡¨åŠ ä¸Šäº† 'averaged_perceptron_tagger_eng' ä»¥ä¿®å¤äº‘ç«¯æŠ¥é”™
    resources = [
        "punkt", 
        "averaged_perceptron_tagger", 
        "averaged_perceptron_tagger_eng", 
        "wordnet", 
        "omw-1.4", 
        "stopwords"
    ]
    for r in resources:
        try:
            nltk.data.find(f'tokenizers/{r}')
        except LookupError:
            nltk.download(r, quiet=True)
        except ValueError:
            nltk.download(r, quiet=True)

@st.cache_resource
def load_spacy_model():
    if _HAS_SPACY:
        try:
            # å°è¯•åŠ è½½å°æ¨¡å‹
            return spacy.load("en_core_web_sm", disable=["ner", "parser"])
        except Exception:
            return None
    return None

# åˆå§‹åŒ–èµ„æº
download_nltk_resources()
nlp_spacy = load_spacy_model()

# ------------------ æ ¸å¿ƒå·¥å…·å‡½æ•° ------------------

def extract_text_from_bytes(file_obj, filename):
    """ä»å†…å­˜æ–‡ä»¶å¯¹è±¡ä¸­æå–æ–‡æœ¬"""
    ext = filename.split('.')[-1].lower()
    text = ""
    
    try:
        if ext == 'docx':
            doc = Document(file_obj)
            paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]
            text = "\n".join(paragraphs)
        else:
            # äºŒè¿›åˆ¶è¯»å–å¹¶æ£€æµ‹ç¼–ç 
            raw = file_obj.read()
            enc = chardet.detect(raw).get('encoding') or 'utf-8'
            text = raw.decode(enc, errors='ignore')
    except Exception as e:
        st.warning(f"âš ï¸ è¯»å– {filename} å¤±è´¥: {e}")
        return ""

    # é’ˆå¯¹å­—å¹•æ ¼å¼çš„æ¸…æ´—
    if ext == 'srt':
        return extract_english_from_srt(text)
    elif ext ==

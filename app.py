import streamlit as st
import io
import re
import zipfile
import math
import chardet
import os
import json
import random
import time
import pandas as pd
import streamlit.components.v1 as components
from github import Github

# --- ã€ä¿®æ­£ã€‘ç¡®ä¿å¼•å…¥ copy åº“ ---
import copy

import streamlit_authenticator as stauth

# NLP Imports
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet, stopwords
from nltk import pos_tag

# DOCX Import
from docx import Document

# Optional Spacy
try:
    import spacy
    _HAS_SPACY = True
except ImportError:
    _HAS_SPACY = False

# ------------------ 0. åˆå§‹åŒ–ï¼šå‡†å¤‡é¢„ç½®è¯åº“æ•°æ® ------------------
WORDLIST_DIR = "wordlists"
if not os.path.exists(WORDLIST_DIR):
    os.makedirs(WORDLIST_DIR)
    # åˆ›å»ºæ¼”ç¤ºç”¨æ–‡ä»¶ (è¯·ç”¨çœŸå®æ–‡ä»¶æ›¿æ¢)
    with open(os.path.join(WORDLIST_DIR, "primary.txt"), "w", encoding="utf-8") as f:
        f.write("a\nan\nthe\nis\nare\nam\nhello\ngood\nmorning\napple\nbanana\ncat\ndog\nbook\npen")
    with open(os.path.join(WORDLIST_DIR, "zhongkao.txt"), "w", encoding="utf-8") as f:
        f.write("ability\nabsent\naccept\naccording\nachieve\nactive\nactually\nadd\naddress\nadmit")
    with open(os.path.join(WORDLIST_DIR, "gaokao.txt"), "w", encoding="utf-8") as f:
        f.write("abandon\nability\nabnormal\naboard\nabolish\nabortion\nabrupt\nabsence\nabsolute\nabsorb")
    print(f"å·²åœ¨ {WORDLIST_DIR} ç›®å½•ä¸‹åˆ›å»ºæ¼”ç¤ºè¯è¡¨æ–‡ä»¶ã€‚è¯·æ›¿æ¢ä¸ºçœŸå®æ•°æ®ã€‚")

PRESET_WORDLISTS = {
    "ğŸ‘¶ å°å­¦æ ¸å¿ƒè¯ ": os.path.join(WORDLIST_DIR, "primary.txt"),
    "ğŸ‘¦ ä¸­è€ƒå¿…å¤‡è¯ ": os.path.join(WORDLIST_DIR, "zhongkao.txt"),
    "ğŸ‘¨â€ğŸ“ é«˜è€ƒ3500è¯ ": os.path.join(WORDLIST_DIR, "gaokao.txt"),
}

# ------------------ 1. é¡µé¢é…ç½® ------------------
st.set_page_config(
    page_title="VocabMaster Pro | æ™ºèƒ½è¯ä¹¦å·¥åŠ",
    page_icon="âš¡",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==============================================================================
# === ã€èº«ä»½éªŒè¯é€»è¾‘å¼€å§‹ã€‘ ===
# ==============================================================================

# 1. ä» Streamlit Secrets åŠ è½½è®¤è¯é…ç½®å¹¶åˆ›å»ºå‰¯æœ¬
try:
    # æ£€æŸ¥ keys æ˜¯å¦å­˜åœ¨
    if "auth" not in st.secrets:
         st.error("Secrets ä¸­æœªæ‰¾åˆ° 'auth' å­—æ®µé…ç½®ã€‚è¯·æ£€æŸ¥ Streamlit Cloud è®¾ç½®ã€‚")
         st.stop()
    
    # --- ã€æ ¸å¿ƒä¿®æ­£ç‚¹ã€‘ ---
    # ä½¿ç”¨ copy.deepcopy() åˆ›å»º Secrets çš„æ·±æ‹·è´ã€‚
    # è¿™æ · authenticator å°±å¯ä»¥ä¿®æ”¹ config è¿™ä¸ªå‰¯æœ¬ï¼Œè€Œä¸ä¼šè§¦å‘ Secrets çš„åªè¯»é”™è¯¯ã€‚
    config = copy.deepcopy(st.secrets["auth"])

except Exception as e:
    st.error(f"åŠ è½½è®¤è¯é…ç½®å¤±è´¥: {e}")
    st.stop()

# 2. åˆå§‹åŒ–è®¤è¯å¯¹è±¡
# æ­¤æ—¶ä¼ å…¥çš„ config æ˜¯ä¸€ä¸ªå¯ä¿®æ”¹çš„å‰¯æœ¬
authenticator = stauth.Authenticate(
    config['credentials'],
    config['cookie']['name'],
    config['cookie']['key'],
    config['cookie']['expiry_days']
)

# 3. åœ¨ä¾§è¾¹æ åˆ›å»ºç™»å½•æ¡†
name, authentication_status, username = authenticator.login('ç™»å½• VocabMaster', 'sidebar')

# 4. æ ¹æ®ç™»å½•çŠ¶æ€å†³å®šæ˜¾ç¤ºä»€ä¹ˆ
if authentication_status is False:
    st.sidebar.error('ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯')
    st.stop()
elif authentication_status is None:
    st.sidebar.warning('è¯·åœ¨ä¸Šæ–¹è¾“å…¥è´¦å·å¯†ç ç™»å½•ã€‚')
    st.title("ğŸ”’ æ¬¢è¿æ¥åˆ° VocabMaster Pro")
    st.markdown("### æ‚¨çš„æ™ºèƒ½ç”Ÿè¯æœ¬ä¸“å®¶")
    st.info("ä¸ºäº†ä¿æŠ¤æ•°æ®å®‰å…¨å¹¶æä¾›ä¸ªæ€§åŒ–æœåŠ¡ï¼Œè¯·å…ˆåœ¨å·¦ä¾§ä¾§è¾¹æ ç™»å½•ã€‚")
    st.stop()

# ==============================================================================
# === ã€èº«ä»½éªŒè¯é€»è¾‘ç»“æŸã€‘ ===
# === åªæœ‰å½“ authentication_status ä¸º True æ—¶ï¼Œä»£ç æ‰ä¼šç»§ç»­å¾€ä¸‹èµ° ===
# ==============================================================================

# ç™»å½•æˆåŠŸï¼
elif authentication_status:
    with st.sidebar:
        st.write(f'ğŸ‘‹ æ¬¢è¿å›æ¥, **{name}**!')
        authenticator.logout('é€€å‡ºç™»å½•', 'sidebar')
        st.divider()

    # ------------------------------------------------------------------------------
    # åŸæœ‰æ ¸å¿ƒä¸šåŠ¡ä»£ç  (å·²ç¼©è¿›)
    # ------------------------------------------------------------------------------

    # æ³¨å…¥ CSS
    st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');
        .stApp { background-color: #f8fafc; font-family: 'Inter', sans-serif; }
        h1, h2, h3, h4, h5 { color: #1e293b; font-family: 'Inter', sans-serif; font-weight: 700; letter-spacing: -0.025em; }
        p, div, span { color: #475569; line-height: 1.6; }
        section[data-testid="stSidebar"] { background-color: #ffffff; border-right: 1px solid #f1f5f9; box-shadow: 4px 0 24px rgba(0,0,0,0.02); }
        [data-testid="stExpander"], [data-testid="stForm"], [data-testid="stContainer"] { background: #ffffff; border: 1px solid #e2e8f0; border-radius: 16px; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05), 0 2px 4px -1px rgba(0, 0, 0, 0.03); transition: all 0.3s ease; }
        .streamlit-expanderHeader { background-color: transparent; color: #334155; font-weight: 600; }
        div.stButton > button { background: linear-gradient(135deg, #6366f1 0%, #4f46e5 100%); color: white; border: none; border-radius: 10px; padding: 0.6rem 1.2rem; font-weight: 600; font-size: 0.95rem; box-shadow: 0 4px 12px rgba(79, 70, 229, 0.3); transition: transform 0.2s ease, box-shadow 0.2s ease; }
        div.stButton > button:hover { transform: translateY(-2px); box-shadow: 0 8px 16px rgba(79, 70, 229, 0.4); color: white; }
        div.stButton > button[kind="secondary"] { background: white; color: #4f46e5; border: 1px solid #e2e8f0; box-shadow: 0 2px 5px rgba(0,0,0,0.05); }
        a[kind="primary"] { background: linear-gradient(135deg, #10b981 0%, #059669 100%) !important; border: none !important; box-shadow: 0 4px 12px rgba(16, 185, 129, 0.3) !important; border-radius: 10px !important; color: white !important; font-weight: 600 !important; text-decoration: none !important; display: flex; justify-content: center; align-items: center; padding: 0.6rem 1.2rem; transition: transform 0.2s ease !important; }
        a[kind="primary"]:hover { transform: translateY(-2px); }
        .stTextInput > div > div, .stSelectbox > div > div, .stNumberInput > div > div, .stMultiSelect > div > div { background-color: #f8fafc; border: 1px solid #e2e8f0; border-radius: 10px; color: #334155; }
        .stTextInput > div > div:focus-within, .stMultiSelect > div > div:focus-within { border-color: #6366f1; box-shadow: 0 0 0 3px rgba(99, 102, 241, 0.1); }
        .stMultiSelect [data-baseweb="tag"] { background-color: #e0e7ff; border: 1px solid #c7d2fe; color: #4f46e5; }
        .step-header { font-size: 1.25rem; font-weight: 700; color: #334155; margin-bottom: 16px; display: flex; align-items: center; padding-bottom: 8px; border-bottom: 2px solid #f1f5f9; }
        .info-box { background: rgba(239, 246, 255, 0.7); backdrop-filter: blur(10px); border: 1px solid #dbeafe; border-left: 4px solid #3b82f6; padding: 16px; border-radius: 8px; color: #1e40af; font-size: 0.95em; margin-bottom: 20px; }
        .stCodeBlock { max-height: 220px !important; overflow-y: auto !important; border: 1px solid #cbd5e1; border-radius: 8px; background-color: #f1f5f9; font-family: 'JetBrains Mono', monospace; }
        [data-testid="stMetric"] { background-color: white; padding: 15px; border-radius: 12px; border: 1px solid #f1f5f9; box-shadow: 0 2px 4px rgba(0,0,0,0.02); text-align: center; }
        [data-testid="stMetricLabel"] { color: #64748b; font-size: 0.9rem; }
        [data-testid="stMetricValue"] { color: #4f46e5; font-size: 1.8rem; font-weight: 700; }
        .stTabs [data-baseweb="tab-list"] { gap: 8px; background-color: transparent; }
        .stTabs [data-baseweb="tab"] { background-color: white; border-radius: 8px 8px 0 0; border: 1px solid #e2e8f0; border-bottom: none; padding: 10px 20px; color: #64748b; }
        .stTabs [aria-selected="true"] { background-color: #f8fafc; color: #4f46e5; font-weight: 600; border-top: 2px solid #4f46e5; }
    </style>
    """, unsafe_allow_html=True)

    # è‡ªå®šä¹‰å¤åˆ¶æŒ‰é’®ç»„ä»¶
    def render_copy_button(text_content, unique_key):
        safe_text = json.dumps(text_content)
        html_code = f"""
        <!DOCTYPE html>
        <html>
        <head>
        <style>
            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@600&display=swap');
            body {{ margin: 0; padding: 0; }}
            .copy-btn {{ width: 100%; padding: 14px; background: linear-gradient(135deg, #6366f1 0%, #4f46e5 100%); color: white; border: none; border-radius: 10px; font-family: 'Inter', sans-serif; font-weight: 600; font-size: 15px; cursor: pointer; transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1); display: flex; align-items: center; justify-content: center; box-shadow: 0 4px 12px rgba(79, 70, 229, 0.3); letter-spacing: 0.02em; }}
            .copy-btn:hover {{ transform: translateY(-2px); box-shadow: 0 8px 20px rgba(79, 70, 229, 0.4); filter: brightness(1.05); }}
            .copy-btn:active {{ transform: translateY(0); }}
            .icon {{ margin-right: 8px; font-size: 18px; }}
        </style>
        </head>
        <body>
            <button id="btn_{unique_key}" class="copy-btn" onclick="copyText()">
                <span class="icon">ğŸ“‹</span> ä¸€é”®å¤åˆ¶ (Copy All)
            </button>
            <script>
            function copyText() {{
                const text = {safe_text};
                const textArea = document.createElement("textarea");
                textArea.value = text;
                document.body.appendChild(textArea);
                textArea.select();
                try {{
                    document.execCommand('copy');
                    const btn = document.getElementById("btn_{unique_key}");
                    const originalHTML = btn.innerHTML;
                    btn.innerHTML = '<span class="icon">âœ…</span> æˆåŠŸï¼(Copied)';
                    btn.style.background = "linear-gradient(135deg, #10b981 0%, #059669 100%)";
                    btn.style.boxShadow = "0 4px 12px rgba(16, 185, 129, 0.3)";
                    setTimeout(() => {{
                        btn.innerHTML = originalHTML;
                        btn.style.background = "linear-gradient(135deg, #6366f1 0%, #4f46e5 100%)";
                        btn.style.boxShadow = "0 4px 12px rgba(79, 70, 229, 0.3)";
                    }}, 2000);
                }} catch (err) {{}}
                document.body.removeChild(textArea);
            }}
            </script>
        </body>
        </html>
        """
        components.html(html_code, height=55)

    # èµ„æºåŠ è½½
    @st.cache_resource
    def download_nltk_resources():
        resources = ["punkt", "averaged_perceptron_tagger", "averaged_perceptron_tagger_eng", "wordnet", "omw-1.4", "stopwords"]
        for r in resources:
            try: nltk.data.find(f'tokenizers/{r}')
            except LookupError: nltk.download(r, quiet=True)
            except ValueError: nltk.download(r, quiet=True)

    @st.cache_resource
    def load_spacy_model():
        if _HAS_SPACY:
            try: return spacy.load("en_core_web_sm", disable=["ner", "parser"])
            except Exception: return None
        return None

    download_nltk_resources()
    nlp_spacy = load_spacy_model()

    # æ ¸å¿ƒé€»è¾‘å‡½æ•°
    def save_to_github_library(filename, content, title, desc):
        try:
            if "GITHUB_TOKEN" not in st.secrets or "GITHUB_USERNAME" not in st.secrets or "GITHUB_REPO" not in st.secrets:
                st.error("ğŸ”’ ç³»ç»Ÿæœªé…ç½® GitHub Secretsï¼Œæ— æ³•å‘å¸ƒã€‚")
                return

            token = st.secrets["GITHUB_TOKEN"]
            username = st.secrets["GITHUB_USERNAME"]
            repo_name = st.secrets["GITHUB_REPO"]
            
            g = Github(token)
            repo = g.get_repo(f"{username}/{repo_name}")
            
            library_path = f"library/{filename}"
            info_path = "library/info.json"
            
            try:
                contents = repo.get_contents(library_path)
                repo.update_file(library_path, f"Update {filename}", content, contents.sha)
            except:
                repo.create_file(library_path, f"Create {filename}", content)

            try:
                info_contents = repo.get_contents(info_path)
                info_data = json.loads(info_contents.decoded_content.decode("utf-8"))
            except:
                info_data = {}
                info_contents = None

            info_data[filename] = {
                "title": title,
                "desc": desc,
                "date": time.strftime("%Y-%m-%d"),
                # ä½¿ç”¨ç™»å½•ç”¨æˆ·çš„åå­—ä½œä¸ºä½œè€…
                "author": name 
            }
            
            new_info_str = json.dumps(info_data, indent=2, ensure_ascii=False)
            if info_contents:
                repo.update_file(info_path, "Update info.json", new_info_str, info_contents.sha)
            else:
                repo.create_file(info_path, "Create info.json", new_info_str)
                
            local_lib = "library"
            if not os.path.exists(local_lib): os.makedirs(local_lib)
            with open(os.path.join(local_lib, filename), "w", encoding="utf-8") as f:
                f.write(content)
            
            local_info_path = os.path.join(local_lib, "info.json")
            local_info = {}
            if os.path.exists(local_info_path):
                with open(local_info_path, "r", encoding="utf-8") as f:
                    try: local_info = json.load(f)
                    except: pass
            local_info[filename] = info_data[filename]
            with open(local_info_path, "w", encoding="utf-8") as f:
                json.dump(local_info, f, indent=2, ensure_ascii=False)

            st.toast("âœ… å‘å¸ƒæˆåŠŸï¼", icon="ğŸ‰")
            time.sleep(1.5)
            st.rerun()
            
        except Exception as e:
            st.error(f"ä¸Šä¼ å¤±è´¥: {e}")

    def extract_text_from_bytes(file_obj, filename):
        if '.' in filename: ext = filename.split('.')[-1].lower()
        else: ext = 'txt'
        text = ""
        try:
            if ext == 'docx':
                doc = Document(file_obj)
                text_content = []
                for p in doc.paragraphs:
                    if p.text.strip(): text_content.append(p.text)
                for table in doc.tables:
                    for row in table.rows:
                        for cell in row.cells:
                            if cell.text.strip(): text_content.append(cell.text)
                text = "\n".join(text_content)
            elif ext == 'doc':
                return ""
            else:
                raw = file_obj.read()
                enc = chardet.detect(raw).get('encoding') or 'utf-8'
                text = raw.decode(enc, errors='ignore')
        except Exception: return ""
        
        if ext in ['srt', 'vtt', 'ass']:
            clean_text = re.sub(r"<.*?>", "", text)
            clean_text = re.sub(r"\d{2}:\d{2}:\d{2},\d{3} --> \d{2}:\d{2}:\d{2},\d{3}", "", clean_text)
            return clean_text
        return text

    def process_words(all_text, mode, min_len, filter_set=None):
        TOKEN_RE = re.compile(r"[A-Za-z-]+")
        cleaned = [re.sub(r'[^a-z]', '', w.lower()) for w in TOKEN_RE.findall(all_text) if w]
        lemmatized = []
        
        if mode == "spacy" and nlp_spacy:
            chunk_size = 50000
            chunks = [cleaned[i:i + chunk_size] for i in range(0, len(cleaned), chunk_size)]
            for chunk in chunks:
                doc = nlp_spacy(" ".join(chunk))
                for token in doc:
                    lw = token.lemma_.lower()
                    if lw.isalpha() and wordnet.synsets(lw): lemmatized.append(lw)
        else:
            lemmatizer = WordNetLemmatizer()
            for w, tag in pos_tag(cleaned):
                wn = {'J':wordnet.ADJ,'V':wordnet.VERB,'N':wordnet.NOUN,'R':wordnet.ADV}.get(tag[0], None)
                lw = lemmatizer.lemmatize(w, wn) if wn else lemmatizer.lemmatize(w)
                if wordnet.synsets(lw): lemmatized.append(lw)

        seen = set()
        final_words = []
        sys_stopwords = set(stopwords.words('english'))
        for w in lemmatized:
            if len(w) < min_len: continue
            if w in sys_stopwords: continue
            if filter_set and w in filter_set: continue
            if w not in seen:
                seen.add(w)
                final_words.append(w)
        return final_words

    # UI æ¶æ„
    with st.sidebar:
        st.image("https://img.icons8.com/fluency/96/dictionary.png", width=64)
        st.markdown("### VocabMaster")
        st.caption("v12.0 Pro Login Edition")
        st.markdown("---")
        menu = st.radio("é€‰æ‹©åŠŸèƒ½", ["âš¡ åˆ¶ä½œç”Ÿè¯æœ¬", "ğŸŒ å…¬å…±è¯ä¹¦åº“"])
        st.markdown("---")
        st.info("**å°è´´å£«**\nä½¿ç”¨ Spacy å¼•æ“è¿˜åŸè¯å½¢æ›´å‡†ã€‚")

    # === åˆ¶ä½œç”Ÿè¯æœ¬ ===
    if menu == "âš¡ åˆ¶ä½œç”Ÿè¯æœ¬":
        st.title("âš¡ æ™ºèƒ½ç”Ÿè¯æå–å·¥åŠ")
        
        with st.expander("ğŸ“– æ–°æ‰‹æŒ‡å— & å®è—èµ„æºåº“ (ç‚¹å‡»å±•å¼€)", expanded=False):
            t1, t2, t3, t4 = st.tabs(["ğŸ’¡ æ“ä½œæŒ‡å¼•", "ğŸ¬ å½±è§†å­—å¹•", "ğŸ“š åŸè‘—é˜…è¯»", "ğŸ§ å¬åŠ›ç´ æ"])
            with t1:
                st.markdown("""<div style="padding:5px;"><h5 style="margin-top:0">ğŸš€ å››æ­¥åˆ¶ä½œä¸“å±è¯ä¹¦ï¼š</h5><ol><li><b>å‡†å¤‡ç´ æ</b>ï¼šä»å³ä¾§æ ‡ç­¾é¡µä¸‹è½½ <code>.srt</code> å­—å¹•æˆ– <code>.txt</code> ç”µå­ä¹¦ã€‚</li><li><b>æ¸…æ´—è®¾ç½®</b>ï¼šåœ¨ä¸‹æ–¹ã€è®¾ç½®æå–è§„åˆ™ã€‘ä¸­ï¼Œé€‰æ‹©<b>â€œé¢„ç½®ç†Ÿè¯åº“â€</b>æˆ–ä¸Šä¼ è‡ªå®šä¹‰ç†Ÿè¯è¡¨ã€‚</li><li><b>æ™ºèƒ½æå–</b>ï¼šå°†æ–‡ä»¶æ‹–å…¥ä¸Šä¼ åŒºï¼ŒAI è‡ªåŠ¨å®Œæˆå»é‡ã€è¯å½¢è¿˜åŸã€‚</li><li><b>é—­ç¯å­¦ä¹ </b>ï¼šç‚¹å‡»ç”Ÿæˆçš„<b>â€œä¸€é”®å¤åˆ¶â€</b>æŒ‰é’®ï¼Œè·³è½¬æ‰‡è´ç½‘æ‰¹é‡åˆ¶å¡ã€‚</li></ol></div>""", unsafe_allow_html=True)
            # ... (çœç•¥å…¶ä»– tab çš„å†…å®¹ä»¥èŠ‚çœç¯‡å¹…ï¼Œå®ƒä»¬éƒ½åœ¨) ...

        if 'result_words' not in st.session_state: st.session_state.result_words = []
        if 'source_files_count' not in st.session_state: st.session_state.source_files_count = 0
        
        c_config, c_upload = st.columns([1, 2], gap="large")
        
        with c_config:
            st.markdown('<div class="step-header">1ï¸âƒ£ è®¾ç½®æå–è§„åˆ™</div>', unsafe_allow_html=True)
            with st.container(border=True):
                nlp_mode = st.selectbox("AI å¼•æ“", ["nltk (å¿«é€Ÿ)", "spacy (ç²¾å‡†)"])
                mode_key = "spacy" if "spacy" in nlp_mode else "nltk"
                min_len = st.number_input("å•è¯æœ€çŸ­é•¿åº¦", 3, 20, 3)
                st.markdown("---")
                sort_order = st.selectbox("ğŸ”€ å•è¯æ’åº", ["æŒ‰æ–‡æœ¬å‡ºç°é¡ºåº", "A-Z æ’åº", "éšæœºæ‰“ä¹±"])
                chunk_size = st.number_input("ğŸ“¥ æ–‡ä»¶æ‹†åˆ†å¤§å° (è¯/æ–‡ä»¶)", 5000, 50000, 5000, step=1000)
                
                st.markdown("---")
                st.markdown("##### ğŸ›¡ï¸ ç†Ÿè¯å±è”½è®¾ç½®")
                selected_presets = st.multiselect("é€‰æ‹©é¢„ç½®ç†Ÿè¯åº“ (å¯å¤šé€‰, å åŠ ç”Ÿæ•ˆ)", options=list(PRESET_WORDLISTS.keys()), default=[], help="é€‰æ‹©ä½ å·²ç»æŒæ¡çš„è¯æ±‡ç­‰çº§ï¼Œè¿™äº›è¯å°†ä¸ä¼šå‡ºç°åœ¨æœ€ç»ˆç»“æœä¸­ã€‚")
                filter_file = st.file_uploader("ä¸Šä¼ è‡ªå®šä¹‰ç†Ÿè¯è¡¨ (.txt)", type=['txt'], help="å¦‚æœä½ æœ‰è‡ªå·±çš„ä¸“å±è¯è¡¨ï¼Œå¯ä»¥åœ¨è¿™é‡Œä¸Šä¼ ï¼Œå°†ä¸é¢„ç½®è¯åº“å åŠ ã€‚")
                
                filter_set = set()
                for preset_name in selected_presets:
                    file_path = PRESET_WORDLISTS[preset_name]
                    if os.path.exists(file_path):
                        try:
                            with open(file_path, "r", encoding="utf-8") as f:
                                words_in_file = set(l.strip().lower() for l in f if l.strip())
                                filter_set.update(words_in_file)
                        except Exception as e:
                             st.warning(f"è¯»å–è¯åº“ {preset_name} å¤±è´¥: {e}")
                    else:
                         st.warning(f"æ‰¾ä¸åˆ°è¯åº“æ–‡ä»¶: {file_path}")

                if filter_file:
                    c = filter_file.getvalue().decode("utf-8", errors='ignore')
                    custom_words = set(l.strip().lower() for l in c.splitlines() if l.strip())
                    filter_set.update(custom_words)
                    
                if filter_set:
                    st.caption(f"âœ… å·²å¯ç”¨å±è”½ï¼Œå…±è®¡ {len(filter_set)} ä¸ªç†Ÿè¯ã€‚")
                else:
                     st.caption("â„¹ï¸ æœªå¯ç”¨ä»»ä½•ç†Ÿè¯å±è”½ã€‚")

        with c_upload:
            st.markdown('<div class="step-header">2ï¸âƒ£ ä¸Šä¼ ä¸åˆ†æ</div>', unsafe_allow_html=True)
            with st.container(border=True):
                uploaded_files = st.file_uploader("æ”¯æŒ .srt,.ass,.docx, .txt", type=['txt','srt','ass','docx'], accept_multiple_files=True)
                st.markdown("<br>", unsafe_allow_html=True)
                if uploaded_files and st.button("ğŸš€ å¼€å§‹æå–", type="primary", use_container_width=True):
                    my_bar = st.progress(0, text="è¯»å–æ–‡ä»¶...")
                    all_text = []
                    for i, f in enumerate(uploaded_files):
                        all_text.append(extract_text_from_bytes(f, f.name))
                        my_bar.progress((i+1)/len(uploaded_files))
                    
                    full_text = "\n".join(all_text)
                    if full_text.strip():
                        my_bar.progress(100, text="AI åˆ†æä¸­ (å¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´)...")
                        words = process_words(full_text, mode_key, min_len, filter_set)
                        if sort_order == "A-Z æ’åº": words.sort()
                        elif sort_order == "éšæœºæ‰“ä¹±": random.shuffle(words)
                        st.session_state.result_words = words
                        st.session_state.source_files_count = len(uploaded_files)
                        my_bar.empty()
                        st.rerun()
                    else:
                        st.error("æœªæå–åˆ°æ–‡æœ¬ã€‚")

        if st.session_state.result_words:
            st.divider()
            st.markdown('<div class="step-header">3ï¸âƒ£ ç»“æœä¸å¯¼å‡º</div>', unsafe_allow_html=True)
            words = st.session_state.result_words
            content_str = "\n".join(words)
            
            with st.container(border=True):
                c1, c2, c3 = st.columns(3)
                c1.metric("å•è¯æ•°", len(words))
                c2.metric("æ’åº", sort_order)
                c3.metric("æ¥æº", f"{st.session_state.source_files_count} æ–‡ä»¶")
                
            col_copy, col_act = st.columns([2, 1], gap="large")
            with col_copy:
                st.markdown("##### ğŸ“‹ å•è¯åˆ—è¡¨ (ä¸€é”®å¤åˆ¶)")
                render_copy_button(content_str, "res_copy")
                st.code(content_str, language="text")
                
            with col_act:
                st.markdown("##### ğŸš€ æ“ä½œ")
                st.markdown("""<a href="https://web.shanbay.com/wordsweb/#/books" target="_blank" kind="primary">ğŸ¦ å¯¼å…¥æ‰‡è´ (Web)</a>""", unsafe_allow_html=True)
                st.markdown("<div style='height:15px'></div>", unsafe_allow_html=True)
                zip_buffer = io.BytesIO()
                num_files = math.ceil(len(words) / chunk_size)
                with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                    for i in range(num_files):
                        s = i * chunk_size
                        e = min(s + chunk_size, len(words))
                        zf.writestr(f"word_list_{i+1}.txt", "\n".join(words[s:e]))
                st.download_button(f"ğŸ“¦ ä¸‹è½½ ZIP ({num_files}ä¸ª)", zip_buffer.getvalue(), "vocab.zip", "application/zip", use_container_width=True)
                
                st.markdown("---")
                with st.expander("â˜ï¸ å‘å¸ƒ"):
                    with st.form("pub"):
                        name_input = st.text_input("æ–‡ä»¶å(.txt)", value=f"v_{int(time.time())}.txt")
                        title_input = st.text_input("æ ‡é¢˜")
                        desc_input = st.text_area("ç®€ä»‹")
                        if st.form_submit_button("æäº¤"):
                            if name_input.endswith(".txt"): save_to_github_library(name_input, content_str, title_input, desc_input)

    # === å…¬å…±è¯ä¹¦åº“ ===
    elif menu == "ğŸŒ å…¬å…±è¯ä¹¦åº“":
        st.title("ğŸŒ ç¤¾åŒºå…¬å…±è¯ä¹¦åº“")
        st.markdown("<div class='info-box'>æ±‡é›†ç²¾é€‰è¯ä¹¦ã€‚ç‚¹å‡»ä¸‹æ–¹<b>â€œå±•å¼€â€</b>æŒ‰é’®æŸ¥çœ‹è¯¦æƒ…å¹¶å¤åˆ¶ã€‚</div>", unsafe_allow_html=True)
        search_q = st.text_input("ğŸ” æœç´¢...", "").lower()
        LIBRARY_DIR = "library"
        if not os.path.exists(LIBRARY_DIR): os.makedirs(LIBRARY_DIR)
        try:
            with open(os.path.join(LIBRARY_DIR, "info.json"), "r", encoding="utf-8") as f: book_info = json.load(f)
        except: book_info = {}
        files = [f for f in os.listdir(LIBRARY_DIR) if f.endswith(".txt")]
        visible = [f for f in files if search_q in f.lower() or search_q in book_info.get(f, {}).get("title", "").lower()]
        if not visible:
            st.warning("ğŸ“­ æš‚æ— æ•°æ®")
        else:
            cols = st.columns(3)
            for i, f in enumerate(visible):
                path = os.path.join(LIBRARY_DIR, f)
                try:
                    with open(path, "r", encoding="utf-8") as file: content = file.read()
                    count = len(content.splitlines())
                    meta = book_info.get(f, {})
                    title = meta.get("title", f)
                    desc = meta.get("desc", "æš‚æ— æè¿°")
                    with cols[i % 3]:
                        with st.container(border=True):
                            st.subheader(f"ğŸ“„ {title}")
                            st.caption(f"ğŸ“ {count} è¯")
                            c1, c2 = st.columns(2)
                            with c1: st.markdown("""<a href="https://web.shanbay.com/wordsweb/#/books" target="_blank" kind="primary" style="font-size:0.8rem; padding:0.4rem;">ğŸš€ å¯¼å…¥</a>""", unsafe_allow_html=True)
                            with c2: st.download_button("â¬‡ï¸ ä¸‹è½½", content, f, "text/plain", use_container_width=True)
                            with st.expander("ğŸ‘€ å±•å¼€æŸ¥çœ‹ä¸å¤åˆ¶"):
                                st.caption(desc)
                                render_copy_button(content, f"lib_copy_{i}")
                                st.code(content, language="text")
                except: continue

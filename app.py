import streamlit as st
import io
import re
import zipfile
import math
import chardet

# NLP Imports
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet, stopwords
from nltk import pos_tag

# DOCX Import
from docx import Document

# Optional Spacy
try:
    import spacy
    _HAS_SPACY = True
except ImportError:
    _HAS_SPACY = False

# ------------------ é¡µé¢é…ç½® ------------------
st.set_page_config(page_title="å•è¯æå–å™¨ Webç‰ˆ", page_icon="ğŸ“˜", layout="wide")

# ------------------ ç¼“å­˜èµ„æºåŠ è½½ (é¿å…é‡å¤åŠ è½½) ------------------
@st.cache_resource
def download_nltk_resources():
    """é™é»˜ä¸‹è½½ NLTK èµ„æº"""
    resources = ["punkt", "averaged_perceptron_tagger", "wordnet", "omw-1.4", "stopwords"]
    for r in resources:
        try:
            nltk.data.find(f'tokenizers/{r}')
        except LookupError:
            nltk.download(r, quiet=True)
        except ValueError:
            # éƒ¨åˆ†èµ„æºè·¯å¾„ä¸åŒï¼Œç®€å•çš„ try-catch å¤„ç†
            nltk.download(r, quiet=True)

@st.cache_resource
def load_spacy_model():
    if _HAS_SPACY:
        try:
            # å°è¯•åŠ è½½å°æ¨¡å‹ï¼Œéœ€æå‰ python -m spacy download en_core_web_sm
            return spacy.load("en_core_web_sm", disable=["ner", "parser"])
        except Exception:
            return None
    return None

# åˆå§‹åŒ–èµ„æº
download_nltk_resources()
nlp_spacy = load_spacy_model()

# ------------------ æ ¸å¿ƒå·¥å…·å‡½æ•° ------------------

def extract_text_from_bytes(file_obj, filename):
    """ä»å†…å­˜æ–‡ä»¶å¯¹è±¡ä¸­æå–æ–‡æœ¬"""
    ext = filename.split('.')[-1].lower()
    text = ""
    
    try:
        if ext == 'docx':
            doc = Document(file_obj)
            paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]
            text = "\n".join(paragraphs)
        else:
            # äºŒè¿›åˆ¶è¯»å–å¹¶æ£€æµ‹ç¼–ç 
            raw = file_obj.read()
            enc = chardet.detect(raw).get('encoding') or 'utf-8'
            text = raw.decode(enc, errors='ignore')
    except Exception as e:
        st.warning(f"âš ï¸ è¯»å– {filename} å¤±è´¥: {e}")
        return ""

    # é’ˆå¯¹å­—å¹•æ ¼å¼çš„æ¸…æ´—
    if ext == 'srt':
        return extract_english_from_srt(text)
    elif ext == 'ass':
        return extract_english_from_ass(text)
    elif ext == 'vtt':
        return extract_english_from_vtt(text)
    else:
        return text

def extract_english_from_srt(text):
    lines = []
    SRT_TIME_RE = re.compile(r"^\d{2}:\d{2}:\d{2}[,.]\d{3}")
    for ln in text.splitlines():
        s = ln.strip()
        if not s: continue
        if s.isdigit() or SRT_TIME_RE.match(s): continue
        s = re.sub(r"<.*?>", "", s)
        s = re.sub(r"\[.*?\]", "", s)
        parts = re.findall(r"[A-Za-z0-9'\",.?!:;()\- ]+", s)
        if parts: lines.append("".join(parts).strip())
    return " ".join(lines)

def extract_english_from_ass(text):
    lines = []
    for ln in text.splitlines():
        if ln.startswith("Dialogue:"):
            parts = ln.split(",", 9)
            if len(parts) >= 10:
                t = re.sub(r"\{.*?\}", "", parts[-1])
                t = re.sub(r"<.*?>", "", t)
                parts2 = re.findall(r"[A-Za-z0-9'\",.?!:;()\- ]+", t)
                if parts2: lines.append("".join(parts2).strip())
    return " ".join(lines)

def extract_english_from_vtt(text):
    lines = []
    VTT_TIME_RE = re.compile(r"^\d{2}:\d{2}:\d{2}\.\d{3}")
    for ln in text.splitlines():
        s = ln.strip()
        if not s or s.startswith("WEBVTT") or VTT_TIME_RE.match(s): continue
        s = re.sub(r"<.*?>", "", s)
        parts = re.findall(r"[A-Za-z0-9'\",.?!:;()\- ]+", s)
        if parts: lines.append("".join(parts).strip())
    return " ".join(lines)

def get_wordnet_pos(tag):
    if tag.startswith('J'): return wordnet.ADJ
    if tag.startswith('V'): return wordnet.VERB
    if tag.startswith('N'): return wordnet.NOUN
    if tag.startswith('R'): return wordnet.ADV
    return None

def process_words(all_text, mode, min_len, filter_set=None):
    """å¤„ç†æ ¸å¿ƒé€»è¾‘ï¼šåˆ†è¯ -> è¿˜åŸ -> è¿‡æ»¤"""
    TOKEN_RE = re.compile(r"[A-Za-z-]+")
    raw_tokens = TOKEN_RE.findall(all_text)
    cleaned = [re.sub(r'[^a-z]', '', w.lower()) for w in raw_tokens]
    cleaned = [w for w in cleaned if w]

    lemmatized = []
    
    # è¿›åº¦æ¡å®¹å™¨
    progress_bar = st.progress(0)
    status_text = st.empty()

    # 1. è¯å½¢è¿˜åŸ
    if mode == "spacy" and nlp_spacy is not None:
        status_text.text("æ­£åœ¨ä½¿ç”¨ spaCy è¿›è¡Œè¯å½¢è¿˜åŸ (é€Ÿåº¦è¾ƒæ…¢ï¼Œè¯·è€å¿ƒ)...")
        # spaCy å¤„ç†é™åˆ¶é•¿åº¦ä»¥å…å†…å­˜æº¢å‡ºï¼Œåˆ†å—å¤„ç†
        chunk_size = 50000
        chunks = [cleaned[i:i + chunk_size] for i in range(0, len(cleaned), chunk_size)]
        
        for i, chunk in enumerate(chunks):
            doc = nlp_spacy(" ".join(chunk))
            for token in doc:
                lw = token.lemma_.lower()
                if lw.isalpha() and wordnet.synsets(lw):
                    lemmatized.append(lw)
            progress_bar.progress((i + 1) / len(chunks))
            
    else:
        status_text.text("æ­£åœ¨ä½¿ç”¨ NLTK è¿›è¡Œè¯å½¢è¿˜åŸ...")
        lemmatizer = WordNetLemmatizer()
        # NLTK ä¹Ÿå¯ä»¥åˆ†å—æ˜¾ç¤ºè¿›åº¦
        tagged = pos_tag(cleaned)
        total = len(tagged)
        for i, (w, tag) in enumerate(tagged):
            wn = get_wordnet_pos(tag)
            lw = lemmatizer.lemmatize(w, wn) if wn else lemmatizer.lemmatize(w)
            # ç®€å•çš„ WordNet æ ¡éªŒ
            if wordnet.synsets(lw):
                lemmatized.append(lw)
            if i % 5000 == 0:
                progress_bar.progress(min(i / total, 1.0))
        progress_bar.progress(1.0)

    status_text.text("æ­£åœ¨å»é‡å’Œè¿‡æ»¤...")
    
    # 2. å»é‡ã€é•¿åº¦è¿‡æ»¤ã€åœç”¨è¯è¿‡æ»¤ã€è‡ªå®šä¹‰è¿‡æ»¤
    seen = set()
    final_words = []
    
    sys_stopwords = set(stopwords.words('english'))
    
    for w in lemmatized:
        if len(w) < min_len: continue
        if w in sys_stopwords: continue
        if filter_set and w in filter_set: continue
        
        if w not in seen:
            seen.add(w)
            final_words.append(w)
            
    status_text.empty()
    progress_bar.empty()
    
    return final_words

# ------------------ UI å¸ƒå±€ ------------------

st.title("ğŸ“˜ è‹±è¯­ç”Ÿè¯æœ¬ç”Ÿæˆå™¨ (Word Extractor)")
st.markdown("""
ä¸Šä¼ å­—å¹•æ–‡ä»¶ (`.srt`, `.ass`, `.vtt`) æˆ–æ–‡æ¡£ (`.docx`, `.txt`)ï¼Œ
ç³»ç»Ÿå°†è‡ªåŠ¨æå–å•è¯ã€è¿˜åŸè¯å½¢ã€å»é™¤ç®€å•è¯ï¼Œç”Ÿæˆå•è¯åˆ—è¡¨ã€‚
""")

with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")
    
    nlp_mode = st.selectbox(
        "NLP å¼•æ“", 
        ["nltk (å¿«é€Ÿ)", "spacy (ç²¾å‡†, éœ€å®‰è£…æ¨¡å‹)"], 
        index=0
    )
    mode_key = "spacy" if "spacy" in nlp_mode else "nltk"

    st.divider()
    
    min_len = st.number_input("æœ€å°å•è¯é•¿åº¦", min_value=1, value=3)
    chunk_size = st.number_input("è¾“å‡ºåˆ‡åˆ† (æ¯ä»½å•è¯æ•°)", min_value=100, value=5000)
    
    sort_order = st.radio("æ’åºæ–¹å¼", ["æŒ‰æ–‡æœ¬å‡ºç°é¡ºåº", "A-Z æ’åº", "éšæœºæ‰“ä¹±"])
    
    st.divider()
    
    filter_file = st.file_uploader("ä¸Šä¼ è¿‡æ»¤è¯è¡¨ (å¦‚: é«˜è€ƒ/å››å…­çº§è¯åº“.txt)", type=['txt'])
    filter_set = set()
    if filter_file:
        content = filter_file.getvalue().decode("utf-8", errors='ignore')
        filter_set = set(line.strip().lower() for line in content.splitlines() if line.strip())
        st.success(f"å·²åŠ è½½ {len(filter_set)} ä¸ªè¿‡æ»¤è¯")

# ä¸»åŒºåŸŸ
uploaded_files = st.file_uploader(
    "æ‹–æ‹½æ–‡ä»¶åˆ°æ­¤å¤„ (æ”¯æŒæ‰¹é‡)", 
    type=['txt', 'srt', 'ass', 'vtt', 'docx'], 
    accept_multiple_files=True
)

if uploaded_files:
    if st.button("ğŸš€ å¼€å§‹æå–", type="primary"):
        all_raw_text = []
        
        # 1. è¯»å–æ–‡ä»¶
        read_bar = st.progress(0)
        for i, file in enumerate(uploaded_files):
            text = extract_text_from_bytes(file, file.name)
            all_raw_text.append(text)
            read_bar.progress((i + 1) / len(uploaded_files))
        read_bar.empty()
        
        full_text = "\n".join(all_raw_text)
        st.info(f"å·²è¯»å–åŸå§‹æ–‡æœ¬ï¼Œçº¦ {len(full_text)} å­—ç¬¦ï¼Œå¼€å§‹å¤„ç†...")
        
        # 2. NLP å¤„ç†
        result_words = process_words(full_text, mode_key, min_len, filter_set)
        
        # 3. æ’åº
        if sort_order == "A-Z æ’åº":
            result_words.sort()
        elif sort_order == "éšæœºæ‰“ä¹±":
            import random
            random.shuffle(result_words)
            
        # 4. ç»“æœå±•ç¤ºä¸æ‰“åŒ…
        st.success(f"ğŸ‰ å¤„ç†å®Œæˆï¼å…±æå–åˆ° **{len(result_words)}** ä¸ªæœ‰æ•ˆç”Ÿè¯ã€‚")
        
        # é¢„è§ˆ
        with st.expander("ğŸ‘€ é¢„è§ˆå‰ 100 ä¸ªå•è¯"):
            st.write(", ".join(result_words[:100]))
            
        # 5. ç”Ÿæˆä¸‹è½½æ–‡ä»¶ (Zip)
        if result_words:
            zip_buffer = io.BytesIO()
            num_files = math.ceil(len(result_words) / chunk_size)
            
            with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                for i in range(num_files):
                    start = i * chunk_size
                    end = min(start + chunk_size, len(result_words))
                    chunk_data = "\n".join(result_words[start:end])
                    fname = f"word_list_{i+1}.txt"
                    zf.writestr(fname, chunk_data)
            
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½å•è¯æœ¬ (ZIP)",
                data=zip_buffer.getvalue(),
                file_name="extracted_words.zip",
                mime="application/zip"
            )
        else:
            st.warning("æœªæå–åˆ°ä»»ä½•å•è¯ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶ã€‚")

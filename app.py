import streamlit as st
import io
import re
import zipfile
import math
import chardet
import os
import json
import random
import time
import streamlit.components.v1 as components
from github import Github

# NLP Imports
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet, stopwords
from nltk import pos_tag
from docx import Document

# Optional Spacy
try:
    import spacy
    _HAS_SPACY = True
except ImportError:
    _HAS_SPACY = False

# ------------------ 0. åˆå§‹åŒ– & èµ„æºåŠ è½½ ------------------
WORDLIST_DIR = "wordlists"
LIBRARY_DIR = "library"
for d in [WORDLIST_DIR, LIBRARY_DIR]:
    if not os.path.exists(d): os.makedirs(d)

# ç¡®ä¿æ¼”ç¤ºæ•°æ®å­˜åœ¨
if not os.path.exists(os.path.join(WORDLIST_DIR, "primary.txt")):
    with open(os.path.join(WORDLIST_DIR, "primary.txt"), "w", encoding="utf-8") as f:
        f.write("a\nan\nthe\nis\nare\nam\nhello\ngood\nbook\npen")

PRESET_WORDLISTS = {
    "ğŸ‘¶ å°å­¦æ ¸å¿ƒè¯": os.path.join(WORDLIST_DIR, "primary.txt"),
    "ğŸ‘¦ ä¸­è€ƒå¿…å¤‡è¯": os.path.join(WORDLIST_DIR, "zhongkao.txt"),
    "ğŸ‘¨â€ğŸ“ é«˜è€ƒ3500è¯": os.path.join(WORDLIST_DIR, "gaokao.txt"),
}

@st.cache_resource
def download_nltk_resources():
    resources = ["punkt", "averaged_perceptron_tagger", "averaged_perceptron_tagger_eng", "wordnet", "omw-1.4", "stopwords"]
    for r in resources:
        try: nltk.data.find(f'tokenizers/{r}')
        except LookupError: nltk.download(r, quiet=True)
        except ValueError: nltk.download(r, quiet=True)

@st.cache_resource
def load_spacy_model():
    if _HAS_SPACY:
        try: return spacy.load("en_core_web_sm", disable=["ner", "parser"])
        except Exception: return None
    return None

download_nltk_resources()
nlp_spacy = load_spacy_model()

# ------------------ 1. æ·±åº¦ CSS è®¾è®¡ç³»ç»Ÿ ------------------
st.set_page_config(page_title="VocabMaster", page_icon="âš¡", layout="wide", initial_sidebar_state="expanded")

st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Plus+Jakarta+Sans:wght@400;500;600;700;800&family=Noto+Sans+SC:wght@400;500;700&display=swap');

    /* å…¨å±€èƒŒæ™¯ä¸å­—ä½“ */
    .stApp {
        background-color: #F8FAFC; 
        font-family: 'Plus Jakarta Sans', 'Noto Sans SC', sans-serif;
        color: #1e293b;
    }
    
    /* éšè—é»˜è®¤ Header */
    header[data-testid="stHeader"] { background: transparent; pointer-events: none; }
    .stMain { margin-top: -50px; }

    /* --- ä¾§è¾¹æ ä¼˜åŒ– --- */
    section[data-testid="stSidebar"] {
        background-color: white;
        border-right: 1px solid #f1f5f9;
        box-shadow: 2px 0 15px rgba(0,0,0,0.01);
    }

    /* --- æ ¸å¿ƒï¼šåŸç”Ÿå¡ç‰‡å®¹å™¨ç¾åŒ– --- */
    div[data-testid="stVerticalBlockBorderWrapper"] > div {
        background-color: white !important;
        border: 1px solid #e2e8f0 !important;
        border-radius: 16px !important;
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05) !important;
        padding: 24px !important;
    }

    /* --- ç»„ä»¶æ ·å¼ --- */
    /* æŒ‰é’® */
    div.stButton > button[kind="primary"] {
        background: #0f172a; color: white; border: none; width: 100%;
        border-radius: 12px; padding: 0.6rem 1.2rem; font-weight: 600;
        transition: all 0.2s;
    }
    div.stButton > button[kind="primary"]:hover {
        background: #334155; transform: translateY(-1px);
        box-shadow: 0 4px 12px rgba(15, 23, 42, 0.15);
    }

    /* è¾“å…¥æ¡†/ä¸‹æ‹‰æ¡† */
    .stTextInput > div > div, .stSelectbox > div > div, .stNumberInput > div > div {
        background-color: #F8FAFC; border: 1px solid #cbd5e1; border-radius: 10px;
    }
    
    /* æ–‡æœ¬åŸŸ (Text Area) */
    .stTextArea textarea {
        background-color: #F8FAFC; border: 1px solid #cbd5e1; border-radius: 10px;
        font-family: 'JetBrains Mono', monospace; font-size: 14px;
    }

    /* æ–‡ä»¶ä¸Šä¼ åŒº (è™šçº¿é£æ ¼) */
    [data-testid="stFileUploader"] {
        background-color: #F8FAFC; border: 2px dashed #94a3b8; border-radius: 12px;
        padding: 20px; transition: all 0.3s;
    }
    [data-testid="stFileUploader"]:hover {
        border-color: #0F766E; background-color: #f0fdfa;
    }
    
    /* é¡¶éƒ¨å¯¼èˆªæ¡ Glass */
    .top-nav {
        background: rgba(255,255,255,0.8); backdrop-filter: blur(10px);
        padding: 15px 20px; border-bottom: 1px solid #e2e8f0;
        display: flex; justify-content: space-between; align-items: center;
        margin-bottom: 20px; border-radius: 0 0 16px 16px;
    }

    /* 3D ä¹¦ç± */
    .book-3d {
        width: 100%; aspect-ratio: 3/4; border-radius: 6px 14px 14px 6px;
        position: relative; transition: transform 0.3s; cursor: pointer;
        display: flex; flex-direction: column; justify-content: center; align-items: center;
        text-align: center; padding: 10px; box-shadow: 5px 5px 15px rgba(0,0,0,0.1);
    }
    .book-3d:hover { transform: translateY(-5px) scale(1.02); box-shadow: 8px 12px 25px rgba(0,0,0,0.15); }
    
</style>
""", unsafe_allow_html=True)

# ------------------ 2. é€»è¾‘å‡½æ•° ------------------
def save_to_github_library(filename, content, title, desc):
    try:
        # 1. ä¼˜å…ˆå°è¯•äº‘ç«¯ä¸Šä¼ 
        if "GITHUB_TOKEN" in st.secrets:
            token = st.secrets["GITHUB_TOKEN"]
            g = Github(token)
            repo = g.get_repo(f"{st.secrets['GITHUB_USERNAME']}/{st.secrets['GITHUB_REPO']}")
            
            # ä¸Šä¼ æ–‡ä»¶
            try: repo.create_file(f"library/{filename}", f"Create {filename}", content)
            except: repo.update_file(f"library/{filename}", f"Update {filename}", content, repo.get_contents(f"library/{filename}").sha)

            # æ›´æ–°äº‘ç«¯ info.json
            info_path = "library/info.json"
            try:
                c = repo.get_contents(info_path)
                info = json.loads(c.decoded_content.decode())
            except:
                info = {}
            
            info[filename] = {"title": title, "desc": desc, "date": time.strftime("%Y-%m-%d")}
            
            try:
                repo.update_file(info_path, "Update info", json.dumps(info, ensure_ascii=False, indent=2), repo.get_contents(info_path).sha)
            except:
                repo.create_file(info_path, "Init info", json.dumps(info, ensure_ascii=False, indent=2))
                
            st.toast("âœ… äº‘ç«¯å‘å¸ƒæˆåŠŸï¼", icon="ğŸ‰")
        else:
            st.toast("âš ï¸ æ—  GitHub Tokenï¼Œä»…ä¿å­˜åˆ°æœ¬åœ°ã€‚", icon="ğŸ“‚")

        # 2. å§‹ç»ˆä¿å­˜åˆ°æœ¬åœ° (ç”¨äºå³æ—¶æ˜¾ç¤º)
        with open(os.path.join(LIBRARY_DIR, filename), "w", encoding="utf-8") as f: f.write(content)
        
        # æ›´æ–°æœ¬åœ° info.json
        local_info_path = os.path.join(LIBRARY_DIR, "info.json")
        try: 
            with open(local_info_path, "r", encoding="utf-8") as f: local_info = json.load(f)
        except: local_info = {}
        
        local_info[filename] = {"title": title, "desc": desc, "date": time.strftime("%Y-%m-%d")}
        
        with open(local_info_path, "w", encoding="utf-8") as f: json.dump(local_info, f, indent=2, ensure_ascii=False)

        time.sleep(1)
        st.rerun()

    except Exception as e:
        st.error(f"å‘å¸ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

def extract_text_from_bytes(file_obj, filename):
    try:
        ext = filename.split('.')[-1].lower()
        if ext == 'docx':
            return "\n".join([p.text for p in Document(file_obj).paragraphs if p.text.strip()])
        raw = file_obj.read()
        return raw.decode(chardet.detect(raw)['encoding'] or 'utf-8', errors='ignore')
    except: return ""

def process_words(text, mode, min_len, filter_set=None):
    with st.spinner("AI æ­£åœ¨åˆ†æè¯­ä¹‰ä¸è¯å½¢..."):
        time.sleep(0.5)
        cleaned = [re.sub(r'[^a-z]', '', w.lower()) for w in re.findall(r"[A-Za-z-]+", text) if w]
        lemmatized = []
        
        if mode == "spacy" and nlp_spacy:
            doc = nlp_spacy(" ".join(cleaned[:100000]))
            lemmatized = [t.lemma_.lower() for t in doc if t.lemma_.isalpha()]
        else:
            l = WordNetLemmatizer()
            lemmatized = [l.lemmatize(w) for w in cleaned]

        seen, final = set(), []
        stops = set(stopwords.words('english'))
        for w in lemmatized:
            if len(w) >= min_len and w not in stops and (not filter_set or w not in filter_set) and w not in seen:
                seen.add(w)
                final.append(w)
        return final

def copy_btn(text):
    safe_text = json.dumps(text)
    components.html(f"""
    <div style="display:flex; justify-content:center;">
        <button id="cbtn" onclick="copy()" style="
            background: linear-gradient(135deg, #0f172a 0%, #334155 100%);
            color: white; border: none; padding: 10px 20px; border-radius: 8px;
            font-family: sans-serif; font-weight: bold; cursor: pointer; width: 100%;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        ">ğŸ“‹ ä¸€é”®å¤åˆ¶ç»“æœ</button>
    </div>
    <script>
    function copy() {{
        navigator.clipboard.writeText({safe_text});
        document.getElementById("cbtn").innerText = "âœ… å·²å¤åˆ¶ï¼";
        setTimeout(() => {{ document.getElementById("cbtn").innerText = "ğŸ“‹ ä¸€é”®å¤åˆ¶ç»“æœ"; }}, 2000);
    }}
    </script>
    """, height=50)

# ------------------ 3. é¡µé¢å¸ƒå±€ ------------------

# ä¾§è¾¹æ 
with st.sidebar:
    st.markdown("""
        <div style="display:flex; align-items:center; gap:12px; margin-bottom:20px; padding:10px;">
            <div style="width:32px; height:32px; background:#0f172a; color:white; border-radius:6px; display:flex; align-items:center; justify-content:center; font-weight:bold;">V</div>
            <h3 style="margin:0; font-size:18px;">VocabMaster</h3>
        </div>
    """, unsafe_allow_html=True)
    menu = st.radio("MAIN MENU", ["âš¡ æ™ºèƒ½å·¥ä½œå°", "ğŸ“š å…¬å…±è¯ä¹¦åº“", "ğŸ‘¤ ä¸ªäººä¸­å¿ƒ"], label_visibility="collapsed")
    st.markdown("---")
    st.info("ğŸ“¢ å­—å¹•æ–‡ä»¶æ— éœ€è½¬æ¢ï¼Œç›´æ¥æ‹–å…¥å³å¯ã€‚")

# é¡¶éƒ¨å¯¼èˆª
st.markdown("""
<div class="top-nav">
    <div style="font-weight:700; color:#334155;">Dashboard</div>
    <div style="font-size:12px; background:white; padding:4px 10px; border-radius:20px; border:1px solid #e2e8f0;">User: Free Plan</div>
</div>
""", unsafe_allow_html=True)

# === âš¡ æ™ºèƒ½å·¥ä½œå° ===
if "å·¥ä½œå°" in menu:
    
    # 1. èµ„æºå¯¼èˆª (å®Œæ•´ä½¿ç”¨ä½ æä¾›çš„å†…å®¹)
    with st.expander("ğŸ“– æ–°æ‰‹æŒ‡å— & å®è—èµ„æºåº“ (ç‚¹å‡»å±•å¼€)", expanded=False):
        t1, t2, t3, t4 = st.tabs(["ğŸ’¡ æ“ä½œæŒ‡å¼•", "ğŸ¬ å½±è§†å­—å¹•", "ğŸ“š åŸè‘—é˜…è¯»", "ğŸ§ å¬åŠ›ç´ æ"])
        
        with t1:
            st.markdown("""
            <div style="padding:5px;">
            <h5 style="margin-top:0">ğŸš€ å››æ­¥åˆ¶ä½œä¸“å±è¯ä¹¦ï¼š</h5>
            <ol>
                <li><b>å‡†å¤‡ç´ æ</b>ï¼šä»å³ä¾§æ ‡ç­¾é¡µä¸‹è½½ <code>.srt</code> å­—å¹•æˆ– <code>.txt</code> ç”µå­ä¹¦ã€‚</li>
                <li><b>æ¸…æ´—è®¾ç½®</b>ï¼šåœ¨ä¸‹æ–¹ã€è®¾ç½®æå–è§„åˆ™ã€‘ä¸­ï¼Œé€‰æ‹©<b>â€œé¢„ç½®ç†Ÿè¯åº“â€</b>æˆ–ä¸Šä¼ è‡ªå®šä¹‰ç†Ÿè¯è¡¨ï¼ˆéå¸¸é‡è¦ï¼èƒ½å±è”½æ‰ is, the ç­‰ç®€å•è¯ï¼‰ã€‚</li>
                <li><b>æ™ºèƒ½æå–</b>ï¼šå°†æ–‡ä»¶æ‹–å…¥ä¸Šä¼ åŒºï¼ŒAI è‡ªåŠ¨å®Œæˆå»é‡ã€è¯å½¢è¿˜åŸï¼ˆRun/Ran/Running â†’ Runï¼‰ã€‚</li>
                <li><b>é—­ç¯å­¦ä¹ </b>ï¼šç‚¹å‡»ç”Ÿæˆçš„<b>â€œä¸€é”®å¤åˆ¶â€</b>æŒ‰é’®ï¼Œè·³è½¬æ‰‡è´ç½‘æ‰¹é‡åˆ¶å¡ï¼Œæˆ–å¯¼å‡ºè¯ä¹¦ã€‚</li>
            </ol>
            </div>
            """, unsafe_allow_html=True)
            
        with t2:
            st.info("ğŸ’¡ å­—å¹•æ–‡ä»¶æ˜¯æå–å£è¯­è¯æ±‡çš„æœ€ä½³ææ–™ã€‚ä¸‹è½½åæ— éœ€è½¬æ¢ï¼Œç›´æ¥æ‹–å…¥æœ¬å·¥å…·ã€‚")
            c1, c2 = st.columns(2)
            c1.markdown("ğŸ¯ **[ä¼ªå°„æ‰‹ç½‘ (Assrt)](https://assrt.net/)**\n<small>è€ç‰Œç«™ç‚¹ï¼Œèµ„æºæœ€å…¨ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ã€‚</small>", unsafe_allow_html=True)
            c1.markdown("ğŸ“º **[å­—å¹•åº“ (Zimuku)](http://zimuku.org/)**\n<small>ç¾å‰§ã€æ—¥å‰§æ›´æ–°é€Ÿåº¦æå¿«ã€‚</small>", unsafe_allow_html=True)
            c1.markdown("âš¡ **[Addic7ed](https://www.addic7ed.com/)**\n<small>ç¾å‰§ç”Ÿè‚‰æ›´æ–°æœ€å¿«çš„åœ°æ–¹ï¼Œé€‚åˆé«˜é˜¶å­¦ä¹ è€…ã€‚</small>", unsafe_allow_html=True)
            
            c2.markdown("ğŸ’ **[SubHD](https://subhd.tv/)**\n<small>ç•Œé¢æ¸…çˆ½ï¼Œé«˜æ¸…å½±è§†å­—å¹•é¦–é€‰ã€‚</small>", unsafe_allow_html=True)
            c2.markdown("ğŸŒ **[OpenSubtitles](https://www.opensubtitles.org/)**\n<small>å…¨çƒæœ€å¤§å­—å¹•åº“ï¼Œå¯»æ‰¾çº¯è‹±æ–‡å­—å¹•é¦–é€‰ã€‚</small>", unsafe_allow_html=True)
            c2.markdown("ğŸï¸ **[YIFY Subtitles](https://yifysubtitles.ch/)**\n<small>ä¸“é—¨é’ˆå¯¹ç”µå½±çš„é«˜è´¨é‡è‹±æ–‡å­—å¹•ã€‚</small>", unsafe_allow_html=True)

        with t3:
            st.success("ğŸ“š æ¨èä¸‹è½½ .txt æˆ– .epub (éœ€è½¬txt) æ ¼å¼ã€‚")
            c1, c2 = st.columns(2)
            c1.markdown("ğŸ›ï¸ **[Project Gutenberg](https://www.gutenberg.org/)**\n<small>æ‹¥æœ‰7ä¸‡+å…è´¹å…¬ç‰ˆç”µå­ä¹¦ï¼Œè‹±æ–‡åŸè‘—çš„å¤§å®åº“ã€‚</small>", unsafe_allow_html=True)
            c1.markdown("ğŸ“– **[ManyBooks](https://manybooks.net/)**\n<small>æ’ç‰ˆç²¾ç¾ï¼Œåˆ†ç±»è¯¦ç»†ï¼Œä¸‹è½½ä½“éªŒå¥½ã€‚</small>", unsafe_allow_html=True)
            
            c2.markdown("ğŸ“° **[Global Times](https://www.globaltimes.cn/)**\n<small>å›½äº§è‹±æ–‡åª’ä½“ï¼Œç”¨è¯è´´è¿‘æ—¶æ”¿ï¼Œé€‚åˆå¤‡è€ƒã€‚</small>", unsafe_allow_html=True)
            c2.markdown("ğŸ§  **[Scientific American](https://www.scientificamerican.com/)**\n<small>é«˜é˜¶ç§‘æ™®æ–‡ç« ï¼Œæ‰˜ç¦/é›…æ€/GRE é˜…è¯»åŒæºç´ æã€‚</small>", unsafe_allow_html=True)

        with t4:
            st.warning("ğŸ§ æŠ€å·§ï¼šä¸‹è½½ Transcript (æ–‡ç¨¿) æå–å•è¯ï¼Œå­¦å®Œå†å»å¬ã€‚")
            c1, c2 = st.columns(2)
            c1.markdown("ğŸ”´ **[TED Talks](https://www.ted.com/)**\n<small>æ€æƒ³ç››å®´ï¼Œæ¯ä¸ªè§†é¢‘éƒ½è‡ªå¸¦å¤šè¯­è¨€æ–‡ç¨¿ã€‚</small>", unsafe_allow_html=True)
            c1.markdown("ğŸ‡ºğŸ‡¸ **[VOA Learning English](https://learningenglish.voanews.com/)**\n<small>ç»å…¸åˆ†çº§å¬åŠ›ææ–™ï¼Œå«çº¯æ­£æ–‡ç¨¿ã€‚</small>", unsafe_allow_html=True)
            
            c2.markdown("ğŸ‡¬ğŸ‡§ **[BBC Learning English](https://www.bbc.co.uk/learningenglish/)**\n<small>è‹±å¼è‹±è¯­é‡‘ç‰Œæ•™ç¨‹ï¼Œ6 Minute English å¿…å¬ã€‚</small>", unsafe_allow_html=True)
            c2.markdown("ğŸ“ **[Coursera](https://www.coursera.org/)**\n<small>å­¦ä¹ ä¸“ä¸šè¯¾ï¼ˆè®¡ç®—æœº/å•†ç§‘ï¼‰çš„æœ€å¥½æ–¹å¼ã€‚</small>", unsafe_allow_html=True)

    # 2. ä¸»æ“ä½œåŒº (å·¦å³åˆ†æ ï¼ŒåŸç”Ÿå¡ç‰‡)
    if 'result_words' not in st.session_state: st.session_state.result_words = []
    
    col_conf, col_input = st.columns([1, 2], gap="medium")

    # å·¦ä¾§ï¼šé…ç½®å¡ç‰‡
    with col_conf:
        with st.container(border=True):
            st.markdown("##### ğŸ› ï¸ æå–é…ç½®")
            nlp_mode = st.selectbox("AI å¼•æ“", ["nltk (å¿«é€Ÿ)", "spacy (ç²¾å‡†)"])
            sort_order = st.selectbox("æ’åº", ["æŒ‰æ–‡æœ¬å‡ºç°é¡ºåº", "A-Z æ’åº", "éšæœºæ‰“ä¹±"])
            min_len = st.slider("æœ€çŸ­è¯é•¿", 2, 15, 3)
            
            st.divider()
            st.markdown("##### ğŸ›¡ï¸ ç†Ÿè¯å±è”½")
            selected_presets = st.multiselect("é¢„ç½®åº“", PRESET_WORDLISTS.keys(), default=[])
            filter_file = st.file_uploader("è‡ªå®šä¹‰å±è”½è¡¨ (.txt)", type=['txt'])

    # å³ä¾§ï¼šè¾“å…¥å¡ç‰‡
    with col_input:
        with st.container(border=True):
            st.markdown("""
            <div style="display:flex; justify-content:space-between; margin-bottom:10px;">
                <b>ğŸ“„ è¾“å…¥æº (Input Source)</b>
                <span style="font-size:12px; color:#64748b; background:#f1f5f9; padding:2px 6px; border-radius:4px;">æ”¯æŒ .txt .srt .docx</span>
            </div>
            """, unsafe_allow_html=True)
            
            tab_txt, tab_file = st.tabs(["âœï¸ ç²˜è´´æ–‡æœ¬", "ğŸ“‚ ä¸Šä¼ æ–‡ä»¶"])
            
            with tab_txt:
                input_text = st.text_area("ç²˜è´´åŒºåŸŸ", height=250, placeholder="åœ¨æ­¤ç›´æ¥ç²˜è´´æ–‡ç« ã€å­—å¹•æ–‡æœ¬...", label_visibility="collapsed")
            with tab_file:
                uploaded_files = st.file_uploader("æ‹–æ‹½åŒºåŸŸ", type=['txt','srt','ass','docx'], accept_multiple_files=True, label_visibility="collapsed")

            st.markdown("<br>", unsafe_allow_html=True)
            start_btn = st.button("ğŸš€ å¼€å§‹æ™ºèƒ½æå–", type="primary")

    # é€»è¾‘å¤„ç†
    if start_btn:
        full_text = input_text
        if uploaded_files:
            for f in uploaded_files:
                full_text += "\n" + extract_text_from_bytes(f, f.name)
        
        if not full_text.strip():
            st.warning("âš ï¸ è¯·å…ˆè¾“å…¥æ–‡æœ¬æˆ–ä¸Šä¼ æ–‡ä»¶")
        else:
            filter_set = set()
            for p in selected_presets:
                if os.path.exists(PRESET_WORDLISTS[p]):
                    with open(PRESET_WORDLISTS[p], 'r', encoding='utf-8') as f: filter_set.update(f.read().splitlines())
            if filter_file:
                filter_set.update(filter_file.getvalue().decode('utf-8', errors='ignore').splitlines())
            
            mode_key = "spacy" if "spacy" in nlp_mode else "nltk"
            words = process_words(full_text, mode_key, min_len, filter_set)
            
            if sort_order == "A-Z æ’åº": words.sort()
            elif sort_order == "éšæœºæ‰“ä¹±": random.shuffle(words)
            
            st.session_state.result_words = words
            st.rerun()

    # 3. ç»“æœå±•ç¤º
    if st.session_state.result_words:
        st.markdown("<br>", unsafe_allow_html=True)
        with st.container(border=True):
            words = st.session_state.result_words
            content_str = "\n".join(words)
            
            st.markdown(f"### ğŸ‰ æå–ç»“æœ (å…± {len(words)} è¯)")
            st.text_area("Result", value=content_str, height=200, label_visibility="collapsed")
            
            c1, c2, c3 = st.columns([1, 1, 1])
            with c1: copy_btn(content_str)
            with c2: st.download_button("ğŸ“¦ ä¸‹è½½ (.txt)", content_str, "vocab.txt", "text/plain", use_container_width=True)
            with c3:
                with st.popover("â˜ï¸ å‘å¸ƒåˆ°ç¤¾åŒºåº“", use_container_width=True):
                    with st.form("pub_form"):
                        name = st.text_input("æ–‡ä»¶å (è‹±æ–‡, e.g. friends_s1.txt)", f"list_{int(time.time())}.txt")
                        title = st.text_input("æ ‡é¢˜")
                        desc = st.text_area("æè¿°")
                        if st.form_submit_button("å‘å¸ƒ"):
                            if name.endswith(".txt"): save_to_github_library(name, content_str, title, desc)
                            else: st.error("æ–‡ä»¶åéœ€ä»¥ .txt ç»“å°¾")

# === ğŸ“š å…¬å…±è¯ä¹¦åº“ ===
elif "è¯ä¹¦åº“" in menu:
    # é¡¶éƒ¨å·¥å…·æ å¡ç‰‡
    with st.container(border=True):
        c_search, c_filter = st.columns([2, 1])
        q = c_search.text_input("æœç´¢", placeholder="ğŸ” æœç´¢ä¹¦å...", label_visibility="collapsed")
        c_filter.multiselect("ç­›é€‰", ["è€ƒç ”", "é›…æ€", "æ‰˜ç¦"], label_visibility="collapsed", placeholder="æ ‡ç­¾ç­›é€‰")

    # åŠ¨æ€è¯»å– Library (ä¿®å¤ç‚¹ï¼šä»æœ¬åœ°ç›®å½•è¯»å–)
    try:
        with open(os.path.join(LIBRARY_DIR, "info.json"), "r", encoding="utf-8") as f: book_info = json.load(f)
    except: book_info = {}
    
    files = [f for f in os.listdir(LIBRARY_DIR) if f.endswith(".txt")]
    visible = [f for f in files if q.lower() in f.lower() or q.lower() in book_info.get(f, {}).get("title", "").lower()]
    
    if not visible:
        st.info("ğŸ“­ æš‚æ— æ•°æ®ï¼Œè¯·å…ˆå»å·¥ä½œå°å‘å¸ƒè¯ä¹¦ã€‚")
    else:
        st.markdown("<br>", unsafe_allow_html=True)
        cols = st.columns(4)
        colors = ["#fef3c7", "#d1fae5", "#dbeafe", "#fee2e2", "#f3e8ff"]
        txt_colors = ["#92400e", "#065f46", "#1e40af", "#991b1b", "#6b21a8"]
        
        for i, f in enumerate(visible):
            meta = book_info.get(f, {})
            title = meta.get("title", f)
            desc = meta.get("desc", "æ— æè¿°")
            idx = i % 5
            
            with cols[i % 4]:
                st.markdown(f"""
                <div class="book-3d" style="background-color:{colors[idx]}; color:{txt_colors[idx]};">
                    <h4 style="margin:0; font-size:16px; overflow:hidden; text-overflow:ellipsis; white-space:nowrap; width:100%;">{title}</h4>
                    <p style="font-size:12px; opacity:0.8; margin-top:5px; height:36px; overflow:hidden;">{desc[:40]}...</p>
                </div>
                """, unsafe_allow_html=True)
                
                with st.expander("æ“ä½œ"):
                    try:
                        with open(os.path.join(LIBRARY_DIR, f), 'r', encoding='utf-8') as _f: content = _f.read()
                        st.caption(f"æ–‡ä»¶å: {f}")
                        st.download_button("â¬‡ï¸ ä¸‹è½½", content, f)
                        copy_btn(content)
                    except: st.error("æ–‡ä»¶è¯»å–å¤±è´¥")

else:
    st.info("ğŸš§ ä¸ªäººä¸­å¿ƒå¼€å‘ä¸­...")
